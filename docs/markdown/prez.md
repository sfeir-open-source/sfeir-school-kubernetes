
<!-- .slide:-->

# Rappels sur Docker


##==##
<!-- .slide:-->

# Qu‚Äôest-ce que Docker ?


Docker permet de packager une application
avec l‚Äôensemble de ses d√©pendances 
dans une unit√© standardis√©e pour le d√©ploiement de logiciels :

les CONTAINERS



Notes:
Docker en une phrase



##==##
<!-- .slide:-->

# Utilisation de Docker


Database SQL


Frontend application


Backend application


Database NoSql


PROD


DEV


UAT


PREPROD


![](./images/container.png)

![](./images/Generic_Databases.png)

![](./images/Generic_Databases.png)

![](./images/Generic_Databases.png)

![](./images/Generic_Databases.png)

![](./images/14822516839733_sevreur.png)

![](./images/14822516839733_sevreur.png)

![](./images/14822516839733_sevreur.png)

![](./images/14822516839733_sevreur.png)

![](./images/docker.png)

Notes:
En informatique, le container peut aussi embarquer une grande vari√©t√© de contenus.

Il s‚Äôex√©cute √† l‚Äôidentique dans plusieurs environnements sans √™tre modifi√©.



##==##
<!-- .slide:-->

# VM vs Containers


![](./images/g41f33631ff_0_169.png)

![](./images/g41f33631ff_0_170.png)

Notes:
Avantages de Docker :

permet un partage des ressources efficace

plus de simple pour g√©rer des environnements d‚Äôex√©cution vari√©s



Avantages des full VMs :

isolation GARANTIE puisqu‚Äôil n‚Äôy a aucune communications entre l'h√¥te et la machine virtuelle

acc√®s au mat√©riel plus simple





##==##
<!-- .slide:-->

# Rappel sur les containers


![](./images/g3f0c37370d_0_342.png)

Notes:
Le Dockerfile :

contient les instructions pour construire l‚Äôimage




Les images :

contiennent les fichiers n√©cessaires √† l‚Äôex√©cution de votre application : binaires, d√©pendances, configuration, ‚Ä¶

sont immuable



Les containers :

isolent l‚Äôex√©cution de processus

dans le syst√®me de fichier de l‚Äôimage

au niveau r√©seau

sont con√ßus pour √™tre √©ph√©m√®res



La registry :

permet le partage d‚Äôimages

Docker Hub, Google Container Registry, ...





##==##
<!-- .slide:-->

# Plus de d√©tails ?


Notes:
Proposer d‚Äôaller plus loin sur Docker avec des extraits de la Sfeir School Docker en 1h/1h30



http://bit.ly/2nUh7Wg






##==##
<!-- .slide:-->

# Kubernetes : les origines


##==##
<!-- .slide:-->

# Google : 15 ans de containers


![](./images/g3f0c37370d_0_482.png)

Notes:
Google, 15 ans d‚Äôexp√©rience sur l‚Äôorchestration de containers

Borg = c++, ~2003



##==##
<!-- .slide:-->

# Cloud Native Computing Foundation


https://github.com/cncf/landscape


![](./images/g3f0c37370d_0_492.png)

![](./images/g3f0c37370d_0_527.png)

Notes:
Kubernetes r√©ecrit en Go par les Googlers

Offert √† la Cloud Native Computing Foundation en 2015

Kubernetes n‚Äôest qu‚Äôun outils parmi tous les outils pouvant vous aider √† cr√©er des architectures ‚Äú
Cloud-Natives
‚Äù



La 
Fondation Linux
 (en anglais 
Linux Foundation
) est un 
consortium
 √† but non lucratif fond√© le 
21
 
janvier
 
2007
, a pour mission de prot√©ger et standardiser 
Linux
 en procurant les ressources et services centralis√©s, la Linux Foundation regroupe 70 membres.




##==##
<!-- .slide: class="with-code" -->

# Kubernetes, pour quoi faire ?


```
Lancer 5 containers bas√©s sur l‚Äôimage redis:4.0
Mettre en place un load-balancer interne au cluster pour servir ces 5 containers
Lancer 10 containers webapp:1.0
Mettre en place un load-balancer public pour permettre d‚Äôacc√©der aux containers de l‚Äôext√©rieur du cluster
Augmenter le nombre de containers webapp pendant les soldes üòâ
Continuer √† servir les requ√™tes pendant la mise √† jour vers webapp:2.0

```

![](./images/g3f0c37370d_0_502.png)

Notes:
Orchestration de containers dans un cluster de machines



‚Äúbarreur d'un navire‚Äù en grec 



##==##
<!-- .slide: class="with-code" -->

# Mais aussi


```
Mise √† l‚Äô√©chelle automatique
D√©ploiement ‚ÄúBlue/green‚Äù et ‚ÄúCanary‚Äù
Ex√©cution de traitements unitaires ou r√©p√©t√©s
Prioriser les t√¢ches en cas de manque de ressources sur le cluster
Ex√©cuter des services qui persistent des donn√©es sur disque
Contr√¥ler l‚Äôacc√®s aux diff√©rentes ressources
S‚Äôint√©grer avec des services externes (‚Äúservice catalog‚Äù)
Automatiser les t√¢ches complexes (‚Äúoperators‚Äù)

```

Notes:
Orchestration de containers dans un cluster de machines



##==##
<!-- .slide:-->

# Premier aper√ßu


##==##
<!-- .slide:-->

# Cluster


![](./images/g3f3310ef84_0_410.png)

Notes:
Un cluster est un ensemble de machines qui collaborent entre elles.

Sur Kubernetes, on distingue le master des noeuds (nodes).



##==##
<!-- .slide:-->

# Ex√©cuter des applications


![](./images/g3f3310ef84_0_411.png)

Notes:
Le master est responsable de d‚Äôessentiel de la partie ‚Äúcontr√¥le‚Äù (control plane) du cluster.

Les noeuds (nodes) sont responsable de l‚Äôex√©cution des applications.



Les fonctions de master et de node sont habituellement d√©ploy√©es sur des machines diff√©rentes.

En dev ces deux fonctions peuvent √™tre d√©ploy√©es sur une unique machine (ex: Minikube)




##==##
<!-- .slide:-->

# Node


![](./images/g3f3310ef84_0_418.png)

Notes:
Les noeuds ex√©cutent les applications packag√©es dans des containers regroup√©s dans des ‚ÄúPods‚Äù.

L‚Äôex√©cution des Pods est g√©r√©e par les kubelet.

Kubelet est un process qui pilote l'engine docker pour deployer les resources



##==##
<!-- .slide:-->

# Pod


![](./images/g40daa77750_5_116.png)

Notes:
Pod :

1 ou plusieurs containers

en g√©n√©ral un seul

partageant :

une seule IP

un ou plusieurs volumes







##==##
<!-- .slide:-->

# Patterns


![](./images/g3f3310ef84_0_993.png)

Notes:
patterns :

sidecar

file-puller + webserver

log forwarder

adapter

nginx + php_fpm

ambassador



##==##
<!-- .slide:-->

# Orchestrer les pods


![](./images/g3386b050b1_0_69.png)

Notes:
Objectif du slide :
 premier aper√ßu des objets Kubernetes. 
Ne pas d√©tailler plus que les commentaires ci-dessous !



On parle d‚Äôobjets Kubernetes, de 
resources
.

Chaque ressource repr√©sente une API.



ReplicaSet :

maintient le nombre demand√© de pod identiques

la plupart du temps cr√©√© par le Deployment



Deployment :

g√®re le cycle de vie des pods : versions, mise √† jour continue (sans interruption)



Service :

load-balancer interne vers un ensemble de pods

accessible par IP ou par dns interne



Ingress :

Point d‚Äôentr√©e HTTP(S)

routage par Path ou par Virtualhost



Namespace :

pour isoler des ressources





On reverra plus en d√©tails ces objets et d‚Äôautres : ConfigMap, DaemonSet, Job, ...



##==##
<!-- .slide:-->

# D√©marrage du cluster
Configuration de kubectl


##==##
<!-- .slide:-->

Google Kubernetes Engine

D√©monstration en direct


# Cr√©er un cluster GKE


Notes:
Cr√©er un cluster en 3 clics depuis la Console GCP

Montrer les options disponibles depuis l‚Äôinterface graphique :

version Kubernetes

taille de VM

type d‚ÄôOS (COS ou Ubuntu)

activation Stackdriver Monitoring / Logging

‚Ä¶



Note : ne pas d√©montrer ici les avantages de GKE vs K8S natif



##==##
<!-- .slide:-->

Machine Virtuelle (VM) ‚ÄúCompute Engine‚Äù gratuite
Cloud SDK, docker, kubectl, git, ... pr√©-install√©s
‚ÄúBoost mode‚Äù


# Cloud Shell


Notes:
Avantage Cloud Shell :

Gratuit, accessible depuis un navigateur

5Go de stockage persistant dans /home

Toujours accessible avec un simple navigateur web





Outils pour GCP et pour docker/kubernetes d√©j√† install√©s



Le ‚ÄúBoost mode‚Äù permet d‚Äôavoir une VM plus performante (n1-standard-1)



##==##
<!-- .slide: class="with-code" -->

# kubectl


```
Client Kubernetes en ligne de commande

Astuces :
kubectl version --short
kubectl completion -h
kubectl help <command>
kubectl explain [--recursive] <resource>
kubectl <verb> <resource>

```

Notes:
Comme Docker, Kubernetes s‚Äôutilise en mode client-server.

Le client s‚Äôappelle ‚Äú
kubectl
‚Äù et appelle un composant du cluster appell√© 
apiserver
.



kubectl int√®gre un m√©canisme pour installer l‚Äôauto-completion des commandes dans bash et zsh

On a aussi acc√®s √† la 
doc des commandes
 et au 
format des ressources
 avec les sous-commandes 
help
 et 
explain
.

‚Ä¶ d√©taill√© dans la suite



##==##
<!-- .slide:-->

# ~/.kube/config


<< auth-provider >>


Notes:
La configuration du client kubectl se fait dans le fichier ~/.kube/config

Ce fichier r√©pertorie les ‚Äúcontextes‚Äù, constitu√©es du couple Cluster et User, ainsi que le namespace courant sur ce cluster.



Sur GKE, la partie user est d√©l√©gu√©e √† un fournisseur d‚Äôauthentification impl√©ment√© par la commande 
gcloud
 du Cloud SDK.



##==##
<!-- .slide: class="with-code" -->

```
gcloud container clusters list

gcloud container clusters get-credentials \   --zone europe-west1-a <mycluster>

```

# Contextes Kubernetes sur GKE


Notes:
La seconde commande permet de cr√©er le contexte kubectl correspondant √† une instance GKE.



##==##
<!-- .slide: class="with-code"  class="with-code"  class="with-code" -->

```
$ kubectl config get-contexts # List contexts
$ kubectl config set-context gke-dev # Update contexts
$ kubectl config view --minify=true # Show current-context configuration

```

# Changer de contexte


```
apiVersion: v1kind: Configcurrent-context: gke-contextcontexts:- name: gke-context  context:    cluster: gke-cluster    user: gke-userclusters:- name: gke-cluster  cluster:    server: https://12.34.56.78

```

```
users:- name: gke-user  user:    auth-provider:      name: gcp      config:        cmd-path: /usr/bin/gcloud        cmd-args: config config-helper --format=json        access-token: ya29.GlwXBuG[...]KcYlQ        [...]


```

Notes:
Exemple de fichier de config de kubectl (~/.kube/config).

On y retrouve les contextes, les clusters, les utilisateurs

On voit aussi l‚Äôutilisation de gcloud comme fournisseur d‚Äôauthentification



##==##
<!-- .slide: class="with-code" -->

# Utilitaires autour de kubectl


```
https://kubernetes.io/docs/tasks/tools/install-kubectl/#optional-kubectl-configurations 
$ kubectl <TAB>		# autocompletion
https://github.com/ahmetb/kubectx$ kubectx gke-dev		# changer de contexte$ kubens kube-system	# modifier le ns du contexte courant

```

##==##
<!-- .slide: class="with-code" -->

# Utilitaires autour de kubectl


```
~800 kubectl aliases (bash/zsh)https://github.com/ahmetb/kubectl-aliases$ kgpo		# kubectl get pod
https://github.com/jonmosco/kube-ps1 : 

```

![](./images/g41f33631ff_0_58.png)

##==##
<!-- .slide:-->

# Architecture interne


##==##
<!-- .slide:-->

# Master et Nodes


devs, ops


clients


master(s)


![](./images/g41ceea32d2_1_122.png)

Notes:
Cluster :

1 ou plusieurs masters (
recommand√© pour la 
HA - Haute Disponibilit√©)

1 ou plusieurs noeuds (
recommand√© pour la 
HA - Haute Disponibilit√©)



Les ops passent des commandes au master

Les clients acc√®dent aux pods via les noeuds





##==##
<!-- .slide:-->

# Master et Nodes


devs, ops


clients


master(s)


Notes:
Cluster :

1 ou plusieurs masters (
recommand√© pour la 
HA - Haute Disponibilit√©)

1 ou plusieurs noeuds (
recommand√© pour la 
HA - Haute Disponibilit√©)



Les ops passent des commandes au master

Les clients acc√®dent aux pods via les noeuds





##==##
<!-- .slide:-->

devs, ops
kubectl


master(s)


# ApiServer et etcd


Valide


Stocke


Notes:
ApiServer

Devs ou ops envoient la description de l‚Äô√©tat voulu des objets

Fichiers au format .yaml, transform√©s en json pour envoi sur l‚Äôapiserver

Objets stock√©s dans serveur etcd (serveur cl√©-valeur distribu√©)





##==##
<!-- .slide:-->

master(s)


# Kubelet


nodes


Notes:
Kubelet

Tourne sur les noeuds

Ex√©cute les pods

S‚Äôenregistre aupr√®s du master via l‚Äôapiserver





##==##
<!-- .slide:-->

master(s)


# Controllers


nodes


Notes:
Controllers

Il y a plusieurs contr√¥leurs, un par type d‚Äôobjet Kubernetes

Ils ont chacun une responsabilit√© pr√©cise

Ils surveillent constamment la configuration de l‚Äôetcd

Ils r√©agissent au changement pour atteindre l‚Äô√©tat demand√©, en mettant √† jour la configuration via l‚Äôapiserver. Exemple :

le Scheduler met √† jour la configuration du pod pour lui assigner un noeud

le kubelet (lui-aussi un type de contr√¥leur) ex√©cute le pod demand√© puis enregistre son statut dans la configuration







##==##
<!-- .slide:-->

# R√©capitulatif


Le Kubelet s‚Äôenregistre dans la config
L‚Äôutilisateur envoi l‚Äô√©tat voulu sur etcd via l‚Äôapiserver
Les contr√¥leurs et le Kubelet r√©agissent aux changement d‚Äô√©tat pour obtenir et maintenir l‚Äô√©tat voulu


##==##
<!-- .slide:-->

# √Ä venir...


![](./images/g3f3310ef84_0_1014.png)

Notes:
Ce qu‚Äôon voit dans le chapitre suivant



##==##
<!-- .slide:-->

# Getting started


##==##
<!-- .slide:-->

# Namespace


##==##
<!-- .slide:-->

# Namespace


Espace de nom pour isoler les d√©ploiements
Peut √™tre utiliser pour s√©parer les environnements
Le nom d‚Äôune ressource est unique au sein d‚Äôun namespace.
Par d√©faut:
default
kube-system
kube-public


Notes:
nom de ressource unique dans un namespace
 (mais pas au sein d‚Äôun cluster)


default : namespace de travail par d√©faut

kube-system : namespace pour les d√©ploiements internes au cluster

kube-public : namespace cr√©√© par d√©faut pour les ressources expos√©s publiquement



##==##
<!-- .slide: class="with-code" -->

```
Clonez le d√©p√¥t github suivant :
git clone https://github.com/sfeir-open-source/sfeir-school-kubernetes.git

et placez-vous dans le d√©p√¥t clon√© :
cd sfeir-school-kubernetes/steps

```

# R√©cup√©rer les exercices


Notes:
√Ä faire

dans Cloud Shell

OU

sur les machines locales si 
git, kubectl et Cloud SDK
 install√©s



##==##
<!-- .slide: class="with-code" -->

```
Dans un premier terminal CloudShell, lancez la commande
kubectl proxy --address=0.0.0.0 \					--www=gcp-live-k8s-visualizer -p 8080

Puis ouvrez la preview Web CloudShell         et modifier l‚Äôurl pour qu‚Äôelle termine par /static/ : 

https://8080-dot-<...>-dot-devshell.appspot.com/static/ 

```

# k8s-visualizer by Ray Tsang


![](./images/g3f3310ef84_0_1027.png)

##==##
<!-- .slide:-->

# k8s-visualizer by Ray Tsang


![](./images/g41f33631ff_0_121.png)

Notes:
La premi√®re commande lance une interface graphique pour voir en temps r√©el les pods et les services.



##==##
<!-- .slide: class="with-code" -->

# k8s-visualizer by Ray Tsang


```
La commande est disponible dans k8s-sfeirschool-2018/bin/viz.sh

Retrouvez cette application sur :

https://github.com/saturnism/gcp-live-k8s-visualizer 


```

Notes:
La premi√®re commande lance une interface graphique pour voir en temps r√©el les pods et les services.



##==##
<!-- .slide: class="with-code" -->

```
Dans un second terminal, envoyez √† Kubernetes le fichier yaml du Deployment :
$ cd 01-getting-started
$ cat deployment.yaml
$ kubectl apply -f deployment.yaml --record

‚Ñπ La ligne de commande suivante (mode imp√©ratif) donne un r√©sultat proche du fichier yaml (mode d√©claratif) mais avec moins d‚Äôoptions :
$ kubectl run nginx --image="nginx:alpine" \					  --labels="app=nginx" --record

```

# Premier d√©ploiement


Notes:
Bien qu‚Äôon puisse cr√©er un Pod individuellement, on utilise rarement cette possibilit√©.


En g√©n√©ral, on utilise les Deployment pour g√©rer des pods.




L‚Äôoption 
--record
 permet de conserver un historique des configurations des Deployment.

√Ä utiliser avec chaque commande qu‚Äôon veut enregistrer dans l‚Äôhistoriques.



On pr√©f√®re le 
mode d√©claratif
, par 
fichiers
, qu‚Äôon peut versionner dans git,

que le 
mode imp√©ratif
 en 
ligne de commande
 qui n‚Äôest pas reproductible.



Observons ensuite les ressources cr√©√©es : pod, replicaset, deployment



##==##
<!-- .slide:-->

# TP : Configurer kubectl


##==##
<!-- .slide:-->

# TP : Configurer kubectl




!! Faites une sauvegarde du fichier ~/.kube/config !!


##==##
<!-- .slide:-->

# TP : Configurer kubectl


Installer gcloud & kubectl
Configurer kubectl
$ wget -qO cred.json bit.ly/2jYVpSI
$ gcloud auth activate-service-account --key-file=cred.json --project=sfeir-builder
$ gcloud config set project sfeir-builder
$ gcloud container clusters get-credentials sfeirschool-gke-cluster --zone europe-west3-a --project sfeir-builder
$ echo 'source <(kubectl completion bash)' >>~/.bashrc


##==##
<!-- .slide:-->

# TP : Configurer kubectl


$ alias k=‚Äùkubectl‚Äù
$ kubectl get nodes
$ kubectl get namespace
$ kubectl config set-context --current --namespace=<insert-namespace-name-here> 
$ kubectl config view --minify=true




##==##
<!-- .slide:-->

# Manifest


Notes:
Configuration files - Written in YAML or JSON, these files describe the desired state of your application in terms of Kubernetes API objects. A file can include one or more API object descriptions (manifests). (See the example YAML from the stateless app).



Fichier de configuration : √©crit en yaml ou json, ce fichier d√©crit l‚Äô√©tat d√©sir√© d‚Äôune ressource Kubernetes. Dans un fichier, on peut inclure une ou plusieurs descriptions d‚Äôobjets 



##==##
<!-- .slide: class="with-code" -->

```
kind: PodapiVersion: v1metadata:  name: nginx  labels:    app: nginxspec:  containers:  - name: nginx    image: nginx:alpinestatus:  ...

```

# Manifest


Notes:
kind : type de resource que l‚Äôon va cr√©√© 

apiVersion: version de l‚Äôapi de la ressource que l‚Äôon veut cr√©√©

metadata: information qui permette d‚Äôidentifier la resource, name est obligatoire 

spec: d√©finit l‚Äô√©tat d√©sir√© de notre ressource

status : √©tat actuel de la ressource



##==##
<!-- .slide:-->

# Cr√©er et int√©ragir avec un pod


##==##
<!-- .slide: class="with-code" -->

# kubectl get pods


```
$ kubectl get podsNAME                     READY     STATUS    RESTARTS   AGEnginx-5dbbff858d-qfb7f   1/1       Running   0          2m
$ kubectl get pod -o yaml nginx-5dbbff858d-qfb7f
apiVersion: v1kind: Pod
metadata:
  name: nginx-5dbbff858d-qfb7f...

```

Notes:
READY : nombre de containers d√©marr√©s / nombre total de container

RESTART : si un container n‚Äôest pas dans l‚Äô√©tat voulu (crash, indisponibilit√©), le pod peut √™tre red√©marr√©





##==##
<!-- .slide: class="with-code" -->

```
apiVersion: v1kind: Podmetadata:  name: nginx  labels:    app: nginxspec:  containers:  - name: nginx    image: nginx:alpinestatus:  ...

```

# Pod yaml


##==##
<!-- .slide: class="with-code" -->

```
$ kubectl explain podKIND:     PodVERSION:  v1DESCRIPTION:     Pod is a collection of containers that can run on a host. This resource is     created by clients and scheduled onto hosts.FIELDS:   apiVersion	<string>   kind	<string>   metadata	<Object>   spec	<Object>[...]

```

# Pods explained


Notes:
La commande 
explain
 donne tous les d√©tails sur les propri√©t√©s de chaque ressources Kubernetes‚Ä¶

‚Ä¶ r√©cursivement (slides suivant)



##==##
<!-- .slide: class="with-code" -->

```
$ kubectl explain pod.spec.containers.imageKIND:     PodVERSION:  v1FIELD:    image <string>DESCRIPTION:     Docker image name. More info:     https://kubernetes.io/docs/concepts/containers/images This field is     optional to allow higher level config management to default or override     container images in workload controllers like Deployments and     StatefulSets.

```

# Pods explained‚Ä¶ deeper


##==##
<!-- .slide: class="with-code" -->

```
...status:  containerStatuses:  - name: nginx    containerID: docker://8db3af41eb87[...]3d103255    image: nginx:alpine    imageID: docker-pullable://nginx@sha256:1aed1[...]a0a5440    state:      running:        startedAt: 2018-08-13T21:08:10Z  hostIP: 192.168.65.3  podIP: 10.1.0.227

```

# Pod status (live)


Notes:
Une fois le fichier yaml envoy√© dans la configuration, les contr√¥leurs viennent mettre √† jour le pod dans la partie status.

On retrouve entre autre l‚ÄôIP du 
node
 qui l‚Äôh√©berge et l‚ÄôIP du 
pod
 lui-m√™me.



##==##
<!-- .slide:-->

# TP : Pod


##==##
<!-- .slide:-->

# TP : Pod


Regarder le code pour cr√©er un pod 
$ cat pod/monolith.yaml
Cr√©er le pod : 
$ kubectl create -f  pod/monolith.yaml


Notes:
Cr√©ation d‚Äôun pod simple : on affiche les specs du pod, et on cr√©√© le pod avec kubectl.





##==##
<!-- .slide:-->

# TP : Information sur le Pod



$ kubectl get pods 
$ kubectl describe pods <pod-name>
Quelle est l‚Äôip du pod ? 
Sur quel node le pod tourne ?
Quel container tourne dans le pod ?




##==##
<!-- .slide:-->

# TP : Int√©ragir avec le Pod



Cr√©er un tunnel depuis son poste avec le pod
$ kubectl port-forward monolith 10080:80
Ouvrir une autre fen√™tre shell
$ curl http://127.0.0.1:10080
$ curl http://127.0.0.1:10080/secure
$ curl -u user http://127.0.0.1:10080/login
Le mot de passe est : "password"
$ curl -H "Authorization: Bearer <token>" http://127.0.0.1:10080/secure



##==##
<!-- .slide:-->

# TP : Int√©ragir avec le Pod



Voir les logs du pods
$ kubectl logs monolith
Ouvrir un shell dans le pod 
$ kubectl exec monolith --stdin --tty -c monolith /bin/sh
Lister les process dans le container 
$ ps aux


##==##
<!-- .slide:-->

# Liveness/Readiness


##==##
<!-- .slide:-->

# Readiness/Liveness


Readiness : V√©rifier que le pod est pr√™t √† recevoir du flux


Liveness : V√©rifier que le service r√©pond correctement




Notes:
Kubeproxy sortira un pod du flux si le readiness devient ko.



Kubelet va red√©marrer le container si le liveness est ko.





##==##
<!-- .slide: class="with-code" -->

# Readiness/Liveness : command



```

    livenessProbe:
      exec:
        command:
        - cat
        - /tmp/healthy
      initialDelaySeconds: 5
      periodSeconds: 5

```

Notes:
Il est possible d‚Äôutiliser une commande pour tester l‚Äô√©tat du service



##==##
<!-- .slide: class="with-code" -->

# Readiness/Liveness : tcp socket



```
    readinessProbe:
      tcpSocket:
        port: 8080
      initialDelaySeconds: 5
      periodSeconds: 10
    livenessProbe:
      tcpSocket:
        port: 8080
      initialDelaySeconds: 15
      periodSeconds: 20


```

##==##
<!-- .slide: class="with-code" -->

# Readiness/Liveness : http


```
...
spec:
  containers:
    - readinessProbe:    # accepte des requ√™tes quand OK 
        httpGet:
          path: /readiness
          port: 81
          scheme: HTTP
        initialDelaySeconds: 5
        timeoutSeconds: 1
      livenessProbe:    # red√©marre le pod si KO 
        httpGet:
          path: /healthz
          port: 81
          scheme: HTTP
        initialDelaySeconds: 5 # D√©lai avant la premi√®re requ√™te
        periodSeconds: 15      # Fr√©quence des tests
        timeoutSeconds: 5

      ...


```

Notes:
readinessProbe indique √† kubelet quand un pod est pr√™t √† servir du traffic 



livenessProbe indique si le pod est ok, si nok kubelet red√©marre le pod 



##==##
<!-- .slide:-->

# TP : Readiness/Liveness 


##==##
<!-- .slide:-->

# TP : Readiness/Liveness


Regarder le contenu du pod 
$ cat readiness/healthy-monolith.yaml
$ kubectl create -f readiness/healthy-monolith.yaml
$ kubectl describe pods/healthy-monolith


##==##
<!-- .slide: class="with-code" -->

# TP : Readiness/Liveness


```
$ kubectl describe pods

Comment est configur√© la sonde readiness ? 
Comment est configur√© la sonde liveness 
√† quelle fr√©quence la surveillance readiness est effectu√© ? 
√† partir de combien de secondes la sonde liveness est effectu√© ? 


```

Notes:
Comment est configur√© la sonde readiness ? httpGet sur /readiness:81 

liveness  sur /healthz:81

la valeur par d√©faut du readiness : 10 secondes

liveness est effectu√© √† partir de 5 secondes





##==##
<!-- .slide:-->

# TP : Tester les sondes 


$ kubectl get pods healthy-monolith
Noter l‚Äô√©tat OK du pod
$ kubectl port-forward healthy-monolith 10081:81
Forcer l‚Äôapplication a pass√© en √©tat ‚Äúfailed‚Äù 
$ curl http://127.0.0.1:10081/readiness/status
Attendre 45 secondes que la sonde en failed 
$ kubectl describe pods healthy-monolith
Vous pouvez noter dans l‚Äôhistorique le moment o√π le pod est pass√© en ‚Äúunhealthy‚Äù



##==##
<!-- .slide:-->

# TP : Tester les sondes 



Maintenant nous allons ‚Äúcasser‚Äù la sonde liveness
$ curl http://127.0.0.1:10081/healthz/status

$ kubectl get pods 

Que se passe-t-il quand le liveness est ko ? 



Notes:
Que se passe-t-il : dans ce cas le pod est red√©marr√©



##==##
<!-- .slide:-->

# Configuration d‚Äôune application


##==##
<!-- .slide:-->

# Configuration


Configurer l‚ÄôURI d‚Äôune base de donn√©es
Injecter un fichier application-prod.yml
Sp√©cifier les credentials d‚Äôune api


##==##
<!-- .slide: class="with-code" -->

```
apiVersion: v1kind: Podmetadata:  name: envar-demospec:  containers:  - name: envar-demo-container    image: debian    env:    - name: DEMO_GREETING      value: "Hello from the environment"    - name: DEMO_FAREWELL      value: "Such a sweet sorrow"

```

# Variables d‚Äôenvironnement


Notes:
On peut injecter une variable d‚Äôenvironnement via le champ ‚Äúenv‚Äù.



##==##
<!-- .slide:-->

# ConfigMap : cr√©ation


$ kubectl create configmap <map-name> <data-source>
$ kubectl create configmap my-app-conf --from-file=my-app-conf/configmap/application-dev.properties
$ kubectl create configmap my-app-conf --from-literal=db-server=mydbserver.mycompany.com



Notes:
Pour injecter un fichier de configuration, on peut injecter un fichier de configuration dans un pod avec un configMap. 



On peut cr√©er un fichier de config, depuis : 

un fichier ou un r√©pertoire  
--from-file
=


depuis la cli : 
--from-literal
=







##==##
<!-- .slide: class="with-code" -->

# ConfigMap: manifests


```
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-app-config
data:
  application.properties: |
    db-server=mydbserver.dev.mycompany.com
    username=my-rw-dbuser

$ kubectl apply -f configmap.yml

```

Notes:
derri√®re le | on peut mettre n‚Äôimporte un fichier non structur√© yaml



##==##
<!-- .slide: class="with-code"  class="with-code" -->

```
apiVersion: v1kind: Podmetadata:  name: configmap-podspec:  containers:    - name: test      image: busybox      volumeMounts:        - name: config-vol          mountPath: /etc/config

```

# ConfigMap : utilisation


```
  volumes:    - name: config-vol      configMap:        name: log-config        items:          - key: log_level            path: log_level

```

Point de montage du fichier : /etc/config/log_level


##==##
<!-- .slide: class="with-code" -->

# Secret : type


```
$ kubectl create secret --help

Available Commands:
  docker-registry Create a secret for use with a Docker registry
  generic         Create a secret from a local file, directory or literal value
  tls             Create a TLS secret



```

Notes:
On peut cr√©er 3 types de secret : 

docker-registry : pour enregistrer les credentials pour pull une image docker depuis une registry priv√©e

generic : √©quivalent √† configmap, permet de stocker des secrets de type cl√©/valeur

tls : pour stocker un certificat serveur pour exposer un service en https



##==##
<!-- .slide: class="with-code" -->

# Secret : manifest 


```
apiVersion: v1
kind: Secret
metadata:
  name: mysecret
type: Opaque
data:
  username: YWRtaW4=
  password: MWYyZDFlMmU2N2Rm

```

##==##
<!-- .slide:-->

# Secret : cr√©ation


Cr√©er un secret depuis un fichier
$ kubectl create secret generic db-user-pass --from-file=./password.txt
Cr√©er un secret depuis une cl√©/valeur
$ kubectl create secret generic prod-db-secret --from-literal=username=produser
Cr√©er un secret ‚Äúdocker-registry ‚Äù
$ kubectl create secret docker-registry regcred --docker-server=<your-registry-server> --docker-username=<your-name> --docker-password=<your-pword> --docker-email=<your-email>


Notes:
Un secret va permettre de stocker des informations sensibles, comme des mots de passe, des cl√©s priv√©es. 







##==##
<!-- .slide: class="with-code" -->

```
apiVersion: v1kind: Podspec:  containers:  - name: envar-demo-container    image: debian    env:    - name: DEMO_GREETING      valueFrom:
secretKeyRef:
name: demo      # le nom du secret
key: password   # la cl√© dans le secret

```

# Secret : env vars


##==##
<!-- .slide: class="with-code"  class="with-code" -->

# Secret : volume


```
...
kind: Pod
spec:
  containers:
  - name: mypod
    image: redis
    volumeMounts:
    - name: secret-volume
      mountPath: "/etc/foo"
      readOnly: true
  

```

```
  volumes:
  - name: secret-volume
    secret:
      secretName: mysecret

```

##==##
<!-- .slide:-->

# TP : Configuration 


##==##
<!-- .slide:-->

# TP : ConfigMap


$ kubectl create configmap nginx-proxy-conf --from-file=configuration/nginx/proxy.conf

$ kubectl describe configmaps nginx-proxy-conf


##==##
<!-- .slide:-->

# Quizz : ConfigMap



Combien y a t-il d‚Äô√©l√©ments dans la configmap nginx-proxy-conf ?
Quel est le nom de ces √©l√©ments ?  



Notes:
1 element : le fichier proxy.conf



##==##
<!-- .slide:-->

# TP : Secret


$ kubectl create secret generic tls-certs --from-file=configuration/tls/

$ kubectl describe secrets tls-certs




##==##
<!-- .slide:-->

# Quizz : Secret



Combien y a t-il d‚Äô√©l√©ments dans le Secret tls-cert ?

Quel est le nom de ces √©l√©ments ? 



Notes:
Le secret contient 4 elements : ca-key.pem

ca.pem

cert.pem

key.pem



on peut voir que les secrets ne sont pas affiche dans la console



##==##
<!-- .slide:-->

# TP : Injection de fichiers


$ cat configuration/pod/secure-monolith.yaml

Comment le secret est inject√© dans le pod ? 

Comment le configmap est inject√© dans le pod ?


Notes:
Les secrets sont inject√©s sous la forme d'un volume

de la meme facon le configMap est injecte sous la forme d'un volume. 



##==##
<!-- .slide:-->

# TP : Injection de fichiers



Cr√©er le pod ‚Äúsecure-monolith‚Äù
$ kubectl create -f configuration/pod/secure-monolith.yaml
Exposer le port sur votre poste 
$ kubectl port-forward secure-monolith 10443:443
Faites une requ√™te avec curl 
$ curl --cacert configuration/tls/ca.pem https://127.0.0.1:10443


Notes:
Du coup on a inject√© les certificats et la configuration du service nginx, et on arrive bien √† faire une requ√™te en https sur notre service. 



Pr√©ciser : ce n‚Äôest pas la bonne m√©thode pour exposer un service en https sur kubernetes ;)) 







##==##
<!-- .slide:-->

# Service


##==##
<!-- .slide:-->

# Exposer des pods


![](./images/g3f3310ef84_0_277.png)

Notes:
Les pods sont expos√©s au sein du cluster via les 
Services
.

Les services obtiennent une IP et un nom DNS interne au cluster.

Ils r√©partissent la charge entre les pods, cibl√©s par des 
labels
.



##==##
<!-- .slide:-->

# Service type ClusterIP


IP interne 10.0.1.8


10.0.0.6


10.0.0.7


Notes:
Un Service de type ClusterIP est accessible depuis une IP interne au cluster.

On l‚Äôutilise en g√©n√©ral pour la communication inter-pods.



##==##
<!-- .slide:-->

# Service type NodePort


10.0.0.7


10.0.0.6


10.0.0.7


Notes:
Un service du type nodePort est associ√© √† un port identique sur chaque noeud du cluster.

Ce port est accessible depuis l‚Äôext√©rieur du cluster.

Il doit appartenir √† la plage de ports 30000 - 32767 (configurable sur le master).

On peut le choisir mais il faut mieux laisser faire Kubernetes, qui g√®rera les probl√®mes de ports occup√©s.



Inconv√©nient : il faut donc conna√Ætre l‚Äôadresse IP d‚Äôau moins un noeud du cluster pour acc√©der au service.



##==##
<!-- .slide:-->

# Service type LoadBalancer


10.0.0.6


10.0.0.7


Notes:
Un Service de type LoadBalancer utilise un √©quilibreur de charge externe au cluster.

Ce type de Service est utilis√© sur les plateformes cloud ainsi que dans Docker-for-Desktop.

L‚Äôinconv√©nient c‚Äôest que chaque service aura son propre load balancer avec sa propre IP publique/Internet



##==##
<!-- .slide:-->

# Service type ExternalName


IP interne 10.0.1.8


Notes:
Un Service de type ExternalName fourni un alias DNS de type CNAME.

Il n‚Äôest utilisable qu‚Äôen interne.



##==##
<!-- .slide:-->

# Service avec Endpoint explicite


IP interne 10.0.1.8


Notes:
En interne, Kubernetes g√©n√®re une ressource Endpoint pour chaque Pod cibl√© par le selecteur.

Il est possible de ne pas renseigner de selecteur et √† la place de cr√©er manuellement les ressources Endpoint avec une IP externe au cluster.

C‚Äôest la mani√®re de rajouter une abstraction vers un service externe au cluster. Les Pods ne voient que le Service interne.



##==##
<!-- .slide: class="with-code" -->

```
apiVersion: v1kind: Servicemetadata:  name: nginxspec:  selector:    app: nginx  type: NodePort  ports:  - name: http    port: 80    protocol: TCP    targetPort: 80

```

# Service yaml


Notes:
Le 
selector
 permet de cibler les pods par leurs labels.



Il y a plusieurs 
types
 de services qu‚Äôon voit tout de suite.



##==##
<!-- .slide: class="with-code" -->

```
$ kubectl apply -f service.yaml

La ligne de commande suivante donne un r√©sultat √©quivalent au fichier yaml :

$ kubectl expose nginx --port=80 --type=NodePort

```

# Cr√©er un service


##==##
<!-- .slide: class="with-code" -->

# Services


```
$ kubectl get servicesNAME        TYPE       CLUSTER-IP   EXTERNAL-IP PORT(S)       AGEkubernetes  ClusterIP  10.96.0.1    <none>      443/TCP       3dnginx       NodePort   10.97.47.155 <none>      80:31450/TCP  2s$ kubectl get svc -o yaml nginx
apiVersion: v1kind: Service
metadata:
  name: nginx...

```

##==##
<!-- .slide:-->

# Service : DNS interne


Entr√©e DNS : <service>.<namespace>.svc.cluster.local

Et dans le namespace : <service>


Notes:
Pour chaque service cr√©√©, le service est accessible via une entr√©e DNS : <service>.<namespace>.svc.cluster.local



Et √† l‚Äôint√©rieur du namespace, uniquement avec le nom du service



##==##
<!-- .slide:-->

# TP : Service 


##==##
<!-- .slide:-->

# TP  : Cr√©er un service 


Lire le manifest de l‚Äôobjet service 
$ cat service/monolith.yaml

Cr√©er le service 
$ kubectl create -f service/monolith.yaml



##==##
<!-- .slide:-->

# TP : Interagir avec le service


$ gcloud compute instances list
NAME                                           ZONE            MACHINE_TYPE   PREEMPTIBLE  INTERNAL_IP  EXTERNAL_IP     STATUS
gke-istio-workshop-default-pool-e37360ba-369z  europe-west1-b  n1-standard-1               10.132.0.3   35.205.92.225   RUNNING
gke-istio-workshop-default-pool-e37360ba-3hz4  europe-west1-b  n1-standard-1               10.132.0.2   35.240.127.238  RUNNING
...
Prenez l‚Äôune des ips externes et connectez vous-y
$ curl -k https://<EXTERNAL_IP>:31000


Notes:
Le r√©sultat est un √©chec :  

curl -k https://35.246.130.12:31000

curl: (7) Failed to connect to 35.246.130.12 port 31000: Connection refused



Normal : il n‚Äôy a pas de label correspondant !





##==##
<!-- .slide:-->

# TP : Service


Pourquoi la requ√™te ne fonctionne pas ? 
$ kubectl get services monolith

$ kubectl describe services monolith
Combien y a t-il de endpoints au service ? 


Notes:
Le r√©sultat est un √©chec :  

curl -k https://35.246.130.12:31000

curl: (7) Failed to connect to 35.246.130.12 port 31000: Connection refused



Normal : il n‚Äôy a pas de label correspondant !





##==##
<!-- .slide:-->

# TP : Service


Lister les pods par labels
$ kubectl get pods -l "app=monolith"
$ kubectl get pods -l "app=monolith,secure=enabled"
On va labelliser un pod 
$ kubectl label pods secure-monolith ‚Äúsecure=enabled‚Äù
Et maintenant ? 
$ kubectl describe services monolith


Notes:
On labellise les pods 



##==##
<!-- .slide:-->

# ReplicaSet


##==##
<!-- .slide: class="with-code" -->

# ReplicaSet.yaml 


```
apiVersion: extensions/v1beta1kind: ReplicaSetmetadata:  name: nginx  labels:    app: nginxspec:  replicas: 1  selector:    matchLabels:      app: nginx  template:    <... pod definition ...>

```

Notes:
Le ReplicaSet inclut la d√©finition du pod dans la partie 
template

Le 
selector
 permet d‚Äôidentifier les pods g√©r√©s par ce ReplicaSet via des 
labels
.

La propri√©t√© 
replicas
 indique le nombre d‚Äôinstances voulues.



##==##
<!-- .slide: class="with-code" -->

# kubectl get replicaset


```
$ kubectl get ReplicaSetNAME               DESIRED   CURRENT   READY     AGEnginx-5dbbff858d   1         1         1         14m

$ kubectl get rs -o yaml nginx-5dbbff858d
apiVersion: extensions/v1beta1kind: ReplicaSet
...

```

Notes:
Le ReplicaSet maintient le nombre demand√© de pods 
en √©tat de fonctionnement
.

La colonne READY montre les pods qui sont fonctionnels, et pas juste les pods cr√©√©s	(
par exemple avec des 
containers en cours de cr√©ation ou crash√©s)



##==##
<!-- .slide: class="with-code" -->

# ReplicaSet status (live)


```
...status:  availableReplicas: 1  fullyLabeledReplicas: 1  readyReplicas: 1  replicas: 1

```

Notes:
On voit le status des replicas 



##==##
<!-- .slide:-->

# Deployment


##==##
<!-- .slide: class="with-code"  class="with-code" -->

```
apiVersion: extensions/v1beta1kind: Deploymentmetadata:  name: nginxspec:  replicas: 1  selector:    matchLabels:      app: nginx  template:
  ...


```

# Deployment.yaml 


```
  ...  template:    metadata:      labels:        app: nginx    spec:      containers:      - name: nginx        image: nginx:alpine

```

Notes:
On retrouve dans le Deployment les specs du Pod et du ReplicaSet



##==##
<!-- .slide: class="with-code" -->

# kubectl get deployment


```
$ kubectl get deploymentNAME      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGEnginx     1         1         1            1           1h
$ kubectl get deploy -o yaml nginx
apiVersion: extensions/v1beta1kind: Deployment
...

```

Notes:
Le Deployment g√®re des 
Pods
 via un 
ReplicaSet
.

Contrairement au ReplicaSet qui maintient la m√™me configuration de Pod en changeant  juste le nombre de replicats,

le Deployment sait g√©rer la mise √† jour des Pods en ‚Äú
Rolling upgrade
‚Äù



##==##
<!-- .slide: class="with-code" -->

```

$ kubectl scale deployment/nginx --replicas=10 

$ kubectl get all

```

# Mise √† l‚Äô√©chelle manuelle


Notes:
La commande 
scale
 permet d‚Äôindiquer au ReplicaSet le nombre de r√©plicas √† maintenir.



On scale ici √† 10 √† instances de pod pour d√©montrer les 
mises √† jours progressives
.



##==##
<!-- .slide:-->

# The Label Game


##==##
<!-- .slide:-->

# Les labels Kubernetes


Pour le fonctionnement interne de Kubernetes
ReplicaSet && Deployments ‚áí Pods
Services ‚áí Pods


##==##
<!-- .slide:-->

# ‚ÄúThe label game‚Äù


![](./images/g41f33631ff_0_65.png)

Notes:
Jeu : trouver le selecteur permettant d‚Äôidentifier les pods encadr√©s



##==##
<!-- .slide:-->

# ‚ÄúThe label game‚Äù


![](./images/g41f33631ff_0_68.png)

Notes:
selector:

  App: MyApp





##==##
<!-- .slide:-->

# ‚ÄúThe label game‚Äù


![](./images/g41f33631ff_0_73.png)

Notes:
selector:

  R√¥le: Interface




##==##
<!-- .slide:-->

# ‚ÄúThe label game‚Äù


![](./images/g41f33631ff_0_78.png)

Notes:
selector:

  Phase: Prod




##==##
<!-- .slide:-->

# ‚ÄúThe label game‚Äù


![](./images/g41f33631ff_0_83.png)

Notes:
selector:

  Phase: test

  R√¥le: BE



##==##
<!-- .slide:-->

# TP : Deployment


##==##
<!-- .slide:-->

# TP : Deployment


On va cr√©er une stack compl√®te hello, auth, frontend

Avec deployment et service 



##==##
<!-- .slide:-->

# TP : Deployment


Cr√©er un d√©ploiement : 
$ kubectl create -f deployments/auth.yaml
$ kubectl describe deployments auth

Et cr√©er le service correspondant
$ kubectl create -f deployments/service-auth.yaml


##==##
<!-- .slide:-->

# TP : Deployement Hello


Idem pour le service Hello 
$ kubectl create -f deployments/hello.yaml
$ kubectl describe deployments hello
$ kubectl create -f deployments/service-hello.yaml


##==##
<!-- .slide:-->

# TP : Deployment Frontend


Et pour le service Front 
$ kubectl create configmap nginx-frontend-conf --from-file=configuration/nginx/
$ kubectl create -f deployments/frontend.yaml
$ kubectl create -f deployments/service-frontend.yaml
$ 
Quizz : Comment le frontend a acc√®s au service auth et hello ? 


##==##
<!-- .slide:-->

# TP : C‚Äôest les soldes ! 


##==##
<!-- .slide:-->

# TP : Scaling Deployment


On va maintenant augmenter le nombre de pod 

$ kubectl scale deployments hello --replicas=3
$ kubectl describe deployments hello
$ kubectl get pods
$ kubectl get replicasets



Notes:
combien 



##==##
<!-- .slide:-->

# TP : Scaling Frontend



Editer le service frontend pour scaler √† 2 replicas
$ vim manifests/app/deployments/frontend.yaml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 2
  ‚Ä¶ 
$ kubectl apply -f deployments/frontend.yaml



##==##
<!-- .slide:-->

service hello


service auth


##==##
<!-- .slide: class="with-code" -->

```
Versions Kubernetes& apiVersion

```

Notes:
Chaque 
ressources
 Kubernetes a ses propres 
apiVersion

Les 
apiVersion
 √©voluent √† chaque 
release
 Kubernetes



##==##
<!-- .slide:-->

# Versions des ressources


https://github.com/benjah1/k8s-apiversion-matrix/blob/master/matrix.md


![](./images/g41f33631ff_0_265.png)

Notes:
Jusqu‚Äô√† maintenant avons abord√© les principales ressources Kubernetes :

Pod, ReplicaSet, 
Deployment, Service et Ingress



Voyons maintenant comment les faire vivre



##==##
<!-- .slide:-->

# Rolling upgrade


##==##
<!-- .slide:-->

# Rolling upgrade 1/4


![](./images/g3f3310ef84_0_1030.png)

Notes:
Etat initial : 4 Pods v1 r√©partis sur 3 noeuds



On va modifier la configuration du Deployment pour utiliser une autre version du pod (changement d‚Äôimage docker, de configuration, ‚Ä¶)



Le DeploymentController (le A sur le master) va r√©agir √† ce changement de configuration



##==##
<!-- .slide:-->

# Rolling upgrade 2/4


![](./images/g3f3310ef84_0_1035.png)

Notes:
Le DeploymentController va instancier un nouveau Pod ‚Äúv2‚Äù et attendre qu‚Äôil soit fonctionnel.

D√®s que ce nouveau Pod est pr√™t, le Service va lui envoyer des requ√™tes.

L‚Äôancien Pod ne re√ßoit plus de nouvelles requ√™tes, il va √™tre arr√™t√© puis supprim√© par le DeploymentController.



##==##
<!-- .slide:-->

# Rolling upgrade 3/4


![](./images/g3f3310ef84_0_1040.png)

Notes:
Chaque Pod est ainsi remplac√© par la nouvelle version, un par un.



##==##
<!-- .slide:-->

# Rolling upgrade 4/4


![](./images/g3f3310ef84_0_1045.png)

Notes:
√Ä la fin du processus de rolling-upgrade, il ne reste que des pods v2.

Il n‚Äôy a pas eu d‚Äôinterruption de service.



##==##
<!-- .slide: class="with-code" -->

```
Mise √† jour progressive (rolling-upgrade) sans interruption de service
G√©r√©e par le Deployment

Remplacer nginx par Apache httpd :

$ kubectl edit deployment nginx --record
‚Äúimage: nginx:alpine‚Äù ‚áí ‚Äúimage: httpd:alpine‚Äù


```

# Mise √† jour progressive


Notes:
Fonctionnalit√© native Kubernetes au niveau d‚Äôun d√©ploiement.



Regarder la mise √† jour dans le visualizer

V√©rifier que nginx a bien √©t√© remplac√© par apache dans le navigateur.




Edition en direct de la spec du Deployment

Sortir de vim avec ‚Äú:wq‚Äù







##==##
<!-- .slide: class="with-code" -->

```
$ kubectl rollout history deployment nginx
REV  CHANGE-CAUSE1    kubectl apply --filename=deployment.yaml2    kubectl edit deployment nginx3    kubectl set image deploy nginx nginx=nginx:stable-alpine

$ kubectl rollout undo deployment --to-revision=1

```

# Annuler une mise √† jour progressive


Notes:
Le nombre de r√©vision conserv√©es est contr√¥le par la propri√©t√© 
.spec.revisionHistoryLimit
, 10 par d√©faut.

Ce sont les ressources ‚ÄúReplicaSet‚Äù qui sont conserv√©s dans etcd, avec 
replicas=0
 pour les anciennes versions.







##==##
<!-- .slide:-->

# Mise √† l‚Äô√©chelle automatique


##==##
<!-- .slide:-->

# Horizontal Pod Autoscaler


AverageCPU Usage %


![](./images/g3f3310ef84_0_1082.png)

Notes:
L‚Äô
Horizontal Pod Autoscaler
 est un Controller Kubernetes responsable de

modifier en temps r√©el le nombre de replicas
 d‚Äôun ReplicationController / Deployment

en fonction de la charge CPU
 moyenne de l‚Äôensemble de pods de ce RC/D√©ployment.





https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
 
https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/
 



##==##
<!-- .slide: class="with-code" -->

```

D√©clarer un Horizontal Pod scaler en CLI :
$ kubectl autoscale deployment nginx \
    --cpu-percent=50 --min=1 --max=10



```

# Horizontal Pod Autoscaler


Notes:
On peut d√©clarer un horizontal pod autoscaler en cli ou via l‚Äôapi, exemple via l‚Äôapi page suivante. 

Le HPA va agir sur le deployment pour monter ou descendre le nombre de pod en fonction du pourcentage de cpu consomm√©. 



D‚Äôautres m√©triques peuvent √™tre utilis√©s. 



##==##
<!-- .slide: class="with-code" -->

# Horizontal Pod Autoscaler



```
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: helloweb
spec:
  maxReplicas: 10
  minReplicas: 1
  targetCPUUtilizationPercentage: 50
  scaleTargetRef:
    apiVersion: extensions/v1beta1
    kind: Deployment
    name: helloweb

```

##==##
<!-- .slide:-->

# TP : Autoscaling


##==##
<!-- .slide:-->

Les tests vont s‚Äôeffectuer sur l‚Äôimage docker k8s.gcr.io/hpa-example.
Cette image contient un apache+php. La home page de cette apache est un script php qui calcule les racines carr√©es des nombres de 1 √† 1 million.
L‚Äôappel √† cette page va provoquer une forte consommation CPU.




# TP : Autoscaling


Notes:
Version 
imp√©rative
 de la commande 
versus
 version 
d√©clarative
 par fichier yaml.



##==##
<!-- .slide:-->

# TP : Autoscaling


Cr√©er un deployment 
$ kubectl create -f deployments/hpa-example.yaml
Exposer le deployment 
$ kubectl expose deployment/hpa-example --port 80
Cr√©er un hpa sur le deployment
$ kubectl autoscale deployment hpa-example --cpu-percent=50 --min=1 --max=10


##==##
<!-- .slide:-->

Vous lancerez ensuite un Pod interactif pour charger l‚ÄôApache :
$ kubectl run -ti load-generator --image=busybox /bin/sh
Dans laquelle vous ex√©cuterez la boucle d‚Äôappels :
$ while true;
do wget -q -O- http://hpa-example;
done


# TP : Autoscaling



##==##
<!-- .slide:-->

# TP : Autoscaling


V√©rifier l‚Äô√©tat de l‚Äôobjet HPA
$ kubectl get hpa/hpa-example 
Arr√™ter la commande wget 
Attendez 5 minutes
V√©rifier √† nouveau l‚Äô√©tat du HPA 
Que constatez-vous ? 


##==##
<!-- .slide:-->

# Ingress


##==##
<!-- .slide:-->

Point d‚Äôentr√©e unique du cluster

HTTP (port 80) et HTTPS (port 443)

Gestion des certificats SSL



# Ingress


##==##
<!-- .slide:-->

# Ingress


www.sfeir.com/


lemag.sfeir.com


www.sfeir.com/formation/institute/


     HTTP (80)     HTTPS (443)


##==##
<!-- .slide: class="with-code" -->

```
apiVersion: v1kind: Ingressmetadata:  name: nginxspec:  rules:  - http:      paths:      - path: /
        backend:          serviceName: nginx          servicePort: http

```

# Ingress yaml


Notes:
Les r√®gles permettent, avec l‚Äôimpl√©mentation actuelle, de router le traffic selon :

le hostname (en-t√™te HTTP ‚Äú
Host:
‚Äù)

le path HTTP (√† partir du 
/
 suivant le nom+port de serveur dans l‚Äôurl)



##==##
<!-- .slide: class="with-code" -->

# Ingress


```
$ kubectl get ingressNAME      HOSTS     ADDRESS     PORTS     AGEnginx     *         localhost   80        3m$ kubectl get ingress -o yaml nginx
apiVersion: extensions/v1beta1kind: Ingress
metadata:
  name: nginx...

```

##==##
<!-- .slide: class="with-code" -->

# Ingress


```
Impl√©mentation par d√©faut √† base de nginx

√Ä activer sur minikube (minikube addons enable ingress)

√Ä d√©ployer manuellement sous Docker-for-Desktop
https://kubernetes.github.io/ingress-nginx/deploy/

```

##==##
<!-- .slide:-->

# Demo : Ingress


##==##
<!-- .slide:-->

# R√©capitulatif


![](./images/g3f3310ef84_0_159.png)

Notes:
Jusqu‚Äô√† maintenant avons abord√© les principales ressources Kubernetes :

Pod, ReplicaSet, 
Deployment, Service et Ingress



Voyons maintenant comment les faire vivre



##==##
<!-- .slide:-->

End of Day 1


##==##
<!-- .slide:-->

Day 2


##==##
<!-- .slide:-->

# Stocker des donn√©es


##==##
<!-- .slide:-->

emptyDir
gcePersistentDisk
hostPath


Juste un dossier mont√© dans un/des containers
Associ√© √† la vie du Pod, survit au restart des containers

Nombreuses impl√©mentations :


# Volumes


persistentVolumeClaim
configMap / secret


##==##
<!-- .slide: class="with-code" -->

```
Volume vide, cr√©√© au d√©marrage d‚Äôun pod, supprim√© avec la suppression du Pod.

volumes:  - name: cache-volume    emptyDir: {}

```

# emptyDir


Notes:
Utile pour passer des fichiers d‚Äôun container √† l‚Äôautre ou d‚Äôun initContainer √† un container.



##==##
<!-- .slide:-->

PersistentDisk GCE
Il doit √™tre dans le m√™me projet et la m√™me zone que les VM du cluster
Il n‚Äôest pas supprim√© √† la suppression du pod



# gcePersistentDisk


##==##
<!-- .slide:-->

volumes:  - name: test-volume    gcePersistentDisk:      pdName: my-data-disk      fsType: ext4


# gcePersistentDisk


Notes:
Il s‚Äôagit d‚Äôun disque compute engine classique. 



Il NE peut √™tre monter en √©criture que sur UN SEUL node √† la fois, mais il peut √™tre monter en lecture sur plusieurs nodes.



##==##
<!-- .slide: class="with-code" -->

```
Monte dans le container un dossier du noeud sur lequel le Pod s‚Äôex√©cute
volumes:  - name: test-volume    hostPath:      # directory location on host      path: /data      # this field is optional      type: Directory


```

# hostPath


Notes:
Il s‚Äôagit d‚Äôun disque mont√© sur une VM du cluster, mais il faut que le pod tourne sur le noeud o√π le volume a √©t√© cr√©√©. Peu recommand√©.



##==##
<!-- .slide:-->

L‚Äôadmin cr√©e une ressource PersistentVolume associ√©e √† une espace de stockage
Le pod r√©clame du disque avec un PersistentVolumeClaim


# persistentVolume & persistentVolumeClaim


Notes:
PersistentVolumeClaim fait r√©f√©rence au PersistentVolume cr√©√© pr√©c√©demment



##==##
<!-- .slide: class="with-code" -->

```
kind: PersistentVolumeapiVersion: v1metadata:  name: task-pv-volume  labels:    type: localspec:  storageClassName: manual  capacity:    storage: 10Gi  accessModes:    - ReadWriteOnce  hostPath:    path: "/mnt/data"

```

# persistentVolume


Notes:
Ce persistent volume va cr√©√© un espace r√©serv√© de 10 Go sur l‚Äôun des noeuds du cluster. 



##==##
<!-- .slide:-->

kind: PersistentVolumeClaimapiVersion: v1metadata:  name: task-pv-claimspec:  storageClassName: manual  accessModes:    - ReadWriteOnce  resources:    requests:      storage: 3Gi


# persistentVolumeClaim


##==##
<!-- .slide:-->

kind: PodapiVersion: v1metadata:  name: task-pv-podspec:  volumes:    - name: task-pv-storage      persistentVolumeClaim:       claimName: task-pv-claim


# Pod


 containers:    - name: task-pv-container      image: nginx      ports:        - containerPort: 80          name: "http-server"      volumeMounts:        - mountPath: "/usr/share/nginx/html"          name: task-pv-storage


##==##
<!-- .slide:-->

# TP : Volumes


##==##
<!-- .slide:-->

# TP : Volume


Objectif : d√©ployer un wordpress + MySql persistant 
G√©n√©rer un secret avec un mot de passe 
$ kubectl create secret generic mysql --from-literal=password=$(openssl rand -hex 12)



##==##
<!-- .slide:-->

# TP : Volume


Cr√©er les volumes 
$ kubectl apply -f volume/mysql-volumeclaim.yaml
$ kubectl apply -f volume/wordpress-volumeclaim.yaml
Cr√©er la base mysql
$ kubectl apply -f volume/mysql.yaml
$ kubectl apply -f volume/mysql-service.yaml


##==##
<!-- .slide:-->

# TP : Volume


Cr√©er l‚Äôinstance Wordpress et exposer le service
$ kubectl apply -f volume/wordpress.yaml
$ kubectl apply -f volume/wordpress-service.yaml
Acc√©der √† votre wordpress
$ kubectl get service wordpress -o jsonpath="{.status.loadBalancer.ingress[0].ip}" 



##==##
<!-- .slide:-->

# TP : Volume


D√©truiser les pods mysql et wordpress
$ k delete pods -l="app=mysql"
$ k delete pods -l="app=wordpress"
Que se passe-t-il ? (un peu de patience quand m√™me ;) )


##==##
<!-- .slide:-->

# TP : Volume Nettoyage


$ kubectl delete service wordpress
$ kubectl delete deployment wordpress
$ kubectl delete pvc wordpress
$ kubectl delete service mysql
$ kubectl delete deployment mysql
$ kubectl delete pvc mysql



##==##
<!-- .slide:-->

# Gestion avanc√©e de Pods


##==##
<!-- .slide:-->

# DaemonSet : un Pod par noeud


Agent de monitoring
Relais de logs
Agent de storage (client glusterfs, ‚Ä¶)


agent-abcd


agent-efgh


agent-ijkl


Notes:
Un objet DaemonSet va ex√©cuter un pod par node, utilis√© surtout pour des t√¢ches d‚Äôadministration/ monitoring du cluster.



##==##
<!-- .slide:-->

# StatefulSet : un Volume par Pod


Pour les applis qui ont besoin de garder un √©tat
Kube r√©associe les Pods aux volumes persistants
Dns : <name>-{0..N-1}.<service>.<ns>.svc.cluster.local


myapp-0


myapp-1


myapp-2


myvol-0


myvol-1


myvol-2


Notes:
Les pods sont num√©rot√©s dans l‚Äôordre, et les pods r√©assign√©s √† ce volumes au red√©marrage.

Quelques contraintes : les pods sont d√©marr√©s et √©teints dans l‚Äôordre de d√©marrage



##==##
<!-- .slide:-->

Job
Batch lanc√© une seule fois
Kubernetes ne relance pas les Pods de ce type sauf code retour en erreur
CronJob 
Job d√©clench√© r√©guli√®rement selon une expression cron : "*/1 * * * *"


# Jobs


Notes:
Utile pour lancer un batch de fa√ßon r√©gulier. 



##==##
<!-- .slide:-->

# Template


##==##
<!-- .slide:-->

# Probl√®me


![](./images/g3404c40d58_0_11.png)

Notes:
Un d√©ploiement peut devenir rapidement tr√®s verbeux, tr√®s long, difficile √† maintenir 



##==##
<!-- .slide:-->

# Probl√®mes


yaml verbeux
beaucoup de code en commun entre 2 d√©ploiements
besoin de d√©ployer un ‚Äúpackage‚Äù


Notes:
Probl√®me de syntaxe avec le yaml

Il y a g√©n√©ralement entre 2 applications similaires, peu de diff√©rence

Plut√¥t que de d√©ployer un deployment + un service  + un configmap + ... , on a besoin de d√©ployer un package complet.





##==##
<!-- .slide: class="with-code" -->

# Template : Helm


```
Le plus populaire aujourd‚Äôhui
Templating sur la base du moteur de template go
Chart
Repository de chart
Tiller et √©tat
$ helm repo update
$ helm install stable/mysql


```

##==##
<!-- .slide: class="with-code" -->

# Helm : chart


```
wordpress/
  Chart.yaml          # A YAML file containing information about the chart
  LICENSE             # OPTIONAL: A plain text file containing the license for the chart
  README.md           # OPTIONAL: A human-readable README file
  requirements.yaml   # OPTIONAL: A YAML file listing dependencies for the chart
  values.yaml         # The default configuration values for this chart
  charts/             # A directory containing any charts upon which this chart depends.
  templates/          # A directory of templates that, when combined with values,
                      # will generate valid Kubernetes manifest files.
  templates/NOTES.txt # OPTIONAL: A plain text file containing short usage notes

```

Notes:
L‚Äôorganisation d‚Äôun chart helm 



##==##
<!-- .slide: class="with-code"  class="with-code" -->

# Helm : Template


```
[deployment.yaml]
spec:
  containers:
    - name: deis-database
      image: postgres:{{.Values.dockerTag}}
      imagePullPolicy: {{.Values.pullPolicy}}
      ports:
        - containerPort: 5432
      env:
        - name: DATABASE_STORAGE
          value: {{default "minio" .Values.storage}}

```

```
[Values.yaml]

# The tag for the docker image.
dockerTag: 7.1.2
pullPolicy: Always
# The storage backend, whose default is set to "minio"
# storage: 

```

Notes:
Values.yaml

imageRegistry
: The source registry for the Docker image.

dockerTag
: The tag for the docker image.

pullPolicy
: The Kubernetes pull policy.

storage
: The storage backend, whose default is set to 
"minio"





##==##
<!-- .slide:-->

# Template : Kustomize


Challenger
Plain yaml 
Pas de ‚Äútemplating‚Äù
Mais plut√¥t du ‚Äúpatch‚Äù
Int√©gr√© √† kubectl   ‚Äòkubectl apply -k ...‚Äô


##==##
<!-- .slide:-->

# Kustomize


![](./images/g3404c40d58_0_23.png)

Notes:
Un exemple avec kustomize 



##==##
<!-- .slide:-->

# S√©curit√©


##==##
<!-- .slide:-->

# RBAC






Role-based Access Control
Contr√¥le l‚Äôacc√®s √† l‚Äôapi pour les utilisateurs
Gestion des droits par d√©faut dans kubernetes

Role / ClusterRole
RoleBinding / ClusterRoleBinding






##==##
<!-- .slide: class="with-code"  class="with-code" -->

# RBAC


```
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: default
  name: pod-reader
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "watch", "list"]
  

```

```
...
kind: RoleBinding
subjects:
- kind: User
  name: jane # Name is case sensitive
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: pod-reader 
  apiGroup: rbac.authorization.k8s.io

```

##==##
<!-- .slide:-->

# RBAC : r√¥les par d√©faut


view
edit
admin
cluster-admin


Notes:
view : permet d‚Äôavoir une vue readonly sur la plupart des objets d‚Äôun namespace, ne permet pas de voir les secrets, ou les objets roles et rolebinding

edit : permet de modifier la plupart des objets, mais pas les objet roles rolebinding

admin : permet d‚Äô√©diter les objets + les droits sur un namespace mais pas les quotas

cluster-admin : tous les droits



##==##
<!-- .slide:-->

# Monitoring


##==##
<!-- .slide:-->

# Monitoring : Prometheus


Pull de m√©triques sur les applications.
Inverse la logique de monitoring.


Notes:
Traditionnellement, les applications envoient leurs m√©triques √† des syst√®mes centralis√©s. Cette fa√ßon de faire rend l'application responsable de cet envoi des m√©triques. Dans une architecture cloud moderne, la logique est invers√©e pour faciliter ce monitoring.

Prometheus est un des outils qui impl√©mente cette logique. Il va interroger p√©riodiquement une URL sur vos applications pour lire vos m√©triques, puis envoyer ces m√©triques √† une base de donn√©es sp√©cialis√©e dans les s√©ries temporelles. Cette base pourra √™tre interrog√©e pour la surveillance des applications.



##==##
<!-- .slide:-->

# M√©thodes d‚Äôinstallation de clusters


##==##
<!-- .slide:-->

# Installation du k8s : d√©veloppement 


Docker for (Mac|Windows)
Installation manuelle ‚Äúk8s the hard way‚Äù
Quid du r√©seau? 
S√©curit√© first
Kops


Notes:
‚ÄúThe hard way‚Äú : pour les tr√®s courageux : comment installer un cluster k8s compl√©mente √† la main. (tr√®s int√©ressant pour comprendre k8s mais tr√®s peu recommand√©)

Docker for (Mac|Windows) : ‚Äúminikube‚Äù int√©gr√© dans la version desktop de docker (√† activer manuellement)

Kops : cli pour cr√©er un cluster k8s sur des VMs chez des cloud provider AWS, GCE 





##==##
<!-- .slide:-->

# Installation du cluster : production 


On premise 
Kubeadm
Cloud
GKE (Google)
AKS	(Azure)
EKS  (AWS)
OVH
‚Ä¶.


Notes:


Kubeadm : la version pr√©conis√©e aujourd‚Äôhui pour une installation on premise

Kubernetes Manag√© : GCP, Amazon, Azure, mais aussi Digital Ocean, OVH (and counting‚Ä¶)



##==##
<!-- .slide:-->

# Service mesh


##==##
<!-- .slide:-->

# Istio


Routage
S√©curit√©
Observabilit√©
En option sur GKE
CNCF 




Notes:
routage : canary release / circuit breaker

s√©curit√© : SSL entre les services, authentification, autorisation 

observabilit√© : remonte des m√©triques de connexion entre les services

cncf : le projet fait partie comme kubernetes de la cncf





##==##
<!-- .slide:-->

# Architecture micro-service


##==##
<!-- .slide:-->

Base de code : syst√®me de contr√¥le de version
D√©pendances : D√©clarez explicitement et isolez les d√©pendances
Configuration : Stockez la configuration dans l‚Äôenvironnement
Services externes : ressources rattach√©es
Build, release, run : S√©parez le build/packaging et d‚Äôex√©cution
Processus : Ex√©cutez l‚Äôapplication comme un ou plusieurs processus sans √©tat


# Les 12 facteurs des applications Cloud


Notes:
I. Base de codeUne base de code suivie avec un syst√®me de contr√¥le de version, plusieurs d√©ploiementsII. D√©pendancesD√©clarez explicitement et isolez les d√©pendancesIII. ConfigurationStockez la configuration dans l‚Äôenvironnement ===> ConfigMap, Secrets, Consul, ‚Ä¶

IV. Services externesTraitez les services externes comme des ressources attach√©esV. Build, release, runS√©parez strictement les √©tapes d‚Äôassemblage et d‚Äôex√©cutionVI. ProcessusEx√©cutez l‚Äôapplication comme un ou plusieurs processus sans √©tatVII. Associations de portsExportez les services via des associations de portsVIII. ConcurrenceGrossissez √† l‚Äôaide du mod√®le de processusIX. JetableMaximisez la robustesse avec des d√©marrages rapides et des arr√™ts gracieuxX. Parit√© dev/prodGardez le d√©veloppement, la validation et la production aussi proches que possibleXI. LogsTraitez les logs comme des flux d‚Äô√©v√®nementsXII. Processus d‚ÄôadministrationLancez les processus d‚Äôadministration et de maintenance comme des one-off-processes



##==##
<!-- .slide:-->

Associations de ports : Exportez les services via des associations de ports
VIII. Concurrence : Grossissez √† l‚Äôaide du mod√®le de processus
IX. Jetable : Maximisez la robustesse avec des d√©marrages rapides et des arr√™ts gracieux
X. Parit√© dev/prod : Gardez le d√©veloppement, la validation et la production aussi proches que possible
XI. Logs : Traitez les logs comme des flux d‚Äô√©v√®nements
XII. Processus d‚Äôadministration : Lancez les processus d‚Äôadministration et de maintenance comme des one-off-processes



# Les 12 facteurs des applications Cloud


Notes:
I. Base de codeUne base de code suivie avec un syst√®me de contr√¥le de version, plusieurs d√©ploiementsII. D√©pendancesD√©clarez explicitement et isolez les d√©pendancesIII. ConfigurationStockez la configuration dans l‚Äôenvironnement ===> ConfigMap, Secrets, Consul, ‚Ä¶

IV. Services externesTraitez les services externes comme des ressources attach√©esV. Build, release, runS√©parez strictement les √©tapes d‚Äôassemblage et d‚Äôex√©cutionVI. ProcessusEx√©cutez l‚Äôapplication comme un ou plusieurs processus sans √©tatVII. Associations de portsExportez les services via des associations de portsVIII. ConcurrenceGrossissez √† l‚Äôaide du mod√®le de processusIX. JetableMaximisez la robustesse avec des d√©marrages rapides et des arr√™ts gracieuxX. Parit√© dev/prodGardez le d√©veloppement, la validation et la production aussi proches que possibleXI. LogsTraitez les logs comme des flux d‚Äô√©v√®nementsXII. Processus d‚ÄôadministrationLancez les processus d‚Äôadministration et de maintenance comme des one-off-processes



##==##
<!-- .slide:-->

Creer un conteneur/pod/deploiement pour acceder au service hello-namespaces dans le namespace ‚Äútony‚Äù
trigger le endpoint /hello/mon-nom sur le port 443


# TP: Namespaces


Notes:
I. Base de codeUne base de code suivie avec un syst√®me de contr√¥le de version, plusieurs d√©ploiementsII. D√©pendancesD√©clarez explicitement et isolez les d√©pendancesIII. ConfigurationStockez la configuration dans l‚Äôenvironnement ===> ConfigMap, Secrets, Consul, ‚Ä¶

IV. Services externesTraitez les services externes comme des ressources attach√©esV. Build, release, runS√©parez strictement les √©tapes d‚Äôassemblage et d‚Äôex√©cutionVI. ProcessusEx√©cutez l‚Äôapplication comme un ou plusieurs processus sans √©tatVII. Associations de portsExportez les services via des associations de portsVIII. ConcurrenceGrossissez √† l‚Äôaide du mod√®le de processusIX. JetableMaximisez la robustesse avec des d√©marrages rapides et des arr√™ts gracieuxX. Parit√© dev/prodGardez le d√©veloppement, la validation et la production aussi proches que possibleXI. LogsTraitez les logs comme des flux d‚Äô√©v√®nementsXII. Processus d‚ÄôadministrationLancez les processus d‚Äôadministration et de maintenance comme des one-off-processes



