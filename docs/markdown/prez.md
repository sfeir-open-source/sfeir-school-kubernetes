<!-- .slide: class="first-slide"  sfeir-level="1"  sfeir-techno="kub" -->

# Kubernetes, les fondamentaux

##==##

<!-- .slide: class="speaker-slide" -->

# Votre h√¥te pour la journ√©e

![speaker](./assets/images/julien.png)

![compagny](./assets/images/logo-sfeir.png)

![badge first-badge](./assets/images/certif-cloud-archi.png)

![badge second-badge](./assets/images/certif-cloud-network.png)

![badge third-badge](./assets/images/certif-cloud-sec.png)

<h2> Julien <span>Furgerot</span></h2>

### GCP Trainer

<!-- .element: class="icon-rule icon-first" -->

##==##

<!-- .slide:-->

![center h-800](./assets/images/tour-de-table.png)

##==##

<!-- .slide:-->

# Pr√©-requis

Pour assister √† cette formation, il est n√©cessaire de conna√Ætre et comprendre les notions de base associ√©es aux conteneurs. Vous √™tes capable de construire une image (par exemple avec un Dockerfile), lancer un conteneur, l'arr√™ter, inspecter ses logs.

- Un navigateur
- Un compte GCP

Notes:
Note : ce slide sera √† exporter dans un document √† part pour les SfeirSchools

Sondage :

qui conna√Æt docker ?

qui conna√Æt un orchestrateur de container ?

qui conna√Æt kubernetes ?

##==##

<!-- .slide: data-type-show="hide"-->

# Self-link

https://bit.ly/sfeir-school-k8s-2024

##==##

<!-- .slide:-->

# Agenda

- Rappel sur Docker
- Kubernetes : les origines
- Premier aper√ßu
- Architecture interne
- Getting started
- Configuration d‚Äôune application
- Mise √† l‚Äô√©chelle et mise √† jour
- Stocker des donn√©es
- Gestion avanc√©e de Pods
- M√©thodes d‚Äôinstallation de clusters

##==##

<!-- .slide: class="transition"-->

# Rappels sur Docker

##==##

<!-- .slide:-->

# Qu‚Äôest-ce que Docker ?

Docker permet de **packager une application**

<!-- .element: class="center" -->

avec **l‚Äôensemble de ses d√©pendances**

<!-- .element: class="center" -->

dans une **unit√© standardis√©e** pour le d√©ploiement de logiciels :

<!-- .element: class="center" -->

<br>

les **CONTAINERS**

<!-- .element: class="center" -->

<br>
Notes:
Docker en une phrase

##==##

<!-- .slide:-->

# Utilisation de Docker

![center h-800](./assets/images/docker-utilisation.svg)

Notes:
En informatique, le container peut aussi embarquer une grande vari√©t√© de contenus.

Il s‚Äôex√©cute √† l‚Äôidentique dans plusieurs environnements sans √™tre modifi√©.

##==##

<!-- .slide: class="flex-row" -->

# VM vs Containers

![h-600](./assets/images/vms.png)
![h-600](./assets/images/containers.png)

Notes:
Avantages de Docker :

permet un partage des ressources efficace

plus simple pour g√©rer des environnements d‚Äôex√©cution vari√©s

Avantages des full VMs :

isolation GARANTIE puisqu‚Äôil n‚Äôy a aucune communications entre l'h√¥te et la machine virtuelle

acc√®s au mat√©riel plus simple

##==##

<!-- .slide:-->

# Rappel sur les containers

![center h-800](./assets/images/docker-recap.png)

Notes:
Le Dockerfile :

contient les instructions pour construire l‚Äôimage

Les images :

contiennent les fichiers n√©cessaires √† l‚Äôex√©cution de votre application : binaires, d√©pendances, configuration, ‚Ä¶

sont immuable

Les containers :

isolent l‚Äôex√©cution de processus

dans le syst√®me de fichier de l‚Äôimage

au niveau r√©seau

sont con√ßus pour √™tre √©ph√©m√®res

La registry :

permet le partage d‚Äôimages

Docker Hub, Google Container Registry, ...

##==##

<!-- .slide: class="transition"-->

# Kubernetes : les origines

##==##

<!-- .slide:-->

# Google : 20 ans de containers

![h-800 center](./assets/images/google-containers.png)

Notes:
Google, 20 ans d‚Äôexp√©rience sur l‚Äôorchestration de containers

Borg = c++, ~2003

##==##

<!-- .slide:-->

# Cloud Native Computing Foundation

![center h-800](./assets/images/cloud-native-foundation.svg)

[landscape.cncf.io](landscape.cncf.io)

<!-- .element: class="credits" -->

Notes:
Kubernetes r√©ecrit en Go par les Googlers

Offert √† la Cloud Native Computing Foundation en 2015

Kubernetes n‚Äôest qu‚Äôun outils parmi tous les outils pouvant vous aider √† cr√©er des architectures ‚Äú
Cloud-Natives
‚Äù

La
Fondation Linux
(en anglais
Linux Foundation
) est un
consortium
√† but non lucratif fond√© le
21

janvier

2007
, a pour mission de prot√©ger et standardiser
Linux
en procurant les ressources et services centralis√©s, la Linux Foundation regroupe 70 membres.

##==##

<!-- .slide: class="two-column" -->

# Kubernetes, pour quoi faire ?

![w-600](./assets/images/g3f0c37370d_0_502.png)

##--##

<br><br>

- Lancer 5 containers bas√©s sur l‚Äôimage redis:4.0
- Mettre en place un load-balancer interne au cluster pour servir ces 5 containers
- Lancer 10 containers webapp:1.0
- Mettre en place un load-balancer public pour permettre d‚Äôacc√©der aux containers de l‚Äôext√©rieur du cluster
- Augmenter le nombre de containers webapp pendant les soldes üòâ
- Continuer √† servir les requ√™tes pendant la mise √† jour vers webapp:2.0

Notes:
Orchestration de containers dans un cluster de machines

‚Äúbarreur d'un navire‚Äù en grec

##==##

<!-- .slide: class="with-code" -->

# Mais aussi

- Mise √† l‚Äô√©chelle automatique
- D√©ploiement ‚ÄúBlue/green‚Äù et ‚ÄúCanary‚Äù
- Ex√©cution de traitements unitaires ou r√©p√©t√©s
- Prioriser les t√¢ches en cas de manque de ressources sur le cluster
- Ex√©cuter des services qui persistent des donn√©es sur disque
- Contr√¥ler l‚Äôacc√®s aux diff√©rentes ressources
- Automatiser les t√¢ches complexes (‚Äúoperators‚Äù)

Notes:
Orchestration de containers dans un cluster de machines

##==##

<!-- .slide: data-type-show="hide"-->

# Question 1

texte

choix 1
choix 2
choix 3

##==##

<!-- .slide: data-type-show="hide"-->

# Question 1

texte

choix 1
choix 2
choix 3

##==##

<!-- .slide: data-type-show="hide"-->

# Question 1

Quelle est la diff√©rence entre une VM et un container?

Un container fait tourner un nouvel OS, √©mule du mat√©riel physique tandis qu‚Äôune VM partage les m√™mes OS et noyau que son h√¥te et r√©alise une virtualisation au niveau syst√®me
Une VM fait tourner un nouvel OS, √©mule du mat√©riel physique tandis qu‚Äôun container partage les m√™mes OS et noyau que son h√¥te et r√©alise une virtualisation au niveau syst√®me
Les deux sont des moyens de virtualisation mais le container peut tourner sur tous les OS contrairement √† la VM

##==##

<!-- .slide: data-type-show="hide"-->

# Question 1

Quelle est la diff√©rence entre une VM et un container?

Une VM fait tourner un nouvel OS, √©mule du mat√©riel physique tandis qu‚Äôun container partage les m√™mes OS et noyau que son h√¥te et r√©alise une virtualisation au niveau syst√®me
Un container fait tourner un nouvel OS, √©mule du mat√©riel physique tandis qu‚Äôune VM partage les m√™mes OS et noyau que son h√¥te et r√©alise une virtualisation au niveau syst√®me
Les deux sont des moyens de virtualisation mais le container peut tourner sur tous les OS contrairement √† la VM

##==##

<!-- .slide: class="transition"-->

# Premier aper√ßu

##==##

<!-- .slide:-->

# Cluster

![center h-800](./assets/images/cluster.png)

Notes:
Un cluster est un ensemble de machines qui collaborent entre elles.

Sur Kubernetes, on distingue le master des noeuds (nodes).

##==##

<!-- .slide: -->

# Ex√©cuter des applications

![center h-800](./assets/images/cluster-2.png)

Notes:
Le master est responsable de d‚Äôessentiel de la partie ‚Äúcontr√¥le‚Äù (control plane) du cluster.

Les noeuds (nodes) sont responsables de l‚Äôex√©cution des applications.

Les fonctions de master et de node sont habituellement d√©ploy√©es sur des machines diff√©rentes.

En dev ces deux fonctions peuvent √™tre d√©ploy√©es sur une unique machine (ex: Minikube)

##==##

<!-- .slide:-->

# Node

![center h-800](./assets/images/cluster-node.png)

Notes:
Les noeuds ex√©cutent les applications packag√©es dans des containers regroup√©s dans des ‚ÄúPods‚Äù.

L‚Äôex√©cution des Pods est g√©r√©e par les kubelet.

Kubelet est un process qui pilote l'engine docker pour deployer les resources

##==##

<!-- .slide:-->

# Pod

![center-h-600](./assets/images/cluster-pod.png)

Notes:
Pod :

1 ou plusieurs containers

en g√©n√©ral un seul

partageant :

une seule IP

un ou plusieurs volumes

##==##

<!-- .slide:-->

# Patterns

![center h-700](./assets/images/patterns.png)

Notes:
patterns :

sidecar

file-puller + webserver

log forwarder

adapter

nginx + php_fpm

ambassador

##==##

<!-- .slide:-->

# Orchestrer les pods

![center h-800](./assets/images/orchestration-pod.png)

Notes:
Objectif du slide :
premier aper√ßu des objets Kubernetes.
Ne pas d√©tailler plus que les commentaires ci-dessous !

On parle d‚Äôobjets Kubernetes, de
ressources
.

Chaque ressource repr√©sente une API.

ReplicaSet :

maintient le nombre demand√© de pod identiques

la plupart du temps cr√©√© par le Deployment

Deployment :

g√®re le cycle de vie des pods : versions, mise √† jour continue (sans interruption)

Service :

load-balancer interne vers un ensemble de pods

accessible par IP ou par dns interne

Ingress :

Point d‚Äôentr√©e HTTP(S)

routage par Path ou par Virtualhost

Namespace :

pour isoler des ressources

On reverra plus en d√©tails ces objets et d‚Äôautres : ConfigMap, DaemonSet, Job, ...

##==##

<!-- .slide: data-type-show="hide"-->

# Question 2

Quelles sont les deux types de machines que l‚Äôon trouve dans un cluster Kubernetes

Master et node
Deployment et pod
Kubelet et container

##==##

<!-- .slide: data-type-show="hide"-->

# Question 2

Quelles sont les deux types de machines que l‚Äôon trouve dans un cluster Kubernetes

Master et node
Deployment et pod
Kubelet et container

##==##

<!-- .slide: class="transition"-->

# Architecture interne

##==##

<!-- .slide:-->

# Master et Nodes

![center h-800](./assets/images/master-and-node.svg)

Notes:
Cluster :

1 ou plusieurs masters (
recommand√© pour la
HA - Haute Disponibilit√©)

1 ou plusieurs noeuds (
recommand√© pour la
HA - Haute Disponibilit√©)

Les ops passent des commandes au master

Les clients acc√®dent aux pods via les noeuds

##==##

<!-- .slide:-->

# ApiServer et etcd

![center h-800](./assets/images/apiserver-etcd.svg)

Notes:
ApiServer

Devs ou ops envoient la description de l‚Äô√©tat voulu des objets

Fichiers au format .yaml, transform√©s en json pour envoi sur l‚Äôapiserver

Objets stock√©s dans serveur etcd (serveur cl√©-valeur distribu√©)

##==##

<!-- .slide:-->

# Kubelet

![center h-800](./assets/images/kubelet.svg)

Notes:
Kubelet

Tourne sur les noeuds

Ex√©cute les pods

S‚Äôenregistre aupr√®s du master via l‚Äôapiserver

##==##

<!-- .slide:-->

# Controllers

![center h-800](./assets/images/controllers-node.svg)

Notes:
Controllers

Il y a plusieurs contr√¥leurs, un par type d‚Äôobjet Kubernetes

Ils ont chacun une responsabilit√© pr√©cise

Ils surveillent constamment la configuration de l‚Äôetcd

Ils r√©agissent au changement pour atteindre l‚Äô√©tat demand√©, en mettant √† jour la configuration via l‚Äôapiserver. Exemple :

le Scheduler met √† jour la configuration du pod pour lui assigner un noeud

le kubelet (lui-aussi un type de contr√¥leur) ex√©cute le pod demand√© puis enregistre son statut dans la configuration

##==##

<!-- .slide: class="two-column" -->

# R√©capitulatif

- Le Kubelet s‚Äôenregistre dans la config

- L‚Äôutilisateur envoi l‚Äô√©tat voulu sur etcd via l‚Äôapiserver

- Les contr√¥leurs et le Kubelet r√©agissent aux changements d‚Äô√©tats pour obtenir et maintenir l‚Äô√©tat voulu

##--##

![](./assets/images/gb710258689_0_72.png)

##==##

<!-- .slide: data-type-show="hide"-->

# Question 1

Quelle est la diff√©rence entre une VM et un container?

Un container fait tourner un nouvel OS, √©mule du mat√©riel physique tandis qu‚Äôune VM partage les m√™mes OS et noyau que son h√¥te et r√©alise une virtualisation au niveau syst√®me
Une VM fait tourner un nouvel OS, √©mule du mat√©riel physique tandis qu‚Äôun container partage les m√™mes OS et noyau que son h√¥te et r√©alise une virtualisation au niveau syst√®me
Les deux sont des moyens de virtualisation mais le container peut tourner sur tous les OS contrairement √† la VM

##==##

<!-- .slide:-->

# √Ä venir...

![center h-800](./assets/images/a-venir.png)

Notes:
Ce qu‚Äôon voit dans le chapitre suivant

##==##

<!-- .slide: data-type-show="hide"-->

# Question 3

Quel est le composant install√© sur tous les noeuds interagissant avec l‚ÄôAPI Server et g√©rant les containers cr√©√©s dans Kubernetes?

Kubelet
Docker
Systemd

##==##

<!-- .slide: data-type-show="hide"-->

# Question 3

Quel est le composant install√© sur tous les noeuds interagissant avec l‚ÄôAPI Server et g√©rant les containers cr√©√©s dans Kubernetes?

Kubelet
Docker
Systemd

##==##

<!-- .slide: data-type-show="hide"-->

# Question 4

Quel est le composant principal recevant toutes les requ√™tes de configuration et de modification de l‚Äô√©tat du cluster?

Kubelet
Kube-api-server
Kube-proxy

##==##

<!-- .slide: data-type-show="hide"-->

# Question 4

Quel est le composant principal recevant toutes les requ√™tes de configuration et de modification de l‚Äô√©tat du cluster?

Kubelet
Kube-api-server
Kube-proxy

##==##

<!-- .slide: data-type-show="hide"-->

# Question 5

Quel est le composant stockant l‚Äô√©tat des ressources du cluster?

Controllers
Master
ETCD

##==##

<!-- .slide: data-type-show="hide"-->

# Question 5

Quel est le composant stockant l‚Äô√©tat des ressources du cluster?

Controllers
Master
ETCD

##==##

<!-- .slide: data-type-show="hide"-->

# Question 6

Quel est le composant choisissant sur quel noeud se lance les pods?

Kubelet
Scheduler
Kube-proxy

##==##

<!-- .slide: data-type-show="hide"-->

# Question 6

Quel est le composant choisissant sur quel noeud se lance les pods?

Kubelet
Scheduler
Kube-proxy

##==##

<!-- .slide: data-type-show="hide"-->

# Question 7

Quel sont les composants permettant d‚Äôatteindre les √©tats voulus des ressources au sein du cluster?

Controllers
Kubelets
Nodes

##==##

<!-- .slide: data-type-show="hide"-->

# Question 7

Quel sont les composants permettant d‚Äôatteindre les √©tats voulus des ressources au sein du cluster?

Controllers
Kubelets
Nodes

##==##

<!-- .slide: data-type-show="hide"-->

# Question 8

Quel est le composant permettant le routage r√©seau au sein du cluster?

Kubelet
Scheduler
Kube-proxy

##==##

<!-- .slide: data-type-show="hide"-->

# Question 8

Quel est le composant permettant le routage r√©seau au sein du cluster?

Kubelet
Scheduler
Kube-proxy

##==##

<!-- .slide: class="transition"-->

# Getting started

##==##

<!-- .slide: class="transition-bg-sfeir-3"-->

# Cr√©ation de cluster

## Configuration de kubectl

##==##

<!-- .slide: data-type-show="hide" -->

Google Kubernetes Engine

D√©monstration en direct

# Cr√©er un cluster GKE

Notes:
Cr√©er un cluster en 3 clics depuis la Console GCP

Montrer les options disponibles depuis l‚Äôinterface graphique :

version Kubernetes

taille de VM

type d‚ÄôOS (COS ou Ubuntu)

activation Stackdriver Monitoring / Logging

‚Ä¶

Note : ne pas d√©montrer ici les avantages de GKE vs K8S natif

##==##

<!-- .slide:  -->

# kubectl

- Client Kubernetes en ligne de commande
- Astuces :
  - `kubectl version`
  - `kubectl completion -h`
  - `kubectl help <command>`
  - `kubectl explain [--recursive] <resource>`
  - `kubectl <verb> <resource>`

Notes:
Comme Docker, Kubernetes s‚Äôutilise en mode client-server.

Le client s‚Äôappelle ‚Äú
kubectl
‚Äù et appelle un composant du cluster appell√©
apiserver
.

kubectl int√®gre un m√©canisme pour installer l‚Äôauto-completion des commandes dans bash et zsh

On a aussi acc√®s √† la
doc des commandes
et au
format des ressources
avec les sous-commandes
help
et
explain
.

‚Ä¶ d√©taill√© dans la suite

##==##

<!-- .slide:-->

# ~/.kube/config

![center h-800](./assets/images/kube-config.svg)

Notes:
La configuration du client kubectl se fait dans le fichier ~/.kube/config

Ce fichier r√©pertorie les ‚Äúcontextes‚Äù, constitu√©es du couple Cluster et User, ainsi que le namespace courant sur ce cluster.

Sur GKE, la partie user est d√©l√©gu√©e √† un fournisseur d‚Äôauthentification impl√©ment√© par la commande
gcloud
du Cloud SDK.

##==##

<!-- .slide: class="with-code big-code" -->

# Contextes Kubernetes sur GKE

```
gcloud container clusters list

gcloud container clusters get-credentials \
   --zone europe-west1-a <mycluster>
```

<!-- .element: class="big-code" -->

Notes:
La seconde commande permet de cr√©er le contexte kubectl correspondant √† une instance GKE.

##==##

<!-- .slide: class="with-code two-column"  -->

# Changer de contexte

```
# Lister les contexts
$ kubectl config get-contexts
# Mettre √† jour les contexts
$ kubectl config set-context gke-dev
# Afficher current-context configuration
$ kubectl config view --minify=true
```

```
apiVersion: v1
kind: Config
current-context: gke-context
contexts:
- name: gke-context
  context:
    cluster: gke-cluster
    user: gke-user
clusters:
- name: gke-cluster
  cluster:
    server: https://12.34.56.78
```

##--##

<!-- .slide: class="with-code"  -->

<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>

```
users:
- name: gke-user
  user:
    auth-provider:
      name: gcp
      config:
        cmd-path: /usr/bin/gcloud
        cmd-args: config config-helper --format=json
        access-token: ya29.GlwXBuG[...]KcYlQ
        [...]
```

Notes:
Exemple de fichier de config de kubectl (~/.kube/config).

On y retrouve les contextes, les clusters, les utilisateurs

On voit aussi l‚Äôutilisation de gcloud comme fournisseur d‚Äôauthentification

##==##

# Aide-m√©moire des commandes de kubectl

- https://kubernetes.io/fr/docs/reference/kubectl/cheatsheet/

##==##

<!-- .slide: class="with-code" -->

# Utilitaires autour de kubectl

- https://kubernetes.io/docs/reference/kubectl/quick-reference/#kubectl-autocomplete

```
$ kubectl <TAB>		# auto-compl√©tion
$ k <TAB>				# alias
```

- https://github.com/ahmetb/kubectx

```
$ kubectx gke-dev		# changer de contexte
$ kubens kube-system	# modifier le ns du contexte courant
```

##==##

<!-- .slide: class="with-code" -->

# Utilitaires autour de kubectl

- ~800 kubectl aliases (bash/zsh)
  https://github.com/ahmetb/kubectl-aliases

`$ kgpo		# kubectl get pod`

- Shell prompt
  https://github.com/jonmosco/kube-ps1

  ![](./assets/images/command-minikube.png)

- https://github.com/mfuentesg/ksd

```
$ kubectl get secret <secret name> -o <yaml|json> | ksd
# D√©coder les √©l√©ments d‚Äôun secret en base 64
```

##==##

<!-- .slide:-->

# k9scli.io

![center h-800](./assets/images/k9scli.png)

Notes:
Installer l‚Äôextension Chrome suivante pour lire l‚Äôasciinema en live dans les slides :

https://chrome.google.com/webstore/detail/google-slides-asciinema/lbaccocfalidoaeacbpabonnljdndmdd

##==##

<!-- .slide: class="transition-bg-sfeir-3"-->

# Namespace

##==##

<!-- .slide:-->

# Namespace

- Espace de nom pour isoler les d√©ploiements
- Peut √™tre utilis√© pour s√©parer les environnements (soft multi-tenant)
- Le nom d‚Äôune ressource est unique au sein d‚Äôun namespace.
- Par d√©faut:
  - default
  - kube-system
  - kube-public

Notes:
nom de ressource unique dans un namespace
(mais pas au sein d‚Äôun cluster)

default : namespace de travail par d√©faut

kube-system : namespace pour les d√©ploiements internes au cluster

kube-public : namespace cr√©√© par d√©faut pour les ressources expos√©s publiquement

##==##

<!-- .slide: class="with-code" -->

# R√©cup√©rer les exercices

- Clonez le d√©p√¥t github suivant :

`git clone https://github.com/sfeir-open-source/sfeir-school-kubernetes.git`

- et placez-vous dans le d√©p√¥t clon√© :

`cd sfeir-school-kubernetes/steps`

Notes:
√Ä faire

dans Cloud Shell

OU

sur les machines locales si
git, kubectl et Cloud SDK
install√©s

##==##

<!-- .slide: class="transition-bg-sfeir-2"-->

# TP : Configurer kubectl

##==##

<!-- .slide: class="exercice"-->

# TP : Configurer kubectl

## LAB

<br>

1. !! Faites une sauvegarde du fichier ~/.kube/config !!

##==##

<!-- .slide: class="exercice with-code" -->

# TP : Configurer kubectl

## LAB

<br>

1. Installer gcloud & kubectl
1. Configurer kubectl

```shell
$ gcloud container clusters get-credentials <cluster> --zone <zone>
$ echo 'source <(kubectl completion bash)' >>~/.bashrc
```

<!-- .element: class="big-code" -->

##==##

<!-- .slide: class="exercice with-code" -->

# TP : Configurer kubectl

## LAB

```shell
$ alias k=‚Äùkubectl‚Äù
$ kubectl get nodes # Lister les noeuds du cluster
$ kubectl get namespace # Lister les espaces de nom
$ kubectl config set-context --current
    --namespace=<insert-namespace-name-here>
$ kubectl config view --minify=true
```

<!-- .element: class="big-code" -->

##==##

<!-- .slide: class="transition-bg-sfeir-3"-->

# Manifest

Notes:
Configuration files - Written in YAML or JSON, these files describe the desired state of your application in terms of Kubernetes API objects. A file can include one or more API object descriptions (manifests). (See the example YAML from the stateless app).

Fichier de configuration : √©crit en yaml ou json, ce fichier d√©crit l‚Äô√©tat d√©sir√© d‚Äôune ressource Kubernetes. Dans un fichier, on peut inclure une ou plusieurs descriptions d‚Äôobjets

##==##

<!-- .slide: class="with-code max-height" -->

# Manifest

```yaml
kind: Pod
apiVersion: v1
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  containers:
    - name: nginx
      image: nginx:alpine
status: ...
```

<!-- .element: class="big-code"-->

Notes:
kind : type de resource que l‚Äôon va cr√©√©

apiVersion: version de l‚Äôapi de la ressource que l‚Äôon veut cr√©√©

metadata: information qui permette d‚Äôidentifier la resource, name est obligatoire

spec: d√©finit l‚Äô√©tat d√©sir√© de notre ressource

status : √©tat actuel de la ressource

##==##

<!-- .slide: class="transition-bg-sfeir-3"-->

# Cr√©er et interagir avec des objets Kubernetes

##==##

<!-- .slide: class="with-code" -->

# Mode d√©claratif (pr√©f√©r√©)

```shell
$ kubectl apply -f my-pod.yaml # Modifier ou mettre √† jour l‚Äôobjet
$ kubectl replace -f my-pod.yaml # Recr√©er l‚Äôobjet
$ kubectl diff -f my-pod.yaml # Afficher les diff√©rences de l‚Äôobjet
$ kubectl delete -f my-pod.yaml # Supprimer l‚Äôobjet
```

<!-- .element: class="big-code" -->

##==##

<!-- .slide: class="with-code" -->

# Mode imp√©ratif

```shell
$ kubectl create <type-of-object> [<subtype-of-object>] <name-of-object>
  <properties> # Cr√©er un objet
$ kubectl create namespace <my-namespace> # Cr√©er un espace de nom
$ kubectl run <pod-name> --image=<image> # Cr√©er un pod
$ kubectl run <pod-name> --image=<image> --dry-run=client -oyaml >
  my-pod.yaml
# G√©n√©rer le yaml du pod sans le cr√©er sur le cluster
```

<!-- .element: class="big-code"-->

##==##

<!-- .slide: class="transition-bg-sfeir-3"-->

# Cr√©er et interagir avec un pod

##==##

<!-- .slide: class="with-code max-height" -->

# kubectl get pods

```shell
$ kubectl get pods

NAME                     READY     STATUS    RESTARTS   AGE
nginx-5dbbff858d-qfb7f   1/1       Running   0          2m

$ kubectl get po -oyaml nginx-5dbbff858d-qfb7f
apiVersion: v1
kind: Pod
metadata:
  name: nginx-5dbbff858d-qfb7f
  ...
```

<!-- .element: class="big-code" -->

Notes:
READY : nombre de containers d√©marr√©s / nombre total de container

RESTART : si un container n‚Äôest pas dans l‚Äô√©tat voulu (crash, indisponibilit√©), le pod peut √™tre red√©marr√©

##==##

<!-- .slide: class="with-code max-height" -->

# Pod yaml

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  containers:
    - name: nginx
      image: nginx:alpine
status: ...
```

<!-- .element: class="big-code" -->

##==##

<!-- .slide: class="with-code" -->

# Pods explained

```shell
$ kubectl explain pod
KIND:     Pod
VERSION:  v1

DESCRIPTION:
     Pod is a collection of containers that can run on a host. This resource is
     created by clients and scheduled onto hosts.

FIELDS:
   apiVersion	<string>
   kind	<string>
   metadata	<Object>
   spec	<Object>
[...]

```

Notes:
La commande
explain
donne tous les d√©tails sur les propri√©t√©s de chaque ressources Kubernetes‚Ä¶

‚Ä¶ r√©cursivement (slides suivant)

##==##

<!-- .slide: class="with-code" -->

# Pods explained‚Ä¶ deeper

```shell
$ kubectl explain pod.spec.containers.image
KIND:     Pod
VERSION:  v1

FIELD:    image <string>

DESCRIPTION:
     Docker image name. More info:
     https://kubernetes.io/docs/concepts/containers/images This field is
     optional to allow higher level config management to default or override
     container images in workload controllers like Deployments and
     StatefulSets.

```

##==##

<!-- .slide: class="with-code" -->

# Pod status (live)

```yaml
---
status:
  containerStatuses:
    - name: nginx
      containerID: docker://8db3af41eb87[...]3d103255
      image: nginx:alpine
      imageID: docker-pullable://nginx@sha256:1aed1[...]a0a5440
      state:
        running:
          startedAt: 2018-08-13T21:08:10Z
  hostIP: 192.168.65.3
  podIP: 10.1.0.227
```

Notes:
Une fois le fichier yaml envoy√© dans la configuration, les contr√¥leurs viennent mettre √† jour le pod dans la partie status.

On retrouve entre autre l‚ÄôIP du
node
qui l‚Äôh√©berge et l‚ÄôIP du
pod
lui-m√™me.

##==##

<!-- .slide: class="transition-bg-sfeir-2"-->

# TP : Pod

##==##

<!-- .slide: class="exercice"-->

# TP : Pod

## LAB

- Regarder le code pour cr√©er un pod

`$ cat pod/monolith.yaml`

- Cr√©er le pod :

`$ kubectl apply -f pod/monolith.yaml`

Notes:
Cr√©ation d‚Äôun pod simple : on affiche les specs du pod, et on cr√©√© le pod avec kubectl.

##==##

<!-- .slide: class="exercice with-code"-->

# TP : Information sur le Pod

## LAB

```shell
$ kubectl get po -owide
$ kubectl describe po <pod-name>
$ kubectl get po <pod-name> -oyaml
```

<!-- .element: class="big-code" -->

- Quelle est l‚Äôip du pod ?
- Sur quel node le pod tourne ?
- Quel container tourne dans le pod ?

##==##

<!-- .slide: class="exercice"-->

# TP : Interagir avec le Pod

## LAB

- Cr√©er un tunnel depuis son poste avec le pod

`$ kubectl port-forward monolith 10080:80`

- Ouvrir une autre fen√™tre shell

`$ curl http://127.0.0.1:10080`
`$ curl http://127.0.0.1:10080/secure`
`$ curl -u user http://127.0.0.1:10080/login`

- Le mot de passe est : "password"

`$ curl -H "Authorization: Bearer <token>" http://127.0.0.1:10080/secure`

##==##

<!-- .slide: class="exercice"-->

# TP : Interagir avec le Pod

## LAB

- Voir les logs du pods

`$ kubectl logs monolith`

- Ouvrir un shell dans le pod

`$ kubectl exec monolith -it [-c monolith] -- /bin/sh`

- Lister les process dans le container

`$ ps aux`

##==##

<!-- .slide: class="transition-bg-sfeir-3"-->

# Liveness/Readiness

##==##

<!-- .slide:-->

# Readiness/Liveness

- Readiness :

  - V√©rifier que le container est pr√™t √† recevoir du flux

- Liveness :

  - V√©rifier que le container r√©pond correctement

- Startup :
  - V√©rifier que le container est d√©marr√©
  - D√©sactiver les sondes pr√©c√©dentes en attendant cet √©tat

Notes:
Kubeproxy sortira un pod du flux si le readiness devient ko.

Kubelet va red√©marrer le container si le liveness est ko.

##==##

<!-- .slide: class="with-code" -->

# Readiness/Liveness : command

```yaml
livenessProbe:
  exec:
    command:
      - cat
      - /tmp/healthy
  initialDelaySeconds: 5
  periodSeconds: 5
```

<!-- .element: class="big-code" -->

Notes:
Il est possible d‚Äôutiliser une commande pour tester l‚Äô√©tat du service

##==##

<!-- .slide: class="with-code max-height" -->

# Readiness/Liveness : tcp socket

```yaml
readinessProbe:
  tcpSocket:
    port: 8080
  initialDelaySeconds: 5
  periodSeconds: 10
livenessProbe:
  tcpSocket:
    port: 8080
  initialDelaySeconds: 15
  periodSeconds: 20
```

<!-- .element: class="big-code" -->

##==##

<!-- .slide: class="with-code max-height" -->

# Readiness/Liveness : http

```yaml
...
spec:
  containers:
    - readinessProbe:    # accepte des requ√™tes quand OK
        httpGet:
          path: /readiness
          port: 81
          scheme: HTTP
        initialDelaySeconds: 5
        timeoutSeconds: 1
      livenessProbe:    # red√©marre le pod si KO
        httpGet:
          path: /healthz
          port: 81
          scheme: HTTP
        initialDelaySeconds: 5 # D√©lai avant la premi√®re requ√™te
        periodSeconds: 15      # Fr√©quence des tests
        timeoutSeconds: 5

      ...


```

Notes:
readinessProbe indique √† kubelet quand un pod est pr√™t √† servir du traffic

livenessProbe indique si le pod est ok, si nok kubelet red√©marre le pod

##==##

<!-- .slide: class="transition-bg-sfeir-2"-->

# TP : Readiness/Liveness

##==##

<!-- .slide: class="exercice"-->

# TP : Readiness/Liveness

## LAB

- Regarder le contenu du pod
  `$ cat readiness/healthy-monolith.yaml`
  `$ kubectl apply -f readiness/healthy-monolith.yaml`
  `$ kubectl describe pods/healthy-monolith`

##==##

<!-- .slide: class="with-code exercice" -->

# TP : Readiness/Liveness

`$ kubectl describe pods`

- Comment est configur√©e la sonde readiness ?
- Comment est configur√©e la sonde liveness
- √† quelle fr√©quence la surveillance readiness est effectu√©e ?
- √† partir de combien de secondes la sonde liveness est effectu√©e ?

Notes:
Comment est configur√© la sonde readiness ? httpGet sur /readiness:81

liveness sur /healthz:81

la valeur par d√©faut du readiness : 10 secondes

liveness est effectu√© √† partir de 5 secondes

##==##

<!-- .slide: class="exercice"-->

# TP : Tester les sondes

```
$ kubectl get pods healthy-monolith
```

- Noter l‚Äô√©tat OK du pod

```
$ kubectl port-forward healthy-monolith 10081:81
```

- Forcer l‚Äôapplication a pass√© en √©tat ‚Äúfailed‚Äù

```
$ curl http://127.0.0.1:10081/readiness/status
```

- Attendre 45 secondes que la sonde en failed

```
$ kubectl describe pods healthy-monolith
```

- Vous pouvez noter dans l‚Äôhistorique le moment o√π le pod est pass√© en ‚Äúunhealthy‚Äù

##==##

<!-- .slide: class="exercice with-code"-->

# TP : Tester les sondes

- Maintenant nous allons ‚Äúcasser‚Äù la sonde liveness

```shell
$ curl http://127.0.0.1:10081/healthz/status

$ kubectl get pods
```

<!-- .element: class="big-code"-->

- Que se passe-t-il quand le liveness est ko ?

Notes:
Que se passe-t-il : dans ce cas le pod est red√©marr√©

##==##

<!-- .slide: class="transition"-->

# Configuration d‚Äôune application

##==##

<!-- .slide:-->

# Configuration

- Configurer l‚ÄôURI d‚Äôune base de donn√©es
- Injecter un fichier application-prod.yml
- Sp√©cifier les credentials d‚Äôune api

##==##

<!-- .slide: class="with-code" -->

# Variables d‚Äôenvironnement

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: envar-demo
spec:
  containers:
    - name: envar-demo-container
      image: debian
      env:
        - name: DEMO_GREETING
          value: 'Hello from the environment'
        - name: DEMO_FAREWELL
          value: 'Such a sweet sorrow'
```

<!-- .element: class="big-code" -->

Notes:
On peut injecter une variable d‚Äôenvironnement via le champ ‚Äúenv‚Äù.

##==##

<!-- .slide: class="with-code"-->

# ConfigMap : cr√©ation

```shell
$ kubectl create configmap <map-name> <data-source>
$ kubectl create configmap my-app-conf
  --from-file=my-app-conf/configmap/application-dev.properties
$ kubectl create cm my-app-conf
  --from-literal=db-server=mydbserver.mycompany.com
```

<!-- .element: class="big-code" -->

Notes:
Pour injecter un fichier de configuration, on peut injecter un fichier de configuration dans un pod avec un configMap.

On peut cr√©er un fichier de config, depuis :

un fichier ou un r√©pertoire  
--from-file
=

depuis la cli :
--from-literal
=

##==##

<!-- .slide: class="with-code" -->

# ConfigMap: manifests

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-app-config
data:
  application.properties: |
    db-server=mydbserver.dev.mycompany.com
    username=my-rw-dbuser
```

<!-- .element: class="big-code" -->

```shell
$ kubectl apply -f configmap.yml
```

<!-- .element: class="big-code" -->

Notes:
derri√®re le | on peut mettre n‚Äôimporte un fichier non structur√© yaml

##==##

<!-- .slide: class="with-code two-column max-height" -->

# ConfigMap : utilisation

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: configmap-pod
spec:
  containers:
    - name: test
      image: busybox
      volumeMounts:
        - name: config-vol
          mountPath: /etc/config
```

<!-- .element: class="big-code" -->

##--##

<!-- .slide: class="with-code" -->

<br>
<br>
<br>

```yaml
volumes:
  - name: config-vol
    configMap:
      name: log-config
      items:
        - key: log_level
          path: log_level
```

<!-- .element: class="big-code" -->

Point de montage du fichier : **/etc/config**/_log_level_

##==##

<!-- .slide: class="with-code" -->

# Secret : type

```shell
$ kubectl create secret --help

Available Commands:
  docker-registry Create a secret for use with a Docker registry
  generic         Create a secret from a local file, directory or literal value
  tls             Create a TLS secret
```

<!-- .element: class="big-code" -->

Notes:
On peut cr√©er 3 types de secret :

docker-registry : pour enregistrer les credentials pour pull une image docker depuis une registry priv√©e

generic : √©quivalent √† configmap, permet de stocker des secrets de type cl√©/valeur

tls : pour stocker un certificat serveur pour exposer un service en https

##==##

<!-- .slide: class="with-code" -->

# Secret : manifest

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: mysecret
type: Opaque
data:
  username: YWRtaW4=
  password: MWYyZDFlMmU2N2Rm
```

<!-- .element: class="big-code"  -->

##==##

<!-- .slide: class="with-code"-->

# Secret : cr√©ation

- Cr√©er un secret depuis un fichier

`$ kubectl create secret generic db-user-pass --from-file=./password.txt`

- Cr√©er un secret depuis une cl√©/valeur

`$ kubectl create secret generic prod-db-secret --from-literal=username=produser`

- Cr√©er un secret ‚Äúdocker-registry ‚Äù

```shell
$ kubectl create secret docker-registry regcred
  --docker-server=<your-registry-server> --docker-username=<your-name>
  --docker-password=<your-pword> --docker-email=<your-email>
```

<!-- .element: class="big-code" -->

Notes:
Un secret va permettre de stocker des informations sensibles, comme des mots de passe, des cl√©s priv√©es.

##==##

<!-- .slide: class="with-code max-height" -->

# Secret : env vars

```yaml
apiVersion: v1
kind: Pod
spec:
  containers:
    - name: envar-demo-container
      image: debian
      env:
        - name: DEMO_GREETING
          valueFrom:
            secretKeyRef:
              name: demo # le nom du secret
              key: password # la cl√© dans le secret
```

<!-- .element: class="big-code" -->

##==##

<!-- .slide: class="with-code two-column" -->

# Secret : volume

```yaml
kind: Pod
spec:
  containers:
    - name: mypod
      image: redis
      volumeMounts:
        - name: secret-volume
          mountPath: '/etc/foo'
          readOnly: true
```

<!-- .element: class="big-code" -->

##--##
<br>
<br>
<br>
<br>

```yaml
volumes:
  - name: secret-volume
    secret:
      secretName: mysecret
```

<!-- .element: class="big-code" -->

##==##

<!-- .slide: class="transition-bg-sfeir-2" -->

# TP : Configuration

##==##

<!-- .slide: class="exercice"-->

# TP : ConfigMap

## LAB

`$ kubectl create configmap nginx-proxy-conf --from-file=configuration/nginx/proxy.conf`

`$ kubectl describe configmaps nginx-proxy-conf`

##==##

<!-- .slide: class="exercice"-->

# Quizz : ConfigMap

## LAB

- Combien y a t-il d‚Äô√©l√©ments dans la configmap nginx-proxy-conf ?
- Quel est le nom de ces √©l√©ments ?

Notes:
1 element : le fichier proxy.conf

##==##

<!-- .slide: class="exercice"-->

# TP : Secret

## LAB

`$ kubectl create secret generic tls-certs --from-file=configuration/tls/`

`$ kubectl describe secrets tls-certs`

##==##

<!-- .slide: class="exercice"-->

# Quizz : Secret

## LAB

- Combien y a t-il d‚Äô√©l√©ments dans le Secret tls-cert ?

- Quel est le nom de ces √©l√©ments ?

Notes:
Le secret contient 4 elements :
ca-key.pem

ca.pem

cert.pem

key.pem

on peut voir que les secrets ne sont pas affiche dans la console

##==##

<!-- .slide: class="exercice" -->

# TP : Injection de fichiers

## LAB

`$ cat configuration/pod/secure-monolith.yaml`

- Comment le secret est inject√© dans le pod ?

- Comment le configmap est inject√© dans le pod ?

Notes:
Les secrets sont inject√©s sous la forme d'un volume

de la meme facon le configMap est injecte sous la forme d'un volume.

##==##

<!-- .slide: class="exercice"-->

# TP : Injection de fichiers

## LAB

- Cr√©er le pod ‚Äúsecure-monolith‚Äù

`$ kubectl apply -f configuration/pod/secure-monolith.yaml`

- Exposer le port sur votre poste

`$ kubectl port-forward secure-monolith 10443:443`

- Faites une requ√™te avec curl

`$ curl --cacert configuration/tls/ca.pem https://127.0.0.1:10443`

Notes:
Du coup on a inject√© les certificats et la configuration du service nginx, et on arrive bien √† faire une requ√™te en https sur notre service.

Pr√©ciser : ce n‚Äôest pas la bonne m√©thode pour exposer un service en https sur kubernetes ;))

##==##

<!-- .slide: class="transition-bg-sfeir-3"-->

# Service

##==##

<!-- .slide:-->

# Exposer des pods

![center h-800](./assets/images/expose-pods.png)

Notes:
Les pods sont expos√©s au sein du cluster via les
Services
.

Les services obtiennent une IP et un nom DNS interne au cluster.

Ils r√©partissent la charge entre les pods, cibl√©s par des
labels
.

##==##

<!-- .slide:-->

# Service type ClusterIP

![center h-800](./assets/images/cluster-ip.svg)

Notes:
Un Service de type ClusterIP est accessible depuis une IP interne au cluster.

On l‚Äôutilise en g√©n√©ral pour la communication inter-pods.

##==##

<!-- .slide:-->

# Service type NodePort

![center h-800](./assets/images/service-type-node.svg)

Notes:
Un service du type nodePort est associ√© √† un port identique sur chaque noeud du cluster.

Ce port est accessible depuis l‚Äôext√©rieur du cluster.

Il doit appartenir √† la plage de ports 30000 - 32767 (configurable sur le master).

On peut le choisir mais il faut mieux laisser faire Kubernetes, qui g√®rera les probl√®mes de ports occup√©s.

Inconv√©nient : il faut donc conna√Ætre l‚Äôadresse IP d‚Äôau moins un noeud du cluster pour acc√©der au service.

##==##

<!-- .slide:-->

# Service type LoadBalancer

![center h-800](./assets/images/service-type-loadbalancer.svg)

Notes:
Un Service de type LoadBalancer utilise un √©quilibreur de charge externe au cluster.

Ce type de Service est utilis√© sur les plateformes cloud ainsi que dans Docker-for-Desktop.

L‚Äôinconv√©nient c‚Äôest que chaque service aura son propre load balancer avec sa propre IP publique/Internet

##==##

<!-- .slide: data-type-show="hide"-->

# Service type ExternalName

![center h-800](./assets/images/service-type-externalname.svg)

Notes:
Un Service de type ExternalName fourni un alias DNS de type CNAME.

Il n‚Äôest utilisable qu‚Äôen interne.

##==##

<!-- .slide: data-type-show="hide"-->

# Service avec Endpoint explicite

![center h-800](./assets/images/service-endpoint-explicite.svg)

Notes:
En interne, Kubernetes g√©n√®re une ressource Endpoint pour chaque Pod cibl√© par le s√©lecteur.

Il est possible de ne pas renseigner de s√©lecteur et √† la place de cr√©er manuellement les ressources Endpoint avec une IP externe au cluster.

C‚Äôest la mani√®re de rajouter une abstraction vers un service externe au cluster. Les Pods ne voient que le Service interne.

##==##

<!-- .slide: class="with-code max-height" -->

# Service yaml

```yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx
spec:
  selector:
    app: nginx
  type: NodePort
  ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: 80
```

<!-- .element: class="big-code" -->

Notes:
Le
selector
permet de cibler les pods par leurs labels.

Il y a plusieurs
types
de services qu‚Äôon voit tout de suite.

##==##

# Cr√©er un service

`$ kubectl apply -f service.yaml`

La ligne de commande suivante donne un r√©sultat √©quivalent au fichier yaml :

`$ kubectl expose nginx --port=80 --type=NodePort`

##==##

<!-- .slide: class="with-code max-height" -->

# Services

```shell
$ kubectl get services
NAME        TYPE       CLUSTER-IP   EXTERNAL-IP PORT(S)       AGE
kubernetes  ClusterIP  10.96.0.1    <none>      443/TCP       3d
nginx       NodePort   10.97.47.155 <none>      80:31450/TCP  2s

$ kubectl get svc -o yaml nginx
apiVersion: v1
kind: Service
metadata:
  name: nginx
  ...
```

<!-- .element: class="big-code" -->

##==##

<!-- .slide:-->

# Service : DNS interne

- Entr√©e DNS : <service>.<namespace>.svc.cluster.local

- Et dans le namespace : <service>

Notes:
Pour chaque service cr√©√©, le service est accessible via une entr√©e DNS : <service>.<namespace>.svc.cluster.local

Et √† l‚Äôint√©rieur du namespace, uniquement avec le nom du service

##==##

<!-- .slide: data-type-show="hide"-->

# Question 9

Quel est le type de service basique permettant l‚Äôexposition de pods dans le cluster ?

ClusterIP
NodePort
LoadBalancer

##==##

<!-- .slide: data-type-show="hide"-->

# Question 9

Quel est le type de service basique permettant l‚Äôexposition de pods dans le cluster ?

ClusterIP
NodePort
LoadBalancer

##==##

<!-- .slide: data-type-show="hide"-->

# Question 10

Quel est le type de service permettant l‚Äôexposition √† travers le port d‚Äôun noeud?

ClusterIP
NodePort
LoadBalancer

##==##

<!-- .slide: data-type-show="hide"-->

# Question 10

Quel est le type de service permettant l‚Äôexposition √† travers le port d‚Äôun noeud?

ClusterIP
NodePort
LoadBalancer

##==##

<!-- .slide: data-type-show="hide"-->

# Question 11

Quel est le type de service permettant l‚Äôexposition √† travers un load balancer externe?

ClusterIP
NodePort
LoadBalancer

##==##

<!-- .slide: data-type-show="hide"-->

# Question 11

Quel est le type de service permettant l‚Äôexposition √† travers un load balancer externe?

ClusterIP
NodePort
LoadBalancer

##==##

<!-- .slide: class="transition-bg-sfeir-2" -->

# TP : Service

##==##

<!-- .slide: class="exercice"-->

# TP : Cr√©er un service

## LAB

- Lire le manifest de l‚Äôobjet service

`$ cat service/monolith.yaml`

- Cr√©er le service

`$ kubectl apply -f service/monolith.yaml`

##==##

<!-- .slide: class="exercice" data-type-show="hide" -->

# TP : Interagir avec le service

## LAB

```shell
$ gcloud compute instances list
NAME ZONE MACHINE_TYPE PREEMPTIBLE INTERNAL_IP EXTERNAL_IP STATUS
gke-istio-workshop-default-pool-e37360ba-369z europe-west1-b n1-standard-1 10.132.0.3 35.205.92.225 RUNNING
gke-istio-workshop-default-pool-e37360ba-3hz4 europe-west1-b n1-standard-1 10.132.0.2 35.240.127.238 RUNNING
...
```

Prenez l‚Äôune des ips externes et connectez vous-y

`$ curl -k https://<EXTERNAL_IP>:31000`

Notes:
Le r√©sultat est un √©chec :

curl -k https://35.246.130.12:31000

curl: (7) Failed to connect to 35.246.130.12 port 31000: Connection refused

Normal : il n‚Äôy a pas de label correspondant !

##==##

<!-- .slide: class="exercice"-->

# TP : Service

## LAB

- Pourquoi la requ√™te ne fonctionne pas ?

`$ kubectl get svc monolith`

`$ kubectl describe services monolith`

- Combien y a t-il de endpoints au service ?

Notes:
Le r√©sultat est un √©chec :

curl -k https://35.246.130.12:31000

curl: (7) Failed to connect to 35.246.130.12 port 31000: Connection refused

Normal : il n‚Äôy a pas de label correspondant !

##==##

<!-- .slide: class="exercice"-->

# TP : Service

## LAB

- Lister les pods par labels

`$ kubectl get pods -l "app=monolith"`
`$ kubectl get pods -l "app=monolith,secure=enabled"`

- On va labelliser un pod

`$ kubectl label pods secure-monolith secure=enabled`

- Et maintenant ?

`$ kubectl get pods -l "app=monolith,secure=enabled"`

Notes:
On labellise les pods

##==##

<!-- .slide: class="transition-bg-sfeir-2"-->

# ReplicaSet

##==##

<!-- .slide: class="with-code max-height" -->

# ReplicaSet.yaml

```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template: <... pod definition ...>
```

<!-- .element: class="big-code" -->

Notes:
Le ReplicaSet inclut la d√©finition du pod dans la partie
template

Le
selector
permet d‚Äôidentifier les pods g√©r√©s par ce ReplicaSet via des
labels
.

La propri√©t√©
replicas
indique le nombre d‚Äôinstances voulues.

##==##

<!-- .slide: class="with-code" -->

# kubectl get replicaset

```shell
$ kubectl get ReplicaSet
NAME               DESIRED   CURRENT   READY     AGE
nginx-5dbbff858d   1         1         1         14m

$ kubectl get rs -o yaml nginx-5dbbff858d
apiVersion: apps/v1
kind: ReplicaSet
...
```

<!-- .element: class="big-code" -->

Notes:
Le ReplicaSet maintient le nombre demand√© de pods
en √©tat de fonctionnement
.

La colonne READY montre les pods qui sont fonctionnels, et pas juste les pods cr√©√©s
(
par exemple avec des
containers en cours de cr√©ation ou crash√©s)

##==##

<!-- .slide: class="with-code" -->

# ReplicaSet status (live)

```yaml
---
status:
  availableReplicas: 1
  fullyLabeledReplicas: 1
  readyReplicas: 1
  replicas: 1
```

<!-- .element: class="big-code" -->

Notes:
On voit le status des replicas

##==##

<!-- .slide: class="transition-bg-sfeir-3"-->

# Deployment

##==##

<!-- .slide: class="with-code max-height two-column" -->

# Deployment.yaml

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
```

<!-- .element: class="big-code" -->

##--##

<!-- .slide: class="with-code" -->

<br>
<br>
<br>

```yaml
template:
  metadata:
    labels:
      app: nginx
  spec:
    containers:
      - name: nginx
        image: nginx:alpine
```

<!-- .element: class="big-code" -->

Notes:
On retrouve dans le Deployment les specs du Pod et du ReplicaSet

##==##

<!-- .slide: class="with-code" -->

# kubectl get deployment

```shell
$ kubectl get deployment
NAME      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx     1         1         1            1           1h

$ kubectl get deploy -o yaml nginx
apiVersion: apps/v1
kind: Deployment
```

<!-- .element: class="big-code" -->

Notes:
Le Deployment g√®re des
Pods
via un
ReplicaSet
.

Contrairement au ReplicaSet qui maintient la m√™me configuration de Pod en changeant juste le nombre de replicats,

le Deployment sait g√©rer la mise √† jour des Pods en ‚Äú
Rolling upgrade
‚Äù

##==##

<!-- .slide: class="with-code" -->

# Mise √† l‚Äô√©chelle manuelle

```shell

$ kubectl scale deployment/nginx --replicas=10

$ kubectl get all
```

<!-- .element: class="big-code" -->

Notes:
La commande
scale
permet d‚Äôindiquer au ReplicaSet le nombre de r√©plicas √† maintenir.

On scale ici √† 10 √† instances de pod pour d√©montrer les
mises √† jours progressives
.

##==##

<!-- .slide: class="transition-bg-sfeir-3"-->

# The Label Game

##==##

<!-- .slide:-->

# Les labels Kubernetes

- Pour le fonctionnement interne de Kubernetes
  - ReplicaSet && Deployments ‚áí Pods
  - Services ‚áí Pods

##==##

<!-- .slide:-->

# ‚ÄúThe label game‚Äù

![center h-800](./assets/images/label-game.png)

Notes:
Jeu : trouver le s√©lecteur permettant d‚Äôidentifier les pods encadr√©s

##==##

<!-- .slide:-->

# ‚ÄúThe label game‚Äù

![center h-800](./assets/images/label-game.png)

<div style="border:blue solid 3px; border-radius:100px; position:absolute; width:700px; height: 750px; top:250px; left: 600px;" ></div>

Notes:
selector:

App: MyApp

##==##

<!-- .slide:-->

# ‚ÄúThe label game‚Äù

![center h-800](./assets/images/label-game.png)

<div style="border:blue solid 3px; border-radius:100px; position:absolute; width:350px; height: 750px; top:250px; left: 600px;" ></div>

Notes:
selector:

R√¥le: Interface

##==##

<!-- .slide:-->

# ‚ÄúThe label game‚Äù

![center h-800](./assets/images/label-game.png)

<div style="border:blue solid 3px; border-radius:100px; position:absolute; width:700px; height: 350px; top:250px; left: 600px;" ></div>

Notes:
selector:

Phase: Prod

##==##

<!-- .slide:-->

# ‚ÄúThe label game‚Äù

![center h-800](./assets/images/label-game.png)

<div style="border:blue solid 3px; border-radius:100px; position:absolute; width:350px; height: 350px; top:650px; left: 950px;" ></div>

Notes:
selector:

Phase: test

R√¥le: BE

##==##

<!-- .slide: class="transition-bg-sfeir-2"-->

# TP : Deployment

##==##

<!-- .slide: class="exercice"-->

# TP : Deployment

## LAB

- On va cr√©er une stack compl√®te hello, auth, frontend

- Avec deployment et service

##==##

<!-- .slide: class="exercice"-->

# Architecture de la stack

## LAB

![center h-800](./assets/images/archi-tp-deployment.svg)

##==##

<!-- .slide: class="exercice"-->

# TP : Deployment

## LAB

- Cr√©er un d√©ploiement :

`$ kubectl apply -f deployments/auth.yaml`

`$ kubectl describe deployments auth`

- Et cr√©er le service correspondant

`$ kubectl apply -f deployments/service-auth.yaml`

##==##

<!-- .slide: class="exercice"-->

# TP : Deployment Hello

## LAB

- Idem pour le service Hello

`$ kubectl apply -f deployments/hello.yaml`

`$ kubectl describe deployments hello`

`$ kubectl apply -f deployments/service-hello.yaml`

##==##

<!-- .slide: class="exercice"-->

# TP : Deployment Frontend

## LAB

- Et pour le service Front

`$ kubectl create configmap nginx-frontend-conf --from-file=configuration/nginx/`

`$ kubectl apply -f deployments/frontend.yaml`

`$ kubectl apply -f deployments/service-frontend.yaml`

- Quizz : Comment le frontend a acc√®s au service auth et hello ?

##==##

<!-- .slide: class="transition-bg-sfeir-2"-->

# TP : C‚Äôest les soldes !

##==##

<!-- .slide: class="exercice"-->

# TP : Scaling Deployment

## LAB

- On va maintenant augmenter le nombre de pod

`$ kubectl scale deployments hello --replicas=3`

`$ kubectl describe deployments hello`

`$ kubectl get pods`

`$ kubectl get replicasets`

Notes:
combien

##==##

<!-- .slide: class="exercice with-code"-->

# TP : Scaling Frontend

## LAB

- Editer le service frontend pour scaler √† 2 replicas

`$ vim deployments/frontend.yaml`

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
name: frontend
spec:
replicas: 2
```

`$ kubectl apply -f deployments/frontend.yaml`

##==##

<!-- .slide: class="transition"-->

# Mise √† l‚Äô√©chelle et mise √† jour

##==##

<!-- .slide:-->

# Rolling upgrade 1/4

![center h-800](./assets/images/rolling-update-1.png)

Notes:
Etat initial : 4 Pods v1 r√©partis sur 3 noeuds

On va modifier la configuration du Deployment pour utiliser une autre version du pod (changement d‚Äôimage docker, de configuration, ‚Ä¶)

Le DeploymentController (le A sur le master) va r√©agir √† ce changement de configuration

##==##

<!-- .slide:-->

# Rolling upgrade 2/4

![center h-800](./assets/images/rolling-update-2.png)

Notes:
Le DeploymentController va instancier un nouveau Pod ‚Äúv2‚Äù et attendre qu‚Äôil soit fonctionnel.

D√®s que ce nouveau Pod est pr√™t, le Service va lui envoyer des requ√™tes.

L‚Äôancien Pod ne re√ßoit plus de nouvelles requ√™tes, il va √™tre arr√™t√© puis supprim√© par le DeploymentController.

##==##

<!-- .slide:-->

# Rolling upgrade 3/4

![center h-800](./assets/images/rolling-update-3.png)

Notes:
Chaque Pod est ainsi remplac√© par la nouvelle version, un par un.

##==##

<!-- .slide:-->

# Rolling upgrade 4/4

![center h-800](./assets/images/rolling-update-4.png)

Notes:
√Ä la fin du processus de rolling-upgrade, il ne reste que des pods v2.

Il n‚Äôy a pas eu d‚Äôinterruption de service.

##==##

<!-- .slide: class="with-code" -->

# Mise √† jour progressive

- Mise √† jour progressive (rolling-upgrade) sans interruption de service
- G√©r√©e par le Deployment

Remplacer nginx par Apache httpd :

```shell
$ kubectl edit deployment nginx --record
‚Äúimage: nginx:alpine‚Äù ‚áí ‚Äúimage: httpd:alpine‚Äù
```

<!-- .element: class="big-code" -->

Notes:
Fonctionnalit√© native Kubernetes au niveau d‚Äôun d√©ploiement.

Regarder la mise √† jour dans le visualizer

V√©rifier que nginx a bien √©t√© remplac√© par apache dans le navigateur.

Edition en direct de la spec du Deployment

Sortir de vim avec ‚Äú:wq‚Äù

##==##

<!-- .slide: class="with-code" -->

# Annuler une mise √† jour progressive

```shell
$ kubectl rollout history deployment nginx
REV  CHANGE-CAUSE
1    kubectl apply --filename=deployment.yaml
2    kubectl edit deployment nginx
3    kubectl set image deploy nginx nginx=nginx:stable-alpine

$ kubectl rollout undo deployment --to-revision=1
```

<!-- .element: class="big-code" -->

Notes:
Le nombre de r√©vision conserv√©es est contr√¥le par la propri√©t√©
.spec.revisionHistoryLimit
, 10 par d√©faut.

Ce sont les ressources ‚ÄúReplicaSet‚Äù qui sont conserv√©s dans etcd, avec
replicas=0
pour les anciennes versions.

##==##

<!-- .slide: class="transition-bg-sfeir-3"-->

# Mise √† l‚Äô√©chelle automatique

##==##

<!-- .slide:-->

# Horizontal Pod Autoscaler

![center h-800](./assets/images/horizontal-pod-autoscaler.svg)

Notes:
L‚Äô
Horizontal Pod Autoscaler
est un Controller Kubernetes responsable de

modifier en temps r√©el le nombre de replicas
d‚Äôun ReplicationController / Deployment

en fonction de la charge CPU
moyenne de l‚Äôensemble de pods de ce RC/D√©ployment.

https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/

https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/

##==##

<!-- .slide: class="with-code" -->

# Horizontal Pod Autoscaler

- D√©clarer un Horizontal Pod scaler en CLI :

`$ kubectl autoscale deployment nginx --cpu-percent=50 --min=1 --max=10`

Notes:
On peut d√©clarer un horizontal pod autoscaler en cli ou via l‚Äôapi, exemple via l‚Äôapi page suivante.

Le HPA va agir sur le deployment pour monter ou descendre le nombre de pod en fonction du pourcentage de cpu consomm√©.

D‚Äôautres m√©triques peuvent √™tre utilis√©s.

##==##

<!-- .slide: class="with-code" -->

# Horizontal Pod Autoscaler

```yaml
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: helloweb
spec:
  maxReplicas: 10
  minReplicas: 1
  targetCPUUtilizationPercentage: 50
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: helloweb
```

##==##

<!-- .slide: class="transition-bg-sfeir-2"-->

# TP : Autoscaling

##==##

<!-- .slide: class="exercice" -->

# TP : Autoscaling

## LAB

- Les tests vont s‚Äôeffectuer sur l‚Äôimage docker k8s.gcr.io/hpa-example.
- Cette image contient un apache+php. La home page de cette apache est un script php qui calcule les racines carr√©es des nombres de 1 √† 1 million.
- L‚Äôappel √† cette page va provoquer une forte consommation CPU.

Notes:
Version
imp√©rative
de la commande
versus
version
d√©clarative
par fichier yaml.

##==##

<!-- .slide: class="exercice"-->

# TP : Autoscaling

## LAB

- Cr√©er un deployment

`$ kubectl apply -f deployments/hpa-example.yaml`

- Exposer le deployment

`$ kubectl expose deployment/hpa-example --port 80`

- Cr√©er un hpa sur le deployment

`$ kubectl autoscale deployment hpa-example --cpu-percent=50 --min=1 --max=10`

##==##

<!-- .slide: class="exercice with-code"-->

# TP : Autoscaling

## LAB

- Vous lancerez ensuite un Pod interactif pour charger l‚ÄôApache :

`$ kubectl run -ti load-generator --image=busybox /bin/sh`

- Dans laquelle vous ex√©cuterez la boucle d‚Äôappels :

```shell
$ while true;
do wget -q -O- http://hpa-example;
done
```

<!-- .element: class="big-code" -->

##==##

<!-- .slide: class="exercice"-->

# TP : Autoscaling

## LAB

- V√©rifier l‚Äô√©tat de l‚Äôobjet HPA

`$ kubectl get hpa/hpa-example`

- Arr√™ter la commande wget
- Attendez 5 minutes
- V√©rifier √† nouveau l‚Äô√©tat du HPA
- Que constatez-vous ?

##==##

<!-- .slide: class="transition-bg-sfeir-3"-->

# Ingress

##==##

<!-- .slide:-->

# Ingress

- Point d‚Äôentr√©e unique du cluster

- HTTP (port 80) et HTTPS (port 443)

- Gestion des certificats SSL

##==##

<!-- .slide:-->

# Ingress

![center h-800](./assets/images/ingress.svg)

##==##

<!-- .slide: class="with-code max-height" -->

# Ingress yaml

```yaml
apiVersion: v1
kind: Ingress
metadata:
  name: nginx
spec:
  rules:
  - host: "institute.sfeir.com"
    http:
          paths:
          - path: /
        pathType: ImplementationSpecific
        backend:
                  service
            name: nginx
            port:
              name: http
```

Notes:
Les r√®gles permettent, avec l‚Äôimpl√©mentation actuelle, de router le traffic selon :

le hostname (en-t√™te HTTP ‚Äú
Host:
‚Äù)

le path HTTP (√† partir du
/
suivant le nom+port de serveur dans l‚Äôurl)

##==##

<!-- .slide: class="with-code" -->

# Ingress

```shell
$ kubectl get ingress
NAME      HOSTS     ADDRESS     PORTS     AGE
nginx     *         localhost   80        3m

$ kubectl get ing -o yaml nginx
apiVersion: apps/v1
kind: Ingress
metadata:
  name: nginx
```

<!-- .element: class="big-code" -->

##==##

<!-- .slide: class="with-code" -->

# Ingress

- Impl√©mentation par d√©faut √† base de nginx

- √Ä activer sur minikube (minikube addons enable ingress)

- √Ä d√©ployer manuellement sous Docker-for-Desktop

https://kubernetes.github.io/ingress-nginx/deploy/

##==##

<!-- .slide: data-type-show="hide"-->

# Demo : Ingress

##==##

<!-- .slide:-->

# R√©capitulatif

![center h-800](./assets/images/recap-ingress.png)

Notes:
Jusqu‚Äô√† maintenant avons abord√© les principales ressources Kubernetes :

Pod, ReplicaSet,
Deployment, Service et Ingress

Voyons maintenant comment les faire vivre

##==##

<!-- .slide: class="transition"-->

# Stocker des donn√©es

##==##

<!-- .slide:-->

# Volumes

- Juste un dossier mont√© dans un/des containers
- Associ√© √† la vie du Pod, survit au restart des containers
- Nombreuses impl√©mentations :
  - emptyDir
  - hostPath
  - persistentVolumeClaim
  - configMap / secret

##==##

<!-- .slide: class="with-code" -->

# emptyDir

- Volume vide, cr√©√© au d√©marrage d‚Äôun pod, supprim√© avec la suppression du Pod.

```yaml
volumes:
  - name: cache-volume
    emptyDir: {}
```

<!-- .element: class="big-code" -->

Notes:
Utile pour passer des fichiers d‚Äôun container √† l‚Äôautre ou d‚Äôun initContainer √† un container.

##==##

<!-- .slide: data-type-show="hide"-->

# gcePersistentDisk

PersistentDisk GCE
Il doit √™tre dans le m√™me projet et la m√™me zone que les VM du cluster
Il n‚Äôest pas supprim√© √† la suppression du pod

##==##

<!-- .slide: data-type-show="hide"-->

# gcePersistentDisk

```yaml
volumes:
  - name: test-volume
    gcePersistentDisk:
    pdName: my-data-disk
    fsType: ext4
```

Notes:
Il s‚Äôagit d‚Äôun disque compute engine classique.

Il NE peut √™tre monter en √©criture que sur UN SEUL node √† la fois, mais il peut √™tre monter en lecture sur plusieurs nodes.

##==##

<!-- .slide: class="with-code" -->

# hostPath

- Monte dans le container un dossier du noeud sur lequel le Pod s‚Äôex√©cute

```
volumes:
  - name: test-volume
    hostPath:
      # directory location on host
      path: /data
      # this field is optional
      type: Directory
```

<!-- .element: class="big-code" -->

Notes:
Il s‚Äôagit d‚Äôun disque mont√© sur une VM du cluster, mais il faut que le pod tourne sur le noeud o√π le volume a √©t√© cr√©√©. Peu recommand√©.

##==##

<!-- .slide:-->

# persistentVolume & persistentVolumeClaim

- L‚Äôadmin cr√©e une ressource PersistentVolume associ√©e √† une espace de stockage
- Le pod r√©clame du disque avec un PersistentVolumeClaim

Notes:
PersistentVolumeClaim fait r√©f√©rence au PersistentVolume cr√©√© pr√©c√©demment

##==##

<!-- .slide: class="with-code max-height" -->

# persistentVolume

```yaml
kind: PersistentVolume
apiVersion: v1
metadata:
  name: task-pv-volume
  labels:
    type: local
spec:
  storageClassName: manual
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: '/mnt/data'
```

Notes:
Ce persistent volume va cr√©√© un espace r√©serv√© de 10 Go sur l‚Äôun des noeuds du cluster.

##==##

<!-- .slide: class="with-code max-height"-->

# persistentVolumeClaim

```yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
name: task-pv-claim
spec:
storageClassName: manual
accessModes:

- ReadWriteOnce
  resources:
  requests:
  storage: 3Gi
```

##==##

<!-- .slide: class="two-column with-code"-->

# Pod

```yaml
kind: Pod
apiVersion: v1
metadata:
name: task-pv-pod
spec:
volumes:
  - name: task-pv-storage
    persistentVolumeClaim:
    claimName: task-pv-claim
```

<!-- .element: class="big-code" -->

##--##

<!-- .slide: class="with-code" -->

<br>
<br>
<br>

```yaml
containers:
  - name: task-pv-container
    image: nginx
    ports:
  - containerPort: 80
    name: 'http-server'
    volumeMounts:
  - mountPath: '/usr/share/nginx/html'
    name: task-pv-storage
```

<!-- .element: class="big-code" -->

##==##

<!-- .slide: class="transition-bg-sfeir-2"-->

# TP : Volumes

##==##

<!-- .slide: class="exercice"-->

# TP : Volume

## LAB

- Objectif : d√©ployer un wordpress + MySql persistant
- G√©n√©rer un secret avec un mot de passe

`$ kubectl create secret generic mysql --from-literal=password=$(openssl rand -hex 12)`

##==##

<!-- .slide: class="exercice"-->

# TP : Volume

## LAB

- Cr√©er les volumes

`$ kubectl apply -f volume/mysql-volumeclaim.yaml`

`$ kubectl apply -f volume/wordpress-volumeclaim.yaml`

- Cr√©er la base mysql

`$ kubectl apply -f volume/mysql.yaml`

`$ kubectl apply -f volume/mysql-service.yaml`

##==##

<!-- .slide: class="exercice"-->

# TP : Volume

## LAB

- Cr√©er l‚Äôinstance Wordpress et exposer le service

`$ kubectl apply -f volume/wordpress.yaml`

`$ kubectl apply -f volume/wordpress-service.yaml`

- Acc√©der √† votre wordpress

`$ curl http://$(kubectl get service wordpress -o jsonpath="{.status.loadBalancer.ingress[0].ip}")`

##==##

<!-- .slide: class="exercice"-->

# TP : Volume

## LAB

- D√©truire les pods mysql et wordpress

`$ k delete pods -l="app=mysql"`

`$ k delete pods -l="app=wordpress"`

- Que se passe-t-il ? (un peu de patience quand m√™me ;) )

##==##

<!-- .slide:  class="exerice"-->

# TP : Volume Nettoyage

## LAB

`$ kubectl delete service wordpress`

`$ kubectl delete deployment wordpress`

`$ kubectl delete pvc wordpress`

`$ kubectl delete service mysql`

`$ kubectl delete deployment mysql`

`$ kubectl delete pvc mysql`

##==##

<!-- .slide: class="transition"-->

# Gestion avanc√©e de Pods

##==##

<!-- .slide:-->

# DaemonSet : un Pod par noeud

![center h-500](./assets/images/daemonset.svg)

- Agent de monitoring
- Relais de logs
- Agent de stockage (client glusterfs, ‚Ä¶)

Notes:
Un objet DaemonSet va ex√©cuter un pod par node, utilis√© surtout pour des t√¢ches d‚Äôadministration/ monitoring du cluster.

##==##

<!-- .slide:-->

# StatefulSet : un Volume par Pod

![center h-500](./assets/images/statefulset.svg)

- Pour les applis qui ont besoin de garder un √©tat
- Kube r√©associe les Pods aux volumes persistants
- Dns : <name>-{0..N-1}.<service>.<ns>.svc.cluster.local

Notes:
Les pods sont num√©rot√©s dans l‚Äôordre, et les pods r√©assign√©s √† ce volumes au red√©marrage.

Quelques contraintes : les pods sont d√©marr√©s et √©teints dans l‚Äôordre de d√©marrage

##==##

<!-- .slide:-->

# Jobs

- Job

  - Batch lanc√© une seule fois
  - Kubernetes ne relance pas les Pods de ce type sauf code retour en erreur

- CronJob
  - Job d√©clench√© r√©guli√®rement selon une expression cron : `"_/1 _ \* \* \*"`

Notes:
Utile pour lancer un batch de fa√ßon r√©gulier.

##==##

<!-- .slide: class="transition-bg-sfeir-3"-->

# Template

##==##

<!-- .slide:-->

# Probl√®me

![center h-800](./assets/images/pb-template.png)

Notes:
Un d√©ploiement peut devenir rapidement tr√®s verbeux, tr√®s long, difficile √† maintenir

##==##

<!-- .slide:-->

# Probl√®mes

- yaml verbeux
- beaucoup de code en commun entre 2 d√©ploiements
- besoin de d√©ployer un ‚Äúpackage‚Äù

Notes:
Probl√®me de syntaxe avec le yaml

Il y a g√©n√©ralement entre 2 applications similaires, peu de diff√©rence

Plut√¥t que de d√©ployer un deployment + un service + un configmap + ... , on a besoin de d√©ployer un package complet.

##==##

<!-- .slide: class="with-code" -->

# Template : Helm

- Le plus populaire aujourd‚Äôhui
- Templating sur la base du moteur de template go
- Chart
- Repository de chart
- Tiller et √©tat

`$ helm repo update`

`$ helm install mysql bitnami/mysql`

##==##

<!-- .slide: class="with-code max-height" -->

# Helm : chart

```
wordpress/
  Chart.yaml          # A YAML file containing information about the chart
  LICENSE             # OPTIONAL: A plain text file containing the license for the chart
  README.md           # OPTIONAL: A human-readable README file
  requirements.yaml   # OPTIONAL: A YAML file listing dependencies for the chart
  values.yaml         # The default configuration values for this chart
  charts/             # A directory containing any charts upon which this chart depends.
  templates/          # A directory of templates that, when combined with values,
                      # will generate valid Kubernetes manifest files.
  templates/NOTES.txt # OPTIONAL: A plain text file containing short usage notes
```

<!-- .element: class="big-code" -->

Notes:
L‚Äôorganisation d‚Äôun chart helm

##==##

<!-- .slide: class="with-code two-column max-height" -->

# Helm : Template

[deployment.yaml]

```yaml
spec:
  containers:
    - name: deis-database
      image: postgres:{{.Values.dockerTag}}
      imagePullPolicy: { { .Values.pullPolicy } }
      ports:
        - containerPort: 5432
      env:
        - name: DATABASE_STORAGE
          value: { { default "minio" .Values.storage } }
```

##--##

<!-- .slide: class="with-code" -->

<br>
<br>
<br>

[Values.yaml]

```yaml
# The tag for the docker image.
dockerTag: 7.1.2
pullPolicy: Always
# The storage backend, whose default is set to "minio"
# storage:
```

Notes:
Values.yaml

imageRegistry
: The source registry for the Docker image.

dockerTag
: The tag for the docker image.

pullPolicy
: The Kubernetes pull policy.

storage
: The storage backend, whose default is set to
"minio"

##==##

<!-- .slide:-->

# Template : Kustomize

- Challenger
- Plain yaml
- Pas de ‚Äútemplating‚Äù
- Mais plut√¥t du ‚Äúpatch‚Äù
- Int√©gr√© √† kubectl ‚Äòkubectl apply **-k ...**‚Äô

##==##

<!-- .slide:-->

# Kustomize

![center h-800](./assets/images/kustomize.png)

Notes:
Un exemple avec kustomize

##==##

<!-- .slide: class="transition-bg-sfeir-3"-->

# S√©curit√©

##==##

<!-- .slide:-->

# RBAC

- Role-based Access Control
- Contr√¥le l‚Äôacc√®s √† l‚Äôapi pour les utilisateurs
- Gestion des droits par d√©faut dans kubernetes

- Role / ClusterRole
- RoleBinding / ClusterRoleBinding

##==##

<!-- .slide: class="with-code two-column" -->

# RBAC

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: default
  name: pod-reader
rules:
  - apiGroups: ['']
    resources: ['pods']
    verbs: ['get', 'watch', 'list']
```

##--##

<br>
<br>
<br>

```yaml
---
kind: RoleBinding
subjects:
  - kind: User
    name: jane # Name is case sensitive
    apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io
```

##==##

<!-- .slide:-->

# RBAC : r√¥les par d√©faut

- view
- edit
- admin
- cluster-admin

Notes:
view : permet d‚Äôavoir une vue readonly sur la plupart des objets d‚Äôun namespace, ne permet pas de voir les secrets, ou les objets roles et rolebinding

edit : permet de modifier la plupart des objets, mais pas les objet roles rolebinding

admin : permet d‚Äô√©diter les objets + les droits sur un namespace mais pas les quotas

cluster-admin : tous les droits

##==##

<!-- .slide: class="transition-bg-sfeir-3"-->

# Monitoring

##==##

<!-- .slide:-->

# Monitoring : Prometheus

- Pull de m√©triques sur les applications.
- Inverse la logique de monitoring.

Notes:
Traditionnellement, les applications envoient leurs m√©triques √† des syst√®mes centralis√©s. Cette fa√ßon de faire rend l'application responsable de cet envoi des m√©triques.
Dans une architecture cloud moderne, la logique est invers√©e pour faciliter ce monitoring.

Prometheus est un des outils qui impl√©mente cette logique. Il va interroger p√©riodiquement une URL sur vos applications pour lire vos m√©triques, puis envoyer ces m√©triques √† une base de donn√©es sp√©cialis√©e dans les s√©ries temporelles. Cette base pourra √™tre interrog√©e pour la surveillance des applications.

##==##

<!-- .slide: class="transition"-->

# M√©thodes d‚Äôinstallation de clusters

##==##

<!-- .slide:-->

# Installation k8s : d√©veloppement

- [Docker Desktop](https://www.docker.com/products/docker-desktop/)
- [Rancher Desktop](https://rancherdesktop.io/)
- Installation manuelle [‚Äúk8s the hard way‚Äù](https://github.com/kelseyhightower/kubernetes-the-hard-way)
  - Quid du r√©seau?
  - S√©curit√© first
- [Kind](https://kind.sigs.k8s.io/)
- [Kops](https://github.com/kubernetes/kops)
- [K3s](https://k3s.io/)

Notes:
‚ÄúThe hard way‚Äú : pour les tr√®s courageux : comment installer un cluster k8s compl√©mente √† la main. (tr√®s int√©ressant pour comprendre k8s mais tr√®s peu recommand√©)

Docker for (Mac|Windows) : ‚Äúminikube‚Äù int√©gr√© dans la version desktop de docker (√† activer manuellement)

Kops : cli pour cr√©er un cluster k8s sur des VMs chez des cloud provider AWS, GCE

##==##

<!-- .slide:-->

# Installation du cluster : production

- On premise
  - Kubeadm
- Cloud
  - GKE (Google)
  - AKS (Azure)
  - EKS (AWS)
    $ OVH
  - ‚Ä¶.

Notes:

Kubeadm : la version pr√©conis√©e aujourd‚Äôhui pour une installation on premise

Kubernetes Manag√© : GCP, Amazon, Azure, mais aussi Digital Ocean, OVH (and counting‚Ä¶)

##==##

<!-- .slide: class="transition-bg-sfeir-3"-->

# Service mesh

##==##

<!-- .slide:-->

# Istio

- Routage
- S√©curit√©
- Observabilit√©
- En option sur GKE
- CNCF

Notes:
routage : canary release / circuit breaker

s√©curit√© : SSL entre les services, authentification, autorisation

observabilit√© : remonte des m√©triques de connexion entre les services

cncf : le projet fait partie comme kubernetes de la cncf

##==##

<!-- .slide: class="transition-bg-sfeir-3"-->

# Architecture micro-service

##==##

<!-- .slide:-->

# Les 12 facteurs des applications Cloud

- Base de code : syst√®me de contr√¥le de version
- D√©pendances : D√©clarez explicitement et isolez les d√©pendances
- Configuration : Stockez la configuration dans l‚Äôenvironnement
- Services externes : ressources rattach√©es
- Build, release, run : S√©parez le build/packaging et d‚Äôex√©cution
- Processus : Ex√©cutez l‚Äôapplication comme un ou plusieurs processus sans √©tat

Notes:
I. Base de code
Une base de code suivie avec un syst√®me de contr√¥le de version, plusieurs d√©ploiements
II. D√©pendances
D√©clarez explicitement et isolez les d√©pendances
III. Configuration
Stockez la configuration dans l‚Äôenvironnement ===> ConfigMap, Secrets, Consul, ‚Ä¶

IV. Services externes
Traitez les services externes comme des ressources attach√©es
V. Build, release, run
S√©parez strictement les √©tapes d‚Äôassemblage et d‚Äôex√©cution
VI. Processus
Ex√©cutez l‚Äôapplication comme un ou plusieurs processus sans √©tat
VII. Associations de ports
Exportez les services via des associations de ports
VIII. Concurrence
Grossissez √† l‚Äôaide du mod√®le de processus
IX. Jetable
Maximisez la robustesse avec des d√©marrages rapides et des arr√™ts gracieux
X. Parit√© dev/prod
Gardez le d√©veloppement, la validation et la production aussi proches que possible
XI. Logs
Traitez les logs comme des flux d‚Äô√©v√®nements
XII. Processus d‚Äôadministration
Lancez les processus d‚Äôadministration et de maintenance comme des one-off-processes

##==##

<!-- .slide:-->

# Les 12 facteurs des applications Cloud

- Associations de ports : Exportez les services via des associations de ports
- VIII. Concurrence : Grossissez √† l‚Äôaide du mod√®le de processus
- IX. Jetable : Maximisez la robustesse avec des d√©marrages rapides et des arr√™ts gracieux
- X. Parit√© dev/prod : Gardez le d√©veloppement, la validation et la production aussi proches que possible
- XI. Logs : Traitez les logs comme des flux d'√©v√©nements
- XII. Processus d‚Äôadministration : Lancez les processus d‚Äôadministration et de maintenance comme des one-off-processes

Notes:
I. Base de code
Une base de code suivie avec un syst√®me de contr√¥le de version, plusieurs d√©ploiements
II. D√©pendances
D√©clarez explicitement et isolez les d√©pendances
III. Configuration
Stockez la configuration dans l‚Äôenvironnement ===> ConfigMap, Secrets, Consul, ‚Ä¶

IV. Services externes
Traitez les services externes comme des ressources attach√©es
V. Build, release, run
S√©parer strictement les √©tapes d‚Äôassemblage et d‚Äôex√©cution
VI. Processus
Ex√©cutez l‚Äôapplication comme un ou plusieurs processus sans √©tat
VII. Associations de ports
Exportez les services via des associations de ports
VIII. Concurrence
Grossissez √† l‚Äôaide du mod√®le de processus
IX. Jetable
Maximisez la robustesse avec des d√©marrages rapides et des arr√™ts gracieux
X. Parit√© dev/prod
Gardez le d√©veloppement, la validation et la production aussi proches que possible
XI. Logs
Traitez les logs comme des flux d'√©v√©nements
XII. Processus d‚Äôadministration
Lancez les processus d‚Äôadministration et de maintenance comme des one-off-processes
