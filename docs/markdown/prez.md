##==##

<!-- .slide: class="first-slide"  sfeir-level="1"  sfeir-techno="" -->

# Kubernetes, les fondamentaux

![](./assets/images/g2adcb44acd5_0_564.png)

##==##

<!-- .slide: class="speaker-slide" -->

# Pr√©sentation

<!-- Please choose for each image between speaker or company -->![speaker company](./assets/images/g2ad4208251d_6_37.png)

![speaker company](./assets/images/g2ad4208251d_6_43.png)

![speaker company](./assets/images/g2ad4208251d_6_44.png)

![speaker company](./assets/images/g2ad4208251d_6_45.png)

![speaker company](./assets/images/g2ad4208251d_6_46.png)

![speaker company](./assets/images/g2ad4208251d_6_47.png)

![speaker company](./assets/images/g2ad4208251d_6_48.png)

##==##

<!-- .slide:-->

![](./assets/images/gb710258689_0_1.png)

##==##

<!-- .slide:-->

Pour assister √† cette formation, il est n√©cessaire de conna√Ætre et comprendre les notions de base associ√©es aux conteneurs. Vous √™tes capable de construire une image (par exemple avec un Dockerfile), lancer un conteneur, l'arr√™ter, inspecter ses logs.
Un navigateur
Un compte GCP

# Pr√©-requis

Notes:
Note : ce slide sera √† exporter dans un document √† part pour les SfeirSchools

Sondage :

qui conna√Æt docker ?

qui conna√Æt un orchestrateur de container ?

qui conna√Æt kubernetes ?

##==##

<!-- .slide:-->

# Self-link

https://bit.ly/sfeir-school-k8s-2024

##==##

<!-- .slide:-->

# Agenda

Rappel sur Docker
Kubernetes : les origines
Premier aper√ßu
Architecture interne
Getting started
Configuration d‚Äôune application
Mise √† l‚Äô√©chelle et mise √† jour
Stocker des donn√©es
Gestion avanc√©e de Pods
M√©thodes d‚Äôinstallation de clusters

##==##

<!-- .slide:-->

# Rappels sur Docker

##==##

<!-- .slide:-->

# Qu‚Äôest-ce que Docker ?

Docker permet de packager une application
avec l‚Äôensemble de ses d√©pendances
dans une unit√© standardis√©e pour le d√©ploiement de logiciels :

les CONTAINERS

Notes:
Docker en une phrase

##==##

<!-- .slide:-->

# Utilisation de Docker

Database SQL

Frontend application

Backend application

Database NoSql

PROD

DEV

UAT

PREPROD

![](./assets/images/C:\Users\zandolsi\Downloads\t√©l√©chargement.png)

![](./assets/images/C:\Users\zandolsi\Downloads\Generic_Databases.png)

![](./assets/images/C:\Users\zandolsi\Downloads\Generic_Databases.png)

![](./assets/images/C:\Users\zandolsi\Downloads\Generic_Databases.png)

![](./assets/images/C:\Users\zandolsi\Downloads\Generic_Databases.png)

![](./assets/images/C:\Users\zandolsi\Downloads\14822516839733_sevreur.png)

![](./assets/images/C:\Users\zandolsi\Downloads\14822516839733_sevreur.png)

![](./assets/images/C:\Users\zandolsi\Downloads\14822516839733_sevreur.png)

![](./assets/images/C:\Users\zandolsi\Downloads\14822516839733_sevreur.png)

![](./assets/images/C:\Users\zandolsi\Downloads\t√©l√©chargement (1).png)

Notes:
En informatique, le container peut aussi embarquer une grande vari√©t√© de contenus.

Il s‚Äôex√©cute √† l‚Äôidentique dans plusieurs environnements sans √™tre modifi√©.

##==##

<!-- .slide:-->

# VM vs Containers

![](./assets/images/g41f33631ff_0_169.png)

![](./assets/images/g41f33631ff_0_170.png)

Notes:
Avantages de Docker :

permet un partage des ressources efficace

plus simple pour g√©rer des environnements d‚Äôex√©cution vari√©s

Avantages des full VMs :

isolation GARANTIE puisqu‚Äôil n‚Äôy a aucune communications entre l'h√¥te et la machine virtuelle

acc√®s au mat√©riel plus simple

##==##

<!-- .slide:-->

# Rappel sur les containers

![](./assets/images/g3f0c37370d_0_342.png)

Notes:
Le Dockerfile :

contient les instructions pour construire l‚Äôimage

Les images :

contiennent les fichiers n√©cessaires √† l‚Äôex√©cution de votre application : binaires, d√©pendances, configuration, ‚Ä¶

sont immuable

Les containers :

isolent l‚Äôex√©cution de processus

dans le syst√®me de fichier de l‚Äôimage

au niveau r√©seau

sont con√ßus pour √™tre √©ph√©m√®res

La registry :

permet le partage d‚Äôimages

Docker Hub, Google Container Registry, ...

##==##

<!-- .slide:-->

# Kubernetes : les origines

##==##

<!-- .slide:-->

# Google : 20 ans de containers

![](./assets/images/g3f0c37370d_0_482.png)

Notes:
Google, 20 ans d‚Äôexp√©rience sur l‚Äôorchestration de containers

Borg = c++, ~2003

##==##

<!-- .slide:-->

# Cloud Native Computing Foundation

landscape.cncf.io

![](./assets/images/g3f0c37370d_0_492.png)

![](./assets/images/g3f0c37370d_0_527.png)

Notes:
Kubernetes r√©ecrit en Go par les Googlers

Offert √† la Cloud Native Computing Foundation en 2015

Kubernetes n‚Äôest qu‚Äôun outils parmi tous les outils pouvant vous aider √† cr√©er des architectures ‚Äú
Cloud-Natives
‚Äù

La
Fondation Linux
(en anglais
Linux Foundation
) est un
consortium
√† but non lucratif fond√© le
21

janvier

2007
, a pour mission de prot√©ger et standardiser
Linux
en procurant les ressources et services centralis√©s, la Linux Foundation regroupe 70 membres.

##==##

<!-- .slide: class="with-code" -->

# Kubernetes, pour quoi faire ?

```
Lancer 5 containers bas√©s sur l‚Äôimage redis:4.0
Mettre en place un load-balancer interne au cluster pour servir ces 5 containers
Lancer 10 containers webapp:1.0
Mettre en place un load-balancer public pour permettre d‚Äôacc√©der aux containers de l‚Äôext√©rieur du cluster
Augmenter le nombre de containers webapp pendant les soldes üòâ
Continuer √† servir les requ√™tes pendant la mise √† jour vers webapp:2.0

```

![](./assets/images/g3f0c37370d_0_502.png)

Notes:
Orchestration de containers dans un cluster de machines

‚Äúbarreur d'un navire‚Äù en grec

##==##

<!-- .slide: class="with-code" -->

# Mais aussi

```
Mise √† l‚Äô√©chelle automatique
D√©ploiement ‚ÄúBlue/green‚Äù et ‚ÄúCanary‚Äù
Ex√©cution de traitements unitaires ou r√©p√©t√©s
Prioriser les t√¢ches en cas de manque de ressources sur le cluster
Ex√©cuter des services qui persistent des donn√©es sur disque
Contr√¥ler l‚Äôacc√®s aux diff√©rentes ressources
Automatiser les t√¢ches complexes (‚Äúoperators‚Äù)

```

Notes:
Orchestration de containers dans un cluster de machines

##==##

<!-- .slide:-->

# Question 1

texte

choix 1
choix 2
choix 3

##==##

<!-- .slide:-->

# Question 1

texte

choix 1
choix 2
choix 3

##==##

<!-- .slide:-->

# Question 1

Quelle est la diff√©rence entre une VM et un container?

Un container fait tourner un nouvel OS, √©mule du mat√©riel physique tandis qu‚Äôune VM partage les m√™mes OS et noyau que son h√¥te et r√©alise une virtualisation au niveau syst√®me
Une VM fait tourner un nouvel OS, √©mule du mat√©riel physique tandis qu‚Äôun container partage les m√™mes OS et noyau que son h√¥te et r√©alise une virtualisation au niveau syst√®me
Les deux sont des moyens de virtualisation mais le container peut tourner sur tous les OS contrairement √† la VM

##==##

<!-- .slide:-->

# Question 1

Quelle est la diff√©rence entre une VM et un container?

Une VM fait tourner un nouvel OS, √©mule du mat√©riel physique tandis qu‚Äôun container partage les m√™mes OS et noyau que son h√¥te et r√©alise une virtualisation au niveau syst√®me
Un container fait tourner un nouvel OS, √©mule du mat√©riel physique tandis qu‚Äôune VM partage les m√™mes OS et noyau que son h√¥te et r√©alise une virtualisation au niveau syst√®me
Les deux sont des moyens de virtualisation mais le container peut tourner sur tous les OS contrairement √† la VM

##==##

<!-- .slide:-->

# Premier aper√ßu

##==##

<!-- .slide:-->

# Cluster

![](./assets/images/g3f3310ef84_0_410.png)

Notes:
Un cluster est un ensemble de machines qui collaborent entre elles.

Sur Kubernetes, on distingue le master des noeuds (nodes).

##==##

<!-- .slide:-->

# Ex√©cuter des applications

![](./assets/images/g3f3310ef84_0_411.png)

Notes:
Le master est responsable de d‚Äôessentiel de la partie ‚Äúcontr√¥le‚Äù (control plane) du cluster.

Les noeuds (nodes) sont responsables de l‚Äôex√©cution des applications.

Les fonctions de master et de node sont habituellement d√©ploy√©es sur des machines diff√©rentes.

En dev ces deux fonctions peuvent √™tre d√©ploy√©es sur une unique machine (ex: Minikube)

##==##

<!-- .slide:-->

# Node

![](./assets/images/g3f3310ef84_0_418.png)

Notes:
Les noeuds ex√©cutent les applications packag√©es dans des containers regroup√©s dans des ‚ÄúPods‚Äù.

L‚Äôex√©cution des Pods est g√©r√©e par les kubelet.

Kubelet est un process qui pilote l'engine docker pour deployer les resources

##==##

<!-- .slide:-->

# Pod

![](./assets/images/g40daa77750_5_116.png)

Notes:
Pod :

1 ou plusieurs containers

en g√©n√©ral un seul

partageant :

une seule IP

un ou plusieurs volumes

##==##

<!-- .slide:-->

# Patterns

![](./assets/images/g3f3310ef84_0_993.png)

Notes:
patterns :

sidecar

file-puller + webserver

log forwarder

adapter

nginx + php_fpm

ambassador

##==##

<!-- .slide:-->

# Orchestrer les pods

![](./assets/images/g3386b050b1_0_69.png)

Notes:
Objectif du slide :
premier aper√ßu des objets Kubernetes.
Ne pas d√©tailler plus que les commentaires ci-dessous !

On parle d‚Äôobjets Kubernetes, de
ressources
.

Chaque ressource repr√©sente une API.

ReplicaSet :

maintient le nombre demand√© de pod identiques

la plupart du temps cr√©√© par le Deployment

Deployment :

g√®re le cycle de vie des pods : versions, mise √† jour continue (sans interruption)

Service :

load-balancer interne vers un ensemble de pods

accessible par IP ou par dns interne

Ingress :

Point d‚Äôentr√©e HTTP(S)

routage par Path ou par Virtualhost

Namespace :

pour isoler des ressources

On reverra plus en d√©tails ces objets et d‚Äôautres : ConfigMap, DaemonSet, Job, ...

##==##

<!-- .slide:-->

# Question 2

Quelles sont les deux types de machines que l‚Äôon trouve dans un cluster Kubernetes

Master et node
Deployment et pod
Kubelet et container

##==##

<!-- .slide:-->

# Question 2

Quelles sont les deux types de machines que l‚Äôon trouve dans un cluster Kubernetes

Master et node
Deployment et pod
Kubelet et container

##==##

<!-- .slide:-->

# Architecture interne

##==##

<!-- .slide:-->

# Master et Nodes

devs, ops

clients

master(s)

Notes:
Cluster :

1 ou plusieurs masters (
recommand√© pour la
HA - Haute Disponibilit√©)

1 ou plusieurs noeuds (
recommand√© pour la
HA - Haute Disponibilit√©)

Les ops passent des commandes au master

Les clients acc√®dent aux pods via les noeuds

##==##

<!-- .slide:-->

devs, ops
kubectl

master(s)

# ApiServer et etcd

Valide

Stocke

Notes:
ApiServer

Devs ou ops envoient la description de l‚Äô√©tat voulu des objets

Fichiers au format .yaml, transform√©s en json pour envoi sur l‚Äôapiserver

Objets stock√©s dans serveur etcd (serveur cl√©-valeur distribu√©)

##==##

<!-- .slide:-->

master(s)

# Kubelet

nodes

Notes:
Kubelet

Tourne sur les noeuds

Ex√©cute les pods

S‚Äôenregistre aupr√®s du master via l‚Äôapiserver

##==##

<!-- .slide:-->

master(s)

# Controllers

nodes

Notes:
Controllers

Il y a plusieurs contr√¥leurs, un par type d‚Äôobjet Kubernetes

Ils ont chacun une responsabilit√© pr√©cise

Ils surveillent constamment la configuration de l‚Äôetcd

Ils r√©agissent au changement pour atteindre l‚Äô√©tat demand√©, en mettant √† jour la configuration via l‚Äôapiserver. Exemple :

le Scheduler met √† jour la configuration du pod pour lui assigner un noeud

le kubelet (lui-aussi un type de contr√¥leur) ex√©cute le pod demand√© puis enregistre son statut dans la configuration

##==##

<!-- .slide:-->

# R√©capitulatif

Le Kubelet s‚Äôenregistre dans la config

L‚Äôutilisateur envoi l‚Äô√©tat voulu sur etcd via l‚Äôapiserver

Les contr√¥leurs et le Kubelet r√©agissent aux changements d‚Äô√©tats pour obtenir et maintenir l‚Äô√©tat voulu

![](./assets/images/gb710258689_0_72.png)

##==##

<!-- .slide:-->

# Question 1

Quelle est la diff√©rence entre une VM et un container?

Un container fait tourner un nouvel OS, √©mule du mat√©riel physique tandis qu‚Äôune VM partage les m√™mes OS et noyau que son h√¥te et r√©alise une virtualisation au niveau syst√®me
Une VM fait tourner un nouvel OS, √©mule du mat√©riel physique tandis qu‚Äôun container partage les m√™mes OS et noyau que son h√¥te et r√©alise une virtualisation au niveau syst√®me
Les deux sont des moyens de virtualisation mais le container peut tourner sur tous les OS contrairement √† la VM

##==##

<!-- .slide:-->

# √Ä venir...

![](./assets/images/g3f3310ef84_0_1014.png)

Notes:
Ce qu‚Äôon voit dans le chapitre suivant

##==##

<!-- .slide:-->

# Question 3

Quel est le composant install√© sur tous les noeuds interagissant avec l‚ÄôAPI Server et g√©rant les containers cr√©√©s dans Kubernetes?

Kubelet
Docker
Systemd

##==##

<!-- .slide:-->

# Question 3

Quel est le composant install√© sur tous les noeuds interagissant avec l‚ÄôAPI Server et g√©rant les containers cr√©√©s dans Kubernetes?

Kubelet
Docker
Systemd

##==##

<!-- .slide:-->

# Question 4

Quel est le composant principal recevant toutes les requ√™tes de configuration et de modification de l‚Äô√©tat du cluster?

Kubelet
Kube-api-server
Kube-proxy

##==##

<!-- .slide:-->

# Question 4

Quel est le composant principal recevant toutes les requ√™tes de configuration et de modification de l‚Äô√©tat du cluster?

Kubelet
Kube-api-server
Kube-proxy

##==##

<!-- .slide:-->

# Question 5

Quel est le composant stockant l‚Äô√©tat des ressources du cluster?

Controllers
Master
ETCD

##==##

<!-- .slide:-->

# Question 5

Quel est le composant stockant l‚Äô√©tat des ressources du cluster?

Controllers
Master
ETCD

##==##

<!-- .slide:-->

# Question 6

Quel est le composant choisissant sur quel noeud se lance les pods?

Kubelet
Scheduler
Kube-proxy

##==##

<!-- .slide:-->

# Question 6

Quel est le composant choisissant sur quel noeud se lance les pods?

Kubelet
Scheduler
Kube-proxy

##==##

<!-- .slide:-->

# Question 7

Quel sont les composants permettant d‚Äôatteindre les √©tats voulus des ressources au sein du cluster?

Controllers
Kubelets
Nodes

##==##

<!-- .slide:-->

# Question 7

Quel sont les composants permettant d‚Äôatteindre les √©tats voulus des ressources au sein du cluster?

Controllers
Kubelets
Nodes

##==##

<!-- .slide:-->

# Question 8

Quel est le composant permettant le routage r√©seau au sein du cluster?

Kubelet
Scheduler
Kube-proxy

##==##

<!-- .slide:-->

# Question 8

Quel est le composant permettant le routage r√©seau au sein du cluster?

Kubelet
Scheduler
Kube-proxy

##==##

<!-- .slide:-->

# Getting started

##==##

<!-- .slide:-->

# Cr√©ation de cluster

Configuration de kubectl

##==##

<!-- .slide:-->

Google Kubernetes Engine

D√©monstration en direct

# Cr√©er un cluster GKE

Notes:
Cr√©er un cluster en 3 clics depuis la Console GCP

Montrer les options disponibles depuis l‚Äôinterface graphique :

version Kubernetes

taille de VM

type d‚ÄôOS (COS ou Ubuntu)

activation Stackdriver Monitoring / Logging

‚Ä¶

Note : ne pas d√©montrer ici les avantages de GKE vs K8S natif

##==##

<!-- .slide: class="with-code" -->

# kubectl

```
Client Kubernetes en ligne de commande
Astuces :
kubectl version
kubectl completion -h
kubectl help <command>
kubectl explain [--recursive] <resource>
kubectl <verb> <resource>

```

Notes:
Comme Docker, Kubernetes s‚Äôutilise en mode client-server.

Le client s‚Äôappelle ‚Äú
kubectl
‚Äù et appelle un composant du cluster appell√©
apiserver
.

kubectl int√®gre un m√©canisme pour installer l‚Äôauto-completion des commandes dans bash et zsh

On a aussi acc√®s √† la
doc des commandes
et au
format des ressources
avec les sous-commandes
help
et
explain
.

‚Ä¶ d√©taill√© dans la suite

##==##

<!-- .slide:-->

# ~/.kube/config

<< auth-provider >>

Notes:
La configuration du client kubectl se fait dans le fichier ~/.kube/config

Ce fichier r√©pertorie les ‚Äúcontextes‚Äù, constitu√©es du couple Cluster et User, ainsi que le namespace courant sur ce cluster.

Sur GKE, la partie user est d√©l√©gu√©e √† un fournisseur d‚Äôauthentification impl√©ment√© par la commande
gcloud
du Cloud SDK.

##==##

<!-- .slide: class="with-code" -->

```
gcloud container clusters list

gcloud container clusters get-credentials \   --zone europe-west1-a <mycluster>

```

# Contextes Kubernetes sur GKE

Notes:
La seconde commande permet de cr√©er le contexte kubectl correspondant √† une instance GKE.

##==##

<!-- .slide: class="with-code"  class="with-code"  class="with-code" -->

```
$ kubectl config get-contexts # Lister les contexts
$ kubectl config set-context gke-dev # Mettre √† jour les contexts
$ kubectl config view --minify=true # Afficher current-context configuration

```

# Changer de contexte

```
apiVersion: v1kind: Configcurrent-context: gke-contextcontexts:- name: gke-context  context:    cluster: gke-cluster    user: gke-userclusters:- name: gke-cluster  cluster:    server: https://12.34.56.78

```

```
users:- name: gke-user  user:    auth-provider:      name: gcp      config:        cmd-path: /usr/bin/gcloud        cmd-args: config config-helper --format=json        access-token: ya29.GlwXBuG[...]KcYlQ        [...]


```

Notes:
Exemple de fichier de config de kubectl (~/.kube/config).

On y retrouve les contextes, les clusters, les utilisateurs

On voit aussi l‚Äôutilisation de gcloud comme fournisseur d‚Äôauthentification

##==##

<!-- .slide: class="with-code" -->

# Aide-m√©moire des commandes de kubectl

```
https://kubernetes.io/fr/docs/reference/kubectl/cheatsheet/



```

##==##

<!-- .slide: class="with-code" -->

# Utilitaires autour de kubectl

```
https://kubernetes.io/docs/reference/kubectl/quick-reference/#kubectl-autocomplete
$ kubectl <TAB>		# auto-compl√©tion
$ k <TAB>				# alias
https://github.com/ahmetb/kubectx$ kubectx gke-dev		# changer de contexte$ kubens kube-system	# modifier le ns du contexte courant

```

##==##

<!-- .slide: class="with-code" -->

# Utilitaires autour de kubectl

```
~800 kubectl aliases (bash/zsh)https://github.com/ahmetb/kubectl-aliases$ kgpo		# kubectl get pod
Shell prompthttps://github.com/jonmosco/kube-ps1

https://github.com/mfuentesg/ksd$ kubectl get secret <secret name> -o <yaml|json> | ksd
 # D√©coder les √©l√©ments d‚Äôun secret en base 64

```

![](./assets/images/gb710258689_0_63.png)

##==##

<!-- .slide:-->

# k9scli.io

![](./assets/images/gb66a827f2c_0_0.png)

Notes:
Installer l‚Äôextension Chrome suivante pour lire l‚Äôasciinema en live dans les slides :

https://chrome.google.com/webstore/detail/google-slides-asciinema/lbaccocfalidoaeacbpabonnljdndmdd

##==##

<!-- .slide:-->

# Namespace

##==##

<!-- .slide:-->

# Namespace

Espace de nom pour isoler les d√©ploiements
Peut √™tre utilis√© pour s√©parer les environnements (soft multi-tenant)
Le nom d‚Äôune ressource est unique au sein d‚Äôun namespace.
Par d√©faut:
default
kube-system
kube-public

Notes:
nom de ressource unique dans un namespace
(mais pas au sein d‚Äôun cluster)

default : namespace de travail par d√©faut

kube-system : namespace pour les d√©ploiements internes au cluster

kube-public : namespace cr√©√© par d√©faut pour les ressources expos√©s publiquement

##==##

<!-- .slide: class="with-code" -->

```
Clonez le d√©p√¥t github suivant :
git clone https://github.com/sfeir-open-source/sfeir-school-kubernetes.git

et placez-vous dans le d√©p√¥t clon√© :
cd sfeir-school-kubernetes/steps

```

# R√©cup√©rer les exercices

Notes:
√Ä faire

dans Cloud Shell

OU

sur les machines locales si
git, kubectl et Cloud SDK
install√©s

##==##

<!-- .slide:-->

# TP : Configurer kubectl

##==##

<!-- .slide:-->

# TP : Configurer kubectl

!! Faites une sauvegarde du fichier ~/.kube/config !!

##==##

<!-- .slide:-->

# TP : Configurer kubectl

Installer gcloud & kubectl
Configurer kubectl
$ gcloud container clusters get-credentials <cluster> --zone <zone>
$ echo 'source <(kubectl completion bash)' >>~/.bashrc

##==##

<!-- .slide: class="with-code" -->

# TP : Configurer kubectl

```
$ alias k=‚Äùkubectl‚Äù
$ kubectl get nodes # Lister les noeuds du cluster
$ kubectl get namespace # Lister les espaces de nom
$ kubectl config set-context --current --namespace=<insert-namespace-name-here>
$ kubectl config view --minify=true



```

##==##

<!-- .slide:-->

# Manifest

Notes:
Configuration files - Written in YAML or JSON, these files describe the desired state of your application in terms of Kubernetes API objects. A file can include one or more API object descriptions (manifests). (See the example YAML from the stateless app).

Fichier de configuration : √©crit en yaml ou json, ce fichier d√©crit l‚Äô√©tat d√©sir√© d‚Äôune ressource Kubernetes. Dans un fichier, on peut inclure une ou plusieurs descriptions d‚Äôobjets

##==##

<!-- .slide: class="with-code" -->

```
kind: PodapiVersion: v1metadata:  name: nginx  labels:    app: nginxspec:  containers:  - name: nginx    image: nginx:alpinestatus:  ...

```

# Manifest

Notes:
kind : type de resource que l‚Äôon va cr√©√©

apiVersion: version de l‚Äôapi de la ressource que l‚Äôon veut cr√©√©

metadata: information qui permette d‚Äôidentifier la resource, name est obligatoire

spec: d√©finit l‚Äô√©tat d√©sir√© de notre ressource

status : √©tat actuel de la ressource

##==##

<!-- .slide:-->

# Cr√©er et interagir avec des objets Kubernetes

##==##

<!-- .slide: class="with-code" -->

# Mode d√©claratif (pr√©f√©r√©)

```
$ kubectl apply -f my-pod.yaml # Modifier ou mettre √† jour l‚Äôobjet$ kubectl replace -f my-pod.yaml # Recr√©er l‚Äôobjet$ kubectl diff -f my-pod.yaml # Afficher les diff√©rences de l‚Äôobjet$ kubectl delete -f my-pod.yaml # Supprimer l‚Äôobjet

```

##==##

<!-- .slide: class="with-code" -->

# Mode imp√©ratif

```
$ kubectl create <type-of-object> [<subtype-of-object>] <name-of-object> <properties> # Cr√©er un objet$ kubectl create namespace <my-namespace> # Cr√©er un espace de nom$ kubectl run <pod-name> --image=<image> # Cr√©er un pod$ kubectl run <pod-name> --image=<image> --dry-run=client -oyaml > my-pod.yaml
# G√©n√©rer le yaml du pod sans le cr√©er sur le cluster

```

##==##

<!-- .slide:-->

# Cr√©er et interagir avec un pod

##==##

<!-- .slide: class="with-code" -->

# kubectl get pods

```
$ kubectl get podsNAME                     READY     STATUS    RESTARTS   AGEnginx-5dbbff858d-qfb7f   1/1       Running   0          2m
$ kubectl get po -oyaml nginx-5dbbff858d-qfb7f
apiVersion: v1kind: Pod
metadata:
  name: nginx-5dbbff858d-qfb7f...

```

Notes:
READY : nombre de containers d√©marr√©s / nombre total de container

RESTART : si un container n‚Äôest pas dans l‚Äô√©tat voulu (crash, indisponibilit√©), le pod peut √™tre red√©marr√©

##==##

<!-- .slide: class="with-code" -->

```
apiVersion: v1kind: Podmetadata:  name: nginx  labels:    app: nginxspec:  containers:  - name: nginx    image: nginx:alpinestatus:  ...

```

# Pod yaml

##==##

<!-- .slide: class="with-code" -->

```
$ kubectl explain podKIND:     PodVERSION:  v1DESCRIPTION:     Pod is a collection of containers that can run on a host. This resource is     created by clients and scheduled onto hosts.FIELDS:   apiVersion	<string>   kind	<string>   metadata	<Object>   spec	<Object>[...]

```

# Pods explained

Notes:
La commande
explain
donne tous les d√©tails sur les propri√©t√©s de chaque ressources Kubernetes‚Ä¶

‚Ä¶ r√©cursivement (slides suivant)

##==##

<!-- .slide: class="with-code" -->

```
$ kubectl explain pod.spec.containers.imageKIND:     PodVERSION:  v1FIELD:    image <string>DESCRIPTION:     Docker image name. More info:     https://kubernetes.io/docs/concepts/containers/images This field is     optional to allow higher level config management to default or override     container images in workload controllers like Deployments and     StatefulSets.

```

# Pods explained‚Ä¶ deeper

##==##

<!-- .slide: class="with-code" -->

```
...status:  containerStatuses:  - name: nginx    containerID: docker://8db3af41eb87[...]3d103255    image: nginx:alpine    imageID: docker-pullable://nginx@sha256:1aed1[...]a0a5440    state:      running:        startedAt: 2018-08-13T21:08:10Z  hostIP: 192.168.65.3  podIP: 10.1.0.227

```

# Pod status (live)

Notes:
Une fois le fichier yaml envoy√© dans la configuration, les contr√¥leurs viennent mettre √† jour le pod dans la partie status.

On retrouve entre autre l‚ÄôIP du
node
qui l‚Äôh√©berge et l‚ÄôIP du
pod
lui-m√™me.

##==##

<!-- .slide:-->

# TP : Pod

##==##

<!-- .slide:-->

# TP : Pod

Regarder le code pour cr√©er un pod
$ cat pod/monolith.yaml
Cr√©er le pod :
$ kubectl apply -f pod/monolith.yaml

Notes:
Cr√©ation d‚Äôun pod simple : on affiche les specs du pod, et on cr√©√© le pod avec kubectl.

##==##

<!-- .slide:-->

# TP : Information sur le Pod

$ kubectl get po -owide
$ kubectl describe po <pod-name>
$ kubectl get po <pod-name> -oyaml
Quelle est l‚Äôip du pod ?
Sur quel node le pod tourne ?
Quel container tourne dans le pod ?

##==##

<!-- .slide:-->

# TP : Interagir avec le Pod

Cr√©er un tunnel depuis son poste avec le pod
$ kubectl port-forward monolith 10080:80
Ouvrir une autre fen√™tre shell
$ curl http://127.0.0.1:10080
$ curl http://127.0.0.1:10080/secure
$ curl -u user http://127.0.0.1:10080/login
Le mot de passe est : "password"
$ curl -H "Authorization: Bearer <token>" http://127.0.0.1:10080/secure

##==##

<!-- .slide:-->

# TP : Interagir avec le Pod

Voir les logs du pods
$ kubectl logs monolith
Ouvrir un shell dans le pod
$ kubectl exec monolith -it [-c monolith] -- /bin/sh
Lister les process dans le container
$ ps aux

##==##

<!-- .slide:-->

# Liveness/Readiness

##==##

<!-- .slide:-->

# Readiness/Liveness

Readiness :
V√©rifier que le container est pr√™t √† recevoir du flux

Liveness :
V√©rifier que le container r√©pond correctement

Startup :
V√©rifier que le container est d√©marr√©
D√©sactiver les sondes pr√©c√©dentes en attendant cet √©tat

Notes:
Kubeproxy sortira un pod du flux si le readiness devient ko.

Kubelet va red√©marrer le container si le liveness est ko.

##==##

<!-- .slide: class="with-code" -->

# Readiness/Liveness : command

```

    livenessProbe:
      exec:
        command:
        - cat
        - /tmp/healthy
      initialDelaySeconds: 5
      periodSeconds: 5

```

Notes:
Il est possible d‚Äôutiliser une commande pour tester l‚Äô√©tat du service

##==##

<!-- .slide: class="with-code" -->

# Readiness/Liveness : tcp socket

```
    readinessProbe:
      tcpSocket:
        port: 8080
      initialDelaySeconds: 5
      periodSeconds: 10
    livenessProbe:
      tcpSocket:
        port: 8080
      initialDelaySeconds: 15
     periodSeconds: 20


```

##==##

<!-- .slide: class="with-code" -->

# Readiness/Liveness : http

```
...
spec:
  containers:
    - readinessProbe:    # accepte des requ√™tes quand OK
        httpGet:
          path: /readiness
          port: 81
          scheme: HTTP
        initialDelaySeconds: 5
        timeoutSeconds: 1
      livenessProbe:    # red√©marre le pod si KO
        httpGet:
          path: /healthz
          port: 81
          scheme: HTTP
        initialDelaySeconds: 5 # D√©lai avant la premi√®re requ√™te
        periodSeconds: 15      # Fr√©quence des tests
        timeoutSeconds: 5

      ...


```

Notes:
readinessProbe indique √† kubelet quand un pod est pr√™t √† servir du traffic

livenessProbe indique si le pod est ok, si nok kubelet red√©marre le pod

##==##

<!-- .slide:-->

# TP : Readiness/Liveness

##==##

<!-- .slide:-->

# TP : Readiness/Liveness

Regarder le contenu du pod
$ cat readiness/healthy-monolith.yaml
$ kubectl apply -f readiness/healthy-monolith.yaml
$ kubectl describe pods/healthy-monolith

##==##

<!-- .slide: class="with-code" -->

# TP : Readiness/Liveness

```
$ kubectl describe pods

Comment est configur√©e la sonde readiness ?
Comment est configur√©e la sonde liveness
√† quelle fr√©quence la surveillance readiness est effectu√©e ?
√† partir de combien de secondes la sonde liveness est effectu√©e ?


```

Notes:
Comment est configur√© la sonde readiness ? httpGet sur /readiness:81

liveness sur /healthz:81

la valeur par d√©faut du readiness : 10 secondes

liveness est effectu√© √† partir de 5 secondes

##==##

<!-- .slide:-->

# TP : Tester les sondes

$ kubectl get pods healthy-monolith
Noter l‚Äô√©tat OK du pod
$ kubectl port-forward healthy-monolith 10081:81
Forcer l‚Äôapplication a pass√© en √©tat ‚Äúfailed‚Äù
$ curl http://127.0.0.1:10081/readiness/status
Attendre 45 secondes que la sonde en failed
$ kubectl describe pods healthy-monolith
Vous pouvez noter dans l‚Äôhistorique le moment o√π le pod est pass√© en ‚Äúunhealthy‚Äù

##==##

<!-- .slide:-->

# TP : Tester les sondes

Maintenant nous allons ‚Äúcasser‚Äù la sonde liveness
$ curl http://127.0.0.1:10081/healthz/status

$ kubectl get pods

Que se passe-t-il quand le liveness est ko ?

Notes:
Que se passe-t-il : dans ce cas le pod est red√©marr√©

##==##

<!-- .slide:-->

# Configuration d‚Äôune application

##==##

<!-- .slide:-->

# Configuration

Configurer l‚ÄôURI d‚Äôune base de donn√©es
Injecter un fichier application-prod.yml
Sp√©cifier les credentials d‚Äôune api

##==##

<!-- .slide: class="with-code" -->

```
apiVersion: v1kind: Podmetadata:  name: envar-demospec:  containers:  - name: envar-demo-container    image: debian    env:    - name: DEMO_GREETING      value: "Hello from the environment"    - name: DEMO_FAREWELL      value: "Such a sweet sorrow"

```

# Variables d‚Äôenvironnement

Notes:
On peut injecter une variable d‚Äôenvironnement via le champ ‚Äúenv‚Äù.

##==##

<!-- .slide:-->

# ConfigMap : cr√©ation

$ kubectl create configmap <map-name> <data-source>
$ kubectl create configmap my-app-conf --from-file=my-app-conf/configmap/application-dev.properties
$ kubectl create cm my-app-conf --from-literal=db-server=mydbserver.mycompany.com

Notes:
Pour injecter un fichier de configuration, on peut injecter un fichier de configuration dans un pod avec un configMap.

On peut cr√©er un fichier de config, depuis :

un fichier ou un r√©pertoire  
--from-file
=

depuis la cli :
--from-literal
=

##==##

<!-- .slide: class="with-code" -->

# ConfigMap: manifests

```
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-app-config
data:
  application.properties: |
    db-server=mydbserver.dev.mycompany.com
    username=my-rw-dbuser

$ kubectl apply -f configmap.yml

```

Notes:
derri√®re le | on peut mettre n‚Äôimporte un fichier non structur√© yaml

##==##

<!-- .slide: class="with-code"  class="with-code" -->

```
apiVersion: v1kind: Podmetadata:  name: configmap-podspec:  containers:    - name: test      image: busybox      volumeMounts:        - name: config-vol          mountPath: /etc/config

```

# ConfigMap : utilisation

```
  volumes:    - name: config-vol      configMap:        name: log-config        items:          - key: log_level            path: log_level

```

Point de montage du fichier : /etc/config/log_level

##==##

<!-- .slide: class="with-code" -->

# Secret : type

```
$ kubectl create secret --help

Available Commands:
  docker-registry Create a secret for use with a Docker registry
  generic         Create a secret from a local file, directory or literal value
  tls             Create a TLS secret



```

Notes:
On peut cr√©er 3 types de secret :

docker-registry : pour enregistrer les credentials pour pull une image docker depuis une registry priv√©e

generic : √©quivalent √† configmap, permet de stocker des secrets de type cl√©/valeur

tls : pour stocker un certificat serveur pour exposer un service en https

##==##

<!-- .slide: class="with-code" -->

# Secret : manifest

```
apiVersion: v1
kind: Secret
metadata:
  name: mysecret
type: Opaque
data:
  username: YWRtaW4=
  password: MWYyZDFlMmU2N2Rm

```

##==##

<!-- .slide:-->

# Secret : cr√©ation

Cr√©er un secret depuis un fichier
$ kubectl create secret generic db-user-pass --from-file=./password.txt
Cr√©er un secret depuis une cl√©/valeur
$ kubectl create secret generic prod-db-secret --from-literal=username=produser
Cr√©er un secret ‚Äúdocker-registry ‚Äù
$ kubectl create secret docker-registry regcred --docker-server=<your-registry-server> --docker-username=<your-name> --docker-password=<your-pword> --docker-email=<your-email>

Notes:
Un secret va permettre de stocker des informations sensibles, comme des mots de passe, des cl√©s priv√©es.

##==##

<!-- .slide: class="with-code" -->

```
apiVersion: v1kind: Podspec:  containers:  - name: envar-demo-container    image: debian    env:    - name: DEMO_GREETING      valueFrom:
secretKeyRef:
name: demo      # le nom du secret
key: password   # la cl√© dans le secret

```

# Secret : env vars

##==##

<!-- .slide: class="with-code"  class="with-code" -->

# Secret : volume

```
...
kind: Pod
spec:
  containers:
  - name: mypod
    image: redis
    volumeMounts:
    - name: secret-volume
      mountPath: "/etc/foo"
      readOnly: true


```

```
  volumes:
  - name: secret-volume
    secret:
      secretName: mysecret

```

##==##

<!-- .slide:-->

# TP : Configuration

##==##

<!-- .slide:-->

# TP : ConfigMap

$ kubectl create configmap nginx-proxy-conf --from-file=configuration/nginx/proxy.conf

$ kubectl describe configmaps nginx-proxy-conf

##==##

<!-- .slide:-->

# Quizz : ConfigMap

Combien y a t-il d‚Äô√©l√©ments dans la configmap nginx-proxy-conf ?
Quel est le nom de ces √©l√©ments ?

Notes:
1 element : le fichier proxy.conf

##==##

<!-- .slide:-->

# TP : Secret

$ kubectl create secret generic tls-certs --from-file=configuration/tls/

$ kubectl describe secrets tls-certs

##==##

<!-- .slide:-->

# Quizz : Secret

Combien y a t-il d‚Äô√©l√©ments dans le Secret tls-cert ?

Quel est le nom de ces √©l√©ments ?

Notes:
Le secret contient 4 elements : ca-key.pem

ca.pem

cert.pem

key.pem

on peut voir que les secrets ne sont pas affiche dans la console

##==##

<!-- .slide:-->

# TP : Injection de fichiers

$ cat configuration/pod/secure-monolith.yaml

Comment le secret est inject√© dans le pod ?

Comment le configmap est inject√© dans le pod ?

Notes:
Les secrets sont inject√©s sous la forme d'un volume

de la meme facon le configMap est injecte sous la forme d'un volume.

##==##

<!-- .slide:-->

# TP : Injection de fichiers

Cr√©er le pod ‚Äúsecure-monolith‚Äù
$ kubectl apply -f configuration/pod/secure-monolith.yaml
Exposer le port sur votre poste
$ kubectl port-forward secure-monolith 10443:443
Faites une requ√™te avec curl
$ curl --cacert configuration/tls/ca.pem https://127.0.0.1:10443

Notes:
Du coup on a inject√© les certificats et la configuration du service nginx, et on arrive bien √† faire une requ√™te en https sur notre service.

Pr√©ciser : ce n‚Äôest pas la bonne m√©thode pour exposer un service en https sur kubernetes ;))

##==##

<!-- .slide:-->

# Service

##==##

<!-- .slide:-->

# Exposer des pods

![](./assets/images/g3f3310ef84_0_277.png)

Notes:
Les pods sont expos√©s au sein du cluster via les
Services
.

Les services obtiennent une IP et un nom DNS interne au cluster.

Ils r√©partissent la charge entre les pods, cibl√©s par des
labels
.

##==##

<!-- .slide:-->

# Service type ClusterIP

IP interne 10.0.1.8

10.0.0.6

10.0.0.7

Notes:
Un Service de type ClusterIP est accessible depuis une IP interne au cluster.

On l‚Äôutilise en g√©n√©ral pour la communication inter-pods.

##==##

<!-- .slide:-->

# Service type NodePort

10.0.0.7

10.0.0.6

10.0.0.7

Notes:
Un service du type nodePort est associ√© √† un port identique sur chaque noeud du cluster.

Ce port est accessible depuis l‚Äôext√©rieur du cluster.

Il doit appartenir √† la plage de ports 30000 - 32767 (configurable sur le master).

On peut le choisir mais il faut mieux laisser faire Kubernetes, qui g√®rera les probl√®mes de ports occup√©s.

Inconv√©nient : il faut donc conna√Ætre l‚Äôadresse IP d‚Äôau moins un noeud du cluster pour acc√©der au service.

##==##

<!-- .slide:-->

# Service type LoadBalancer

10.0.0.6

10.0.0.7

Notes:
Un Service de type LoadBalancer utilise un √©quilibreur de charge externe au cluster.

Ce type de Service est utilis√© sur les plateformes cloud ainsi que dans Docker-for-Desktop.

L‚Äôinconv√©nient c‚Äôest que chaque service aura son propre load balancer avec sa propre IP publique/Internet

##==##

<!-- .slide:-->

# Service type ExternalName

IP interne 10.0.1.8

Notes:
Un Service de type ExternalName fourni un alias DNS de type CNAME.

Il n‚Äôest utilisable qu‚Äôen interne.

##==##

<!-- .slide:-->

# Service avec Endpoint explicite

IP interne 10.0.1.8

Notes:
En interne, Kubernetes g√©n√®re une ressource Endpoint pour chaque Pod cibl√© par le s√©lecteur.

Il est possible de ne pas renseigner de s√©lecteur et √† la place de cr√©er manuellement les ressources Endpoint avec une IP externe au cluster.

C‚Äôest la mani√®re de rajouter une abstraction vers un service externe au cluster. Les Pods ne voient que le Service interne.

##==##

<!-- .slide: class="with-code" -->

```
apiVersion: v1kind: Servicemetadata:  name: nginxspec:  selector:    app: nginx  type: NodePort  ports:  - name: http    port: 80    protocol: TCP    targetPort: 80

```

# Service yaml

Notes:
Le
selector
permet de cibler les pods par leurs labels.

Il y a plusieurs
types
de services qu‚Äôon voit tout de suite.

##==##

<!-- .slide: class="with-code" -->

```
$ kubectl apply -f service.yaml

La ligne de commande suivante donne un r√©sultat √©quivalent au fichier yaml :

$ kubectl expose nginx --port=80 --type=NodePort

```

# Cr√©er un service

##==##

<!-- .slide: class="with-code" -->

# Services

```
$ kubectl get servicesNAME        TYPE       CLUSTER-IP   EXTERNAL-IP PORT(S)       AGEkubernetes  ClusterIP  10.96.0.1    <none>      443/TCP       3dnginx       NodePort   10.97.47.155 <none>      80:31450/TCP  2s$ kubectl get svc -o yaml nginx
apiVersion: v1kind: Service
metadata:
  name: nginx...

```

##==##

<!-- .slide:-->

# Service : DNS interne

Entr√©e DNS : <service>.<namespace>.svc.cluster.local

Et dans le namespace : <service>

Notes:
Pour chaque service cr√©√©, le service est accessible via une entr√©e DNS : <service>.<namespace>.svc.cluster.local

Et √† l‚Äôint√©rieur du namespace, uniquement avec le nom du service

##==##

<!-- .slide:-->

# Question 9

Quel est le type de service basique permettant l‚Äôexposition de pods dans le cluster ?

ClusterIP
NodePort
LoadBalancer

##==##

<!-- .slide:-->

# Question 9

Quel est le type de service basique permettant l‚Äôexposition de pods dans le cluster ?

ClusterIP
NodePort
LoadBalancer

##==##

<!-- .slide:-->

# Question 10

Quel est le type de service permettant l‚Äôexposition √† travers le port d‚Äôun noeud?

ClusterIP
NodePort
LoadBalancer

##==##

<!-- .slide:-->

# Question 10

Quel est le type de service permettant l‚Äôexposition √† travers le port d‚Äôun noeud?

ClusterIP
NodePort
LoadBalancer

##==##

<!-- .slide:-->

# Question 11

Quel est le type de service permettant l‚Äôexposition √† travers un load balancer externe?

ClusterIP
NodePort
LoadBalancer

##==##

<!-- .slide:-->

# Question 11

Quel est le type de service permettant l‚Äôexposition √† travers un load balancer externe?

ClusterIP
NodePort
LoadBalancer

##==##

<!-- .slide:-->

# TP : Service

##==##

<!-- .slide:-->

# TP : Cr√©er un service

Lire le manifest de l‚Äôobjet service
$ cat service/monolith.yaml

Cr√©er le service
$ kubectl apply -f service/monolith.yaml

##==##

<!-- .slide:-->

# TP : Interagir avec le service

$ gcloud compute instances list
NAME ZONE MACHINE_TYPE PREEMPTIBLE INTERNAL_IP EXTERNAL_IP STATUS
gke-istio-workshop-default-pool-e37360ba-369z europe-west1-b n1-standard-1 10.132.0.3 35.205.92.225 RUNNING
gke-istio-workshop-default-pool-e37360ba-3hz4 europe-west1-b n1-standard-1 10.132.0.2 35.240.127.238 RUNNING
...
Prenez l‚Äôune des ips externes et connectez vous-y
$ curl -k https://<EXTERNAL_IP>:31000

Notes:
Le r√©sultat est un √©chec :

curl -k https://35.246.130.12:31000

curl: (7) Failed to connect to 35.246.130.12 port 31000: Connection refused

Normal : il n‚Äôy a pas de label correspondant !

##==##

<!-- .slide:-->

# TP : Service

Pourquoi la requ√™te ne fonctionne pas ?
$ kubectl get svc monolith

$ kubectl describe services monolith
Combien y a t-il de endpoints au service ?

Notes:
Le r√©sultat est un √©chec :

curl -k https://35.246.130.12:31000

curl: (7) Failed to connect to 35.246.130.12 port 31000: Connection refused

Normal : il n‚Äôy a pas de label correspondant !

##==##

<!-- .slide:-->

# TP : Service

Lister les pods par labels
$ kubectl get pods -l "app=monolith"
$ kubectl get pods -l "app=monolith,secure=enabled"
On va labelliser un pod
$ kubectl label pods secure-monolith secure=enabled
Et maintenant ?
$ kubectl get pods -l "app=monolith,secure=enabled"

Notes:
On labellise les pods

##==##

<!-- .slide:-->

# ReplicaSet

##==##

<!-- .slide: class="with-code" -->

# ReplicaSet.yaml

```
apiVersion: apps/v1kind: ReplicaSetmetadata:  name: nginx  labels:    app: nginxspec:  replicas: 1  selector:    matchLabels:      app: nginx  template:    <... pod definition ...>

```

Notes:
Le ReplicaSet inclut la d√©finition du pod dans la partie
template

Le
selector
permet d‚Äôidentifier les pods g√©r√©s par ce ReplicaSet via des
labels
.

La propri√©t√©
replicas
indique le nombre d‚Äôinstances voulues.

##==##

<!-- .slide: class="with-code" -->

# kubectl get replicaset

```
$ kubectl get ReplicaSetNAME               DESIRED   CURRENT   READY     AGEnginx-5dbbff858d   1         1         1         14m

$ kubectl get rs -o yaml nginx-5dbbff858d
apiVersion: apps/v1kind: ReplicaSet
...

```

Notes:
Le ReplicaSet maintient le nombre demand√© de pods
en √©tat de fonctionnement
.

La colonne READY montre les pods qui sont fonctionnels, et pas juste les pods cr√©√©s (
par exemple avec des
containers en cours de cr√©ation ou crash√©s)

##==##

<!-- .slide: class="with-code" -->

# ReplicaSet status (live)

```
...status:  availableReplicas: 1  fullyLabeledReplicas: 1  readyReplicas: 1  replicas: 1

```

Notes:
On voit le status des replicas

##==##

<!-- .slide:-->

# Deployment

##==##

<!-- .slide: class="with-code"  class="with-code" -->

```
apiVersion: apps/v1kind: Deploymentmetadata:  name: nginxspec:  replicas: 1  selector:    matchLabels:      app: nginx  template:
  ...


```

# Deployment.yaml

```
  ...  template:    metadata:      labels:        app: nginx    spec:      containers:      - name: nginx        image: nginx:alpine

```

Notes:
On retrouve dans le Deployment les specs du Pod et du ReplicaSet

##==##

<!-- .slide: class="with-code" -->

# kubectl get deployment

```
$ kubectl get deploymentNAME      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGEnginx     1         1         1            1           1h
$ kubectl get deploy -o yaml nginx
apiVersion: apps/v1kind: Deployment
...

```

Notes:
Le Deployment g√®re des
Pods
via un
ReplicaSet
.

Contrairement au ReplicaSet qui maintient la m√™me configuration de Pod en changeant juste le nombre de replicats,

le Deployment sait g√©rer la mise √† jour des Pods en ‚Äú
Rolling upgrade
‚Äù

##==##

<!-- .slide: class="with-code" -->

```

$ kubectl scale deployment/nginx --replicas=10

$ kubectl get all

```

# Mise √† l‚Äô√©chelle manuelle

Notes:
La commande
scale
permet d‚Äôindiquer au ReplicaSet le nombre de r√©plicas √† maintenir.

On scale ici √† 10 √† instances de pod pour d√©montrer les
mises √† jours progressives
.

##==##

<!-- .slide:-->

# The Label Game

##==##

<!-- .slide:-->

# Les labels Kubernetes

Pour le fonctionnement interne de Kubernetes
ReplicaSet && Deployments ‚áí Pods
Services ‚áí Pods

##==##

<!-- .slide:-->

# ‚ÄúThe label game‚Äù

![](./assets/images/g41f33631ff_0_65.png)

Notes:
Jeu : trouver le s√©lecteur permettant d‚Äôidentifier les pods encadr√©s

##==##

<!-- .slide:-->

# ‚ÄúThe label game‚Äù

![](./assets/images/g41f33631ff_0_68.png)

Notes:
selector:

App: MyApp

##==##

<!-- .slide:-->

# ‚ÄúThe label game‚Äù

![](./assets/images/g41f33631ff_0_73.png)

Notes:
selector:

R√¥le: Interface

##==##

<!-- .slide:-->

# ‚ÄúThe label game‚Äù

![](./assets/images/g41f33631ff_0_78.png)

Notes:
selector:

Phase: Prod

##==##

<!-- .slide:-->

# ‚ÄúThe label game‚Äù

![](./assets/images/g41f33631ff_0_83.png)

Notes:
selector:

Phase: test

R√¥le: BE

##==##

<!-- .slide:-->

# TP : Deployment

##==##

<!-- .slide:-->

# TP : Deployment

On va cr√©er une stack compl√®te hello, auth, frontend

Avec deployment et service

##==##

<!-- .slide:-->

# Architecture de la stack

service hello

service auth

##==##

<!-- .slide:-->

# TP : Deployment

Cr√©er un d√©ploiement :
$ kubectl apply -f deployments/auth.yaml
$ kubectl describe deployments auth

Et cr√©er le service correspondant
$ kubectl apply -f deployments/service-auth.yaml

##==##

<!-- .slide:-->

# TP : Deployment Hello

Idem pour le service Hello
$ kubectl apply -f deployments/hello.yaml
$ kubectl describe deployments hello
$ kubectl apply -f deployments/service-hello.yaml

##==##

<!-- .slide:-->

# TP : Deployment Frontend

Et pour le service Front
$ kubectl create configmap nginx-frontend-conf --from-file=configuration/nginx/
$ kubectl apply -f deployments/frontend.yaml
$ kubectl apply -f deployments/service-frontend.yaml
$
Quizz : Comment le frontend a acc√®s au service auth et hello ?

##==##

<!-- .slide:-->

# TP : C‚Äôest les soldes !

##==##

<!-- .slide:-->

# TP : Scaling Deployment

On va maintenant augmenter le nombre de pod

$ kubectl scale deployments hello --replicas=3
$ kubectl describe deployments hello
$ kubectl get pods
$ kubectl get replicasets

Notes:
combien

##==##

<!-- .slide:-->

# TP : Scaling Frontend

Editer le service frontend pour scaler √† 2 replicas
$ vim deployments/frontend.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
name: frontend
spec:
replicas: 2
‚Ä¶
$ kubectl apply -f deployments/frontend.yaml

##==##

<!-- .slide:-->

# Mise √† l‚Äô√©chelle et mise √† jour

##==##

<!-- .slide:-->

# Rolling upgrade 1/4

![](./assets/images/g3f3310ef84_0_1030.png)

Notes:
Etat initial : 4 Pods v1 r√©partis sur 3 noeuds

On va modifier la configuration du Deployment pour utiliser une autre version du pod (changement d‚Äôimage docker, de configuration, ‚Ä¶)

Le DeploymentController (le A sur le master) va r√©agir √† ce changement de configuration

##==##

<!-- .slide:-->

# Rolling upgrade 2/4

![](./assets/images/g3f3310ef84_0_1035.png)

Notes:
Le DeploymentController va instancier un nouveau Pod ‚Äúv2‚Äù et attendre qu‚Äôil soit fonctionnel.

D√®s que ce nouveau Pod est pr√™t, le Service va lui envoyer des requ√™tes.

L‚Äôancien Pod ne re√ßoit plus de nouvelles requ√™tes, il va √™tre arr√™t√© puis supprim√© par le DeploymentController.

##==##

<!-- .slide:-->

# Rolling upgrade 3/4

![](./assets/images/g3f3310ef84_0_1040.png)

Notes:
Chaque Pod est ainsi remplac√© par la nouvelle version, un par un.

##==##

<!-- .slide:-->

# Rolling upgrade 4/4

![](./assets/images/g3f3310ef84_0_1045.png)

Notes:
√Ä la fin du processus de rolling-upgrade, il ne reste que des pods v2.

Il n‚Äôy a pas eu d‚Äôinterruption de service.

##==##

<!-- .slide: class="with-code" -->

```
Mise √† jour progressive (rolling-upgrade) sans interruption de service
G√©r√©e par le Deployment

Remplacer nginx par Apache httpd :

$ kubectl edit deployment nginx --record
‚Äúimage: nginx:alpine‚Äù ‚áí ‚Äúimage: httpd:alpine‚Äù


```

# Mise √† jour progressive

Notes:
Fonctionnalit√© native Kubernetes au niveau d‚Äôun d√©ploiement.

Regarder la mise √† jour dans le visualizer

V√©rifier que nginx a bien √©t√© remplac√© par apache dans le navigateur.

Edition en direct de la spec du Deployment

Sortir de vim avec ‚Äú:wq‚Äù

##==##

<!-- .slide: class="with-code" -->

```
$ kubectl rollout history deployment nginx
REV  CHANGE-CAUSE1    kubectl apply --filename=deployment.yaml2    kubectl edit deployment nginx3    kubectl set image deploy nginx nginx=nginx:stable-alpine

$ kubectl rollout undo deployment --to-revision=1

```

# Annuler une mise √† jour progressive

Notes:
Le nombre de r√©vision conserv√©es est contr√¥le par la propri√©t√©
.spec.revisionHistoryLimit
, 10 par d√©faut.

Ce sont les ressources ‚ÄúReplicaSet‚Äù qui sont conserv√©s dans etcd, avec
replicas=0
pour les anciennes versions.

##==##

<!-- .slide:-->

# Mise √† l‚Äô√©chelle automatique

##==##

<!-- .slide:-->

# Horizontal Pod Autoscaler

AverageCPU Usage %

![](./assets/images/g3f3310ef84_0_1082.png)

Notes:
L‚Äô
Horizontal Pod Autoscaler
est un Controller Kubernetes responsable de

modifier en temps r√©el le nombre de replicas
d‚Äôun ReplicationController / Deployment

en fonction de la charge CPU
moyenne de l‚Äôensemble de pods de ce RC/D√©ployment.

https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/

https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/



##==##

<!-- .slide: class="with-code" -->

```

D√©clarer un Horizontal Pod scaler en CLI :
$ kubectl autoscale deployment nginx \
    --cpu-percent=50 --min=1 --max=10



```

# Horizontal Pod Autoscaler

Notes:
On peut d√©clarer un horizontal pod autoscaler en cli ou via l‚Äôapi, exemple via l‚Äôapi page suivante.

Le HPA va agir sur le deployment pour monter ou descendre le nombre de pod en fonction du pourcentage de cpu consomm√©.

D‚Äôautres m√©triques peuvent √™tre utilis√©s.

##==##

<!-- .slide: class="with-code" -->

# Horizontal Pod Autoscaler

```
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: helloweb
spec:
  maxReplicas: 10
  minReplicas: 1
  targetCPUUtilizationPercentage: 50
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: helloweb

```

##==##

<!-- .slide:-->

# TP : Autoscaling

##==##

<!-- .slide:-->

Les tests vont s‚Äôeffectuer sur l‚Äôimage docker k8s.gcr.io/hpa-example.
Cette image contient un apache+php. La home page de cette apache est un script php qui calcule les racines carr√©es des nombres de 1 √† 1 million.
L‚Äôappel √† cette page va provoquer une forte consommation CPU.

# TP : Autoscaling

Notes:
Version
imp√©rative
de la commande
versus
version
d√©clarative
par fichier yaml.

##==##

<!-- .slide:-->

# TP : Autoscaling

Cr√©er un deployment
$ kubectl apply -f deployments/hpa-example.yaml
Exposer le deployment
$ kubectl expose deployment/hpa-example --port 80
Cr√©er un hpa sur le deployment
$ kubectl autoscale deployment hpa-example --cpu-percent=50 --min=1 --max=10

##==##

<!-- .slide:-->

Vous lancerez ensuite un Pod interactif pour charger l‚ÄôApache :
$ kubectl run -ti load-generator --image=busybox /bin/sh
Dans laquelle vous ex√©cuterez la boucle d‚Äôappels :
$ while true;
do wget -q -O- http://hpa-example;
done

# TP : Autoscaling

##==##

<!-- .slide:-->

# TP : Autoscaling

V√©rifier l‚Äô√©tat de l‚Äôobjet HPA
$ kubectl get hpa/hpa-example
Arr√™ter la commande wget
Attendez 5 minutes
V√©rifier √† nouveau l‚Äô√©tat du HPA
Que constatez-vous ?

##==##

<!-- .slide:-->

# Ingress

##==##

<!-- .slide:-->

Point d‚Äôentr√©e unique du cluster

HTTP (port 80) et HTTPS (port 443)

Gestion des certificats SSL

# Ingress

##==##

<!-- .slide:-->

# Ingress

sfeir.com/

sfeir.com/fr/livres-blancs-experts-en-numerique/

institute.sfeir.com/

     HTTP (80)     HTTPS (443)

##==##

<!-- .slide: class="with-code" -->

```
apiVersion: v1kind: Ingressmetadata:  name: nginxspec:  rules:  - host: "institute.sfeir.com"
    http:      paths:      - path: /
        pathType: ImplementationSpecific
        backend:          service
            name: nginx
            port:
              name: http

```

# Ingress yaml

Notes:
Les r√®gles permettent, avec l‚Äôimpl√©mentation actuelle, de router le traffic selon :

le hostname (en-t√™te HTTP ‚Äú
Host:
‚Äù)

le path HTTP (√† partir du
/
suivant le nom+port de serveur dans l‚Äôurl)

##==##

<!-- .slide: class="with-code" -->

# Ingress

```
$ kubectl get ingressNAME      HOSTS     ADDRESS     PORTS     AGEnginx     *         localhost   80        3m$ kubectl get ing -o yaml nginx
apiVersion: apps/v1kind: Ingress
metadata:
  name: nginx...

```

##==##

<!-- .slide: class="with-code" -->

# Ingress

```
Impl√©mentation par d√©faut √† base de nginx

√Ä activer sur minikube (minikube addons enable ingress)

√Ä d√©ployer manuellement sous Docker-for-Desktop
https://kubernetes.github.io/ingress-nginx/deploy/

```

##==##

<!-- .slide:-->

# Demo : Ingress

##==##

<!-- .slide:-->

# R√©capitulatif

![](./assets/images/g3f3310ef84_0_159.png)

Notes:
Jusqu‚Äô√† maintenant avons abord√© les principales ressources Kubernetes :

Pod, ReplicaSet,
Deployment, Service et Ingress

Voyons maintenant comment les faire vivre

##==##

<!-- .slide:-->

# Stocker des donn√©es

##==##

<!-- .slide:-->

emptyDir
hostPath

Juste un dossier mont√© dans un/des containers
Associ√© √† la vie du Pod, survit au restart des containers

Nombreuses impl√©mentations :

# Volumes

persistentVolumeClaim
configMap / secret

##==##

<!-- .slide: class="with-code" -->

```
Volume vide, cr√©√© au d√©marrage d‚Äôun pod, supprim√© avec la suppression du Pod.

volumes:  - name: cache-volume    emptyDir: {}

```

# emptyDir

Notes:
Utile pour passer des fichiers d‚Äôun container √† l‚Äôautre ou d‚Äôun initContainer √† un container.

##==##

<!-- .slide:-->

PersistentDisk GCE
Il doit √™tre dans le m√™me projet et la m√™me zone que les VM du cluster
Il n‚Äôest pas supprim√© √† la suppression du pod

# gcePersistentDisk

##==##

<!-- .slide:-->

volumes: - name: test-volume gcePersistentDisk: pdName: my-data-disk fsType: ext4

# gcePersistentDisk

Notes:
Il s‚Äôagit d‚Äôun disque compute engine classique.

Il NE peut √™tre monter en √©criture que sur UN SEUL node √† la fois, mais il peut √™tre monter en lecture sur plusieurs nodes.

##==##

<!-- .slide: class="with-code" -->

```
Monte dans le container un dossier du noeud sur lequel le Pod s‚Äôex√©cute
volumes:  - name: test-volume    hostPath:      # directory location on host      path: /data      # this field is optional      type: Directory


```

# hostPath

Notes:
Il s‚Äôagit d‚Äôun disque mont√© sur une VM du cluster, mais il faut que le pod tourne sur le noeud o√π le volume a √©t√© cr√©√©. Peu recommand√©.

##==##

<!-- .slide:-->

L‚Äôadmin cr√©e une ressource PersistentVolume associ√©e √† une espace de stockage
Le pod r√©clame du disque avec un PersistentVolumeClaim

# persistentVolume & persistentVolumeClaim

Notes:
PersistentVolumeClaim fait r√©f√©rence au PersistentVolume cr√©√© pr√©c√©demment

##==##

<!-- .slide: class="with-code" -->

```
kind: PersistentVolumeapiVersion: v1metadata:  name: task-pv-volume  labels:    type: localspec:  storageClassName: manual  capacity:    storage: 10Gi  accessModes:    - ReadWriteOnce  hostPath:    path: "/mnt/data"

```

# persistentVolume

Notes:
Ce persistent volume va cr√©√© un espace r√©serv√© de 10 Go sur l‚Äôun des noeuds du cluster.

##==##

<!-- .slide:-->

kind: PersistentVolumeClaimapiVersion: v1metadata: name: task-pv-claimspec: storageClassName: manual accessModes: - ReadWriteOnce resources: requests: storage: 3Gi

# persistentVolumeClaim

##==##

<!-- .slide:-->

kind: PodapiVersion: v1metadata: name: task-pv-podspec: volumes: - name: task-pv-storage persistentVolumeClaim: claimName: task-pv-claim

# Pod

containers: - name: task-pv-container image: nginx ports: - containerPort: 80 name: "http-server" volumeMounts: - mountPath: "/usr/share/nginx/html" name: task-pv-storage

##==##

<!-- .slide:-->

# TP : Volumes

##==##

<!-- .slide:-->

# TP : Volume

Objectif : d√©ployer un wordpress + MySql persistant
G√©n√©rer un secret avec un mot de passe
$ kubectl create secret generic mysql --from-literal=password=$(openssl rand -hex 12)

##==##

<!-- .slide:-->

# TP : Volume

Cr√©er les volumes
$ kubectl apply -f volume/mysql-volumeclaim.yaml
$ kubectl apply -f volume/wordpress-volumeclaim.yaml
Cr√©er la base mysql
$ kubectl apply -f volume/mysql.yaml
$ kubectl apply -f volume/mysql-service.yaml

##==##

<!-- .slide:-->

# TP : Volume

Cr√©er l‚Äôinstance Wordpress et exposer le service
$ kubectl apply -f volume/wordpress.yaml
$ kubectl apply -f volume/wordpress-service.yaml
Acc√©der √† votre wordpress
$ curl http://$(kubectl get service wordpress -o jsonpath="{.status.loadBalancer.ingress[0].ip}")

##==##

<!-- .slide:-->

# TP : Volume

D√©truire les pods mysql et wordpress
$ k delete pods -l="app=mysql"
$ k delete pods -l="app=wordpress"
Que se passe-t-il ? (un peu de patience quand m√™me ;) )

##==##

<!-- .slide:-->

# TP : Volume Nettoyage

$ kubectl delete service wordpress
$ kubectl delete deployment wordpress
$ kubectl delete pvc wordpress
$ kubectl delete service mysql
$ kubectl delete deployment mysql
$ kubectl delete pvc mysql

##==##

<!-- .slide:-->

# Gestion avanc√©e de Pods

##==##

<!-- .slide:-->

# DaemonSet : un Pod par noeud

Agent de monitoring
Relais de logs
Agent de stockage (client glusterfs, ‚Ä¶)

agent-abcd

agent-efgh

agent-ijkl

Notes:
Un objet DaemonSet va ex√©cuter un pod par node, utilis√© surtout pour des t√¢ches d‚Äôadministration/ monitoring du cluster.

##==##

<!-- .slide:-->

# StatefulSet : un Volume par Pod

Pour les applis qui ont besoin de garder un √©tat
Kube r√©associe les Pods aux volumes persistants
Dns : <name>-{0..N-1}.<service>.<ns>.svc.cluster.local

myapp-0

myapp-1

myapp-2

myvol-0

myvol-1

myvol-2

Notes:
Les pods sont num√©rot√©s dans l‚Äôordre, et les pods r√©assign√©s √† ce volumes au red√©marrage.

Quelques contraintes : les pods sont d√©marr√©s et √©teints dans l‚Äôordre de d√©marrage

##==##

<!-- .slide:-->

Job
Batch lanc√© une seule fois
Kubernetes ne relance pas les Pods de ce type sauf code retour en erreur
CronJob
Job d√©clench√© r√©guli√®rement selon une expression cron : "_/1 _ \* \* \*"

# Jobs

Notes:
Utile pour lancer un batch de fa√ßon r√©gulier.

##==##

<!-- .slide:-->

# Template

##==##

<!-- .slide:-->

# Probl√®me

![](./assets/images/g3404c40d58_0_11.png)

Notes:
Un d√©ploiement peut devenir rapidement tr√®s verbeux, tr√®s long, difficile √† maintenir

##==##

<!-- .slide:-->

# Probl√®mes

yaml verbeux
beaucoup de code en commun entre 2 d√©ploiements
besoin de d√©ployer un ‚Äúpackage‚Äù

Notes:
Probl√®me de syntaxe avec le yaml

Il y a g√©n√©ralement entre 2 applications similaires, peu de diff√©rence

Plut√¥t que de d√©ployer un deployment + un service + un configmap + ... , on a besoin de d√©ployer un package complet.

##==##

<!-- .slide: class="with-code" -->

# Template : Helm

```
Le plus populaire aujourd‚Äôhui
Templating sur la base du moteur de template go
Chart
Repository de chart
Tiller et √©tat
$ helm repo update
$ helm install mysql bitnami/mysql


```

##==##

<!-- .slide: class="with-code" -->

# Helm : chart

```
wordpress/
  Chart.yaml          # A YAML file containing information about the chart
  LICENSE             # OPTIONAL: A plain text file containing the license for the chart
  README.md           # OPTIONAL: A human-readable README file
  requirements.yaml   # OPTIONAL: A YAML file listing dependencies for the chart
  values.yaml         # The default configuration values for this chart
  charts/             # A directory containing any charts upon which this chart depends.
  templates/          # A directory of templates that, when combined with values,
                      # will generate valid Kubernetes manifest files.
  templates/NOTES.txt # OPTIONAL: A plain text file containing short usage notes

```

Notes:
L‚Äôorganisation d‚Äôun chart helm

##==##

<!-- .slide: class="with-code"  class="with-code" -->

# Helm : Template

```
[deployment.yaml]
spec:
  containers:
    - name: deis-database
      image: postgres:{{.Values.dockerTag}}
      imagePullPolicy: {{.Values.pullPolicy}}
      ports:
        - containerPort: 5432
      env:
        - name: DATABASE_STORAGE
          value: {{default "minio" .Values.storage}}

```

```
[Values.yaml]

# The tag for the docker image.
dockerTag: 7.1.2
pullPolicy: Always
# The storage backend, whose default is set to "minio"
# storage:

```

Notes:
Values.yaml

imageRegistry
: The source registry for the Docker image.

dockerTag
: The tag for the docker image.

pullPolicy
: The Kubernetes pull policy.

storage
: The storage backend, whose default is set to
"minio"

##==##

<!-- .slide:-->

# Template : Kustomize

Challenger
Plain yaml
Pas de ‚Äútemplating‚Äù
Mais plut√¥t du ‚Äúpatch‚Äù
Int√©gr√© √† kubectl ‚Äòkubectl apply -k ...‚Äô

##==##

<!-- .slide:-->

# Kustomize

![](./assets/images/g3404c40d58_0_23.png)

Notes:
Un exemple avec kustomize

##==##

<!-- .slide:-->

# S√©curit√©

##==##

<!-- .slide:-->

# RBAC

Role-based Access Control
Contr√¥le l‚Äôacc√®s √† l‚Äôapi pour les utilisateurs
Gestion des droits par d√©faut dans kubernetes

Role / ClusterRole
RoleBinding / ClusterRoleBinding

##==##

<!-- .slide: class="with-code"  class="with-code" -->

# RBAC

```
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: default
  name: pod-reader
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "watch", "list"]


```

```
...
kind: RoleBinding
subjects:
- kind: User
  name: jane # Name is case sensitive
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io

```

##==##

<!-- .slide:-->

# RBAC : r√¥les par d√©faut

view
edit
admin
cluster-admin

Notes:
view : permet d‚Äôavoir une vue readonly sur la plupart des objets d‚Äôun namespace, ne permet pas de voir les secrets, ou les objets roles et rolebinding

edit : permet de modifier la plupart des objets, mais pas les objet roles rolebinding

admin : permet d‚Äô√©diter les objets + les droits sur un namespace mais pas les quotas

cluster-admin : tous les droits

##==##

<!-- .slide:-->

# Monitoring

##==##

<!-- .slide:-->

# Monitoring : Prometheus

Pull de m√©triques sur les applications.
Inverse la logique de monitoring.

Notes:
Traditionnellement, les applications envoient leurs m√©triques √† des syst√®mes centralis√©s. Cette fa√ßon de faire rend l'application responsable de cet envoi des m√©triques. Dans une architecture cloud moderne, la logique est invers√©e pour faciliter ce monitoring.

Prometheus est un des outils qui impl√©mente cette logique. Il va interroger p√©riodiquement une URL sur vos applications pour lire vos m√©triques, puis envoyer ces m√©triques √† une base de donn√©es sp√©cialis√©e dans les s√©ries temporelles. Cette base pourra √™tre interrog√©e pour la surveillance des applications.

##==##

<!-- .slide:-->

# M√©thodes d‚Äôinstallation de clusters

##==##

<!-- .slide:-->

# Installation k8s : d√©veloppement

Docker Desktop
Rancher Desktop
Installation manuelle ‚Äúk8s the hard way‚Äù
Quid du r√©seau?
S√©curit√© first
Kind
Kops
K3s

Notes:
‚ÄúThe hard way‚Äú : pour les tr√®s courageux : comment installer un cluster k8s compl√©mente √† la main. (tr√®s int√©ressant pour comprendre k8s mais tr√®s peu recommand√©)

Docker for (Mac|Windows) : ‚Äúminikube‚Äù int√©gr√© dans la version desktop de docker (√† activer manuellement)

Kops : cli pour cr√©er un cluster k8s sur des VMs chez des cloud provider AWS, GCE

##==##

<!-- .slide:-->

# Installation du cluster : production

On premise
Kubeadm
Cloud
GKE (Google)
AKS (Azure)
EKS (AWS)
OVH
‚Ä¶.

Notes:

Kubeadm : la version pr√©conis√©e aujourd‚Äôhui pour une installation on premise

Kubernetes Manag√© : GCP, Amazon, Azure, mais aussi Digital Ocean, OVH (and counting‚Ä¶)

##==##

<!-- .slide:-->

# Service mesh

##==##

<!-- .slide:-->

# Istio

Routage
S√©curit√©
Observabilit√©
En option sur GKE
CNCF

Notes:
routage : canary release / circuit breaker

s√©curit√© : SSL entre les services, authentification, autorisation

observabilit√© : remonte des m√©triques de connexion entre les services

cncf : le projet fait partie comme kubernetes de la cncf

##==##

<!-- .slide:-->

# Architecture micro-service

##==##

<!-- .slide:-->

Base de code : syst√®me de contr√¥le de version
D√©pendances : D√©clarez explicitement et isolez les d√©pendances
Configuration : Stockez la configuration dans l‚Äôenvironnement
Services externes : ressources rattach√©es
Build, release, run : S√©parez le build/packaging et d‚Äôex√©cution
Processus : Ex√©cutez l‚Äôapplication comme un ou plusieurs processus sans √©tat

# Les 12 facteurs des applications Cloud

Notes:
I. Base de codeUne base de code suivie avec un syst√®me de contr√¥le de version, plusieurs d√©ploiementsII. D√©pendancesD√©clarez explicitement et isolez les d√©pendancesIII. ConfigurationStockez la configuration dans l‚Äôenvironnement ===> ConfigMap, Secrets, Consul, ‚Ä¶

IV. Services externesTraitez les services externes comme des ressources attach√©esV. Build, release, runS√©parez strictement les √©tapes d‚Äôassemblage et d‚Äôex√©cutionVI. ProcessusEx√©cutez l‚Äôapplication comme un ou plusieurs processus sans √©tatVII. Associations de portsExportez les services via des associations de portsVIII. ConcurrenceGrossissez √† l‚Äôaide du mod√®le de processusIX. JetableMaximisez la robustesse avec des d√©marrages rapides et des arr√™ts gracieuxX. Parit√© dev/prodGardez le d√©veloppement, la validation et la production aussi proches que possibleXI. LogsTraitez les logs comme des flux d‚Äô√©v√®nementsXII. Processus d‚ÄôadministrationLancez les processus d‚Äôadministration et de maintenance comme des one-off-processes

##==##

<!-- .slide:-->

Associations de ports : Exportez les services via des associations de ports
VIII. Concurrence : Grossissez √† l‚Äôaide du mod√®le de processus
IX. Jetable : Maximisez la robustesse avec des d√©marrages rapides et des arr√™ts gracieux
X. Parit√© dev/prod : Gardez le d√©veloppement, la validation et la production aussi proches que possible
XI. Logs : Traitez les logs comme des flux d'√©v√©nements
XII. Processus d‚Äôadministration : Lancez les processus d‚Äôadministration et de maintenance comme des one-off-processes

# Les 12 facteurs des applications Cloud

Notes:
I. Base de codeUne base de code suivie avec un syst√®me de contr√¥le de version, plusieurs d√©ploiementsII. D√©pendancesD√©clarez explicitement et isolez les d√©pendancesIII. ConfigurationStockez la configuration dans l‚Äôenvironnement ===> ConfigMap, Secrets, Consul, ‚Ä¶

IV. Services externesTraitez les services externes comme des ressources attach√©esV. Build, release, runS√©parer strictement les √©tapes d‚Äôassemblage et d‚Äôex√©cutionVI. ProcessusEx√©cutez l‚Äôapplication comme un ou plusieurs processus sans √©tatVII. Associations de portsExportez les services via des associations de portsVIII. ConcurrenceGrossissez √† l‚Äôaide du mod√®le de processusIX. JetableMaximisez la robustesse avec des d√©marrages rapides et des arr√™ts gracieuxX. Parit√© dev/prodGardez le d√©veloppement, la validation et la production aussi proches que possibleXI. LogsTraitez les logs comme des flux d'√©v√©nementsXII. Processus d‚ÄôadministrationLancez les processus d‚Äôadministration et de maintenance comme des one-off-processes
